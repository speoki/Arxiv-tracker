<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-09 03:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251209_0321</div>
    <div class="row"><div class="card">
<div class="title">M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</div>
<div class="meta-line">Authors: David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata</div>
<div class="meta-line">First: 2025-12-05T18:55:58+00:00 · Latest: 2025-12-05T18:55:58+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05959v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05959v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>M4-RAG：大规模多语言多文化多模态RAG</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在视觉问答（VQA）方面取得了出色的表现，但仍然受限于静态训练数据。检索增强生成（RAG）通过提供最新的、文化基础的和多语言的信息来缓解这一限制；然而，多语言多模态RAG仍然很大程度上未被探索。我们引入了M4-RAG，这是一个涵盖42种语言和56种地区方言及体裁的大规模基准，包含超过80,000个文化多样性的图像-问题对，用于评估跨语言和模态的检索增强VQA。为了平衡现实性和可重复性，我们构建了一个受控的检索环境，包含数百万个与查询领域相关的精心策划的多语言文档，近似于真实世界的检索条件，同时确保一致的实验。我们的系统评估表明，尽管RAG持续改善较小的VLMs，但它无法扩展到更大的模型，并且往往甚至会降低其性能，揭示了模型规模与当前检索效果之间的关键不匹配。M4-RAG为推进能够无缝跨越语言、模态和文化背景的下一代RAG系统奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">M4-RAG is a large-scale benchmark for evaluating retrieval-augmented VQA across 42 languages and 56 regional dialects, comprising over 80,000 image-question pairs. It uses a controlled retrieval environment with millions of curated documents to simulate real-world conditions. The study finds that while RAG improves smaller VLMs, it often degrades the performance of larger models, highlighting a mismatch between model size and retrieval effectiveness. M4-RAG aims to advance RAG systems for better cross-language and cross-modal reasoning.</div>
<div class="mono" style="margin-top:8px">M4-RAG 是一个涵盖 42 种语言和 56 种区域方言的大规模基准，包含超过 80,000 个图像-问题对，用于评估跨语言和跨模态的检索增强 VQA。它使用包含数百万精心筛选文档的受控检索环境来模拟真实世界条件。研究发现，虽然 RAG 改善了较小的 VLM，但往往会降低较大模型的性能，揭示了模型大小与当前检索效果之间的不匹配。M4-RAG 旨在推动能够无缝跨语言、跨模态和文化背景进行推理的下一代 RAG 系统的发展。</div>
</details>
</div>
<div class="card">
<div class="title">iMotion-LLM: Instruction-Conditioned Trajectory Generation</div>
<div class="meta-line">Authors: Abdulwahab Felemban, Nussair Hroub, Jian Ding, Eslam Abdelrahman, Xiaoqian Shen, Abduallah Mohamed, Mohamed Elhoseiny</div>
<div class="meta-line">First: 2024-06-10T12:22:06+00:00 · Latest: 2025-12-05T18:52:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.06211v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.06211v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vision-cair.github.io/iMotion-LLM/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce iMotion-LLM, a large language model (LLM) integrated with trajectory prediction modules for interactive motion generation. Unlike conventional approaches, it generates feasible, safety-aligned trajectories based on textual instructions, enabling adaptable and context-aware driving behavior. It combines an encoder-decoder multimodal trajectory prediction model with a pre-trained LLM fine-tuned using LoRA, projecting scene features into the LLM input space and mapping special tokens to a trajectory decoder for text-based interaction and interpretable driving. To support this framework, we introduce two datasets: 1) InstructWaymo, an extension of the Waymo Open Motion Dataset with direction-based motion instructions, and 2) Open-Vocabulary InstructNuPlan, which features safety-aligned instruction-caption pairs and corresponding safe trajectory scenarios. Our experiments validate that instruction conditioning enables trajectory generation that follows the intended condition. iMotion-LLM demonstrates strong contextual comprehension, achieving 84% average accuracy in direction feasibility detection and 96% average accuracy in safety evaluation of open-vocabulary instructions. This work lays the foundation for text-guided motion generation in autonomous driving, supporting simulated data generation, model interpretability, and robust safety alignment testing for trajectory generation models. Our code, pre-trained model, and datasets are available at: https://vision-cair.github.io/iMotion-LLM/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>iMotion-LLM：基于指令的轨迹生成</div>
<div class="mono" style="margin-top:8px">我们介绍了iMotion-LLM，这是一种将大型语言模型（LLM）与轨迹预测模块结合的交互式运动生成系统。与传统方法不同，它基于文本指令生成可行且安全对齐的轨迹，从而实现适应性强且上下文感知的驾驶行为。该系统结合了编码器-解码器多模态轨迹预测模型和使用LoRA微调的预训练LLM，将场景特征投影到LLM输入空间，并将特殊标记映射到轨迹解码器，以实现基于文本的交互和可解释的驾驶。为了支持该框架，我们引入了两个数据集：1）InstructWaymo，这是Waymo开放运动数据集的扩展，包含基于方向的运动指令；2）Open-Vocabulary InstructNuPlan，该数据集包含安全对齐的指令-描述对以及相应的安全轨迹场景。我们的实验验证了指令条件能够使轨迹生成遵循预期条件。iMotion-LLM展示了强大的上下文理解能力，在方向可行性检测中平均准确率为84%，在开放词汇指令的安全评估中平均准确率为96%。这项工作为自主驾驶中的文本引导运动生成奠定了基础，支持模拟数据生成、模型可解释性和轨迹生成模型的稳健安全对齐测试。我们的代码、预训练模型和数据集可在以下网址获取：https://vision-cair.github.io/iMotion-LLM/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">iMotion-LLM is a large language model integrated with trajectory prediction modules for interactive motion generation. It generates feasible and safety-aligned trajectories based on textual instructions, combining an encoder-decoder multimodal trajectory prediction model with a fine-tuned LLM using LoRA. Experiments show that iMotion-LLM achieves 84% accuracy in direction feasibility detection and 96% accuracy in safety evaluation, supporting contextual comprehension and robust safety alignment in autonomous driving scenarios.</div>
<div class="mono" style="margin-top:8px">iMotion-LLM 是一个结合了轨迹预测模块的大语言模型，能够根据文本指令生成可行且安全对齐的轨迹。它结合了编码器-解码器多模态轨迹预测模型和使用 LoRA 细调的 LLM，将场景特征投影到 LLM 输入空间，并将特殊标记映射到轨迹解码器。实验表明，iMotion-LLM 在方向可行性检测中的准确率为 84%，在开放词汇指令的安全评估中的准确率为 96%，展示了强大的上下文理解能力，并支持自动驾驶应用。</div>
</details>
</div>
<div class="card">
<div class="title">SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</div>
<div class="meta-line">Authors: Haowen Liu, Shaoxiong Yao, Haonan Chen, Jiawei Gao, Jiayuan Mao, Jia-Bin Huang, Yilun Du</div>
<div class="meta-line">First: 2025-12-05T18:51:03+00:00 · Latest: 2025-12-05T18:51:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05955v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://simpact-bot.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIMPACT：使用视觉-语言模型的仿真驱动行动规划</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）表现出显著的常识和语义推理能力。然而，它们缺乏对物理动力学的现实理解。这一限制源于VLMs在静态互联网规模的视觉-语言数据上进行训练，这些数据中没有因果交互或动作条件下的变化。因此，利用VLMs进行需要物理理解、推理和相应行动规划的精细机器人操作任务仍然具有挑战性。为了解决这个问题，我们提出了SIMPACT，一种测试时的仿真驱动行动规划框架，通过仿真闭环世界建模为VLM提供物理推理能力，而无需额外的训练。从单个RGB-D观察开始，SIMPACT高效地构建物理仿真，使VLM能够提出有信息的行动，观察仿真滚动，并逐步完善其推理。通过将语言推理与物理预测相结合，我们的仿真驱动的VLM能够以物理为基础的方式理解接触动力学和行动结果。我们的方法在五个需要精细物理推理的现实世界刚体和可变形操作任务上展示了最先进的性能，优于现有的通用机器人操作模型。我们的结果表明，在测试时通过高效的仿真嵌入物理理解为VLM推理提供了通向可泛化的具身智能的有希望的道路。项目网页可访问 https://simpact-bot.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SIMPACT is a framework that enhances Vision-Language Models (VLMs) with physical reasoning capabilities by integrating simulation-in-the-loop world modeling. It allows VLMs to construct physics simulations from a single RGB-D observation, iteratively refine reasoning, and propose informed actions. SIMPACT outperforms existing models on five challenging manipulation tasks, showing state-of-the-art performance and promising potential for embodied intelligence.</div>
<div class="mono" style="margin-top:8px">研究旨在通过增强视觉语言模型（VLMs）的物理理解能力，解决其在机器人操作任务中的不足。SIMPACT 是一个测试时的仿真驱动动作规划框架，它在无需额外训练的情况下将物理仿真集成到 VLMs 中。该方法从单个 RGB-D 观测中构建物理仿真，使 VLM 能够基于物理推理提出并迭代优化动作。实验结果显示，SIMPACT 在五个具有挑战性的操作任务中表现优于现有模型，展示了将物理理解嵌入 VLMs 中以实现具身智能的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Shima Imani, Seungwhan Moon, Lambert Mathias, Lu Zhang, Babak Damavandi</div>
<div class="meta-line">First: 2025-12-05T18:40:18+00:00 · Latest: 2025-12-05T18:40:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05943v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05943v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRACE：分析和增强视觉语言模型逐步推理的一种框架</div>
<div class="mono" style="margin-top:8px">可靠地进行数学和科学推理仍然是大型视觉语言模型面临的开放挑战。标准的最终答案评估往往掩盖了推理错误，允许无声失败持续存在。为了解决这一问题，我们引入了TRACE，一种透明推理和一致性评估框架，该框架诊断推理轨迹而非仅关注最终结果。核心上，TRACE 利用辅助推理集，这是一种分解复杂问题的紧凑子问题答案对，通过基于一致性的度量评估中间步骤，并揭示标准评估中未注意到的失败。我们的实验表明，辅助推理集（ARS）的一致性与最终答案的正确性相关，并有助于确定失败出现的推理步骤，提供可操作的信号以改进模型。此外，TRACE 定义了置信区域，区分可靠和不可靠的推理路径，支持有效的过滤、调试和模型细化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the reliability of mathematical and scientific reasoning in large vision-language models by addressing the limitations of standard final-answer evaluation. TRACE, a framework for Transparent Reasoning And Consistency Evaluation, introduces Auxiliary Reasoning Sets to decompose complex problems and evaluate intermediate steps. Experiments demonstrate that consistency across these sets correlates with correct final answers and helps identify specific reasoning steps where failures occur, providing actionable insights for model improvement. Additionally, TRACE defines confidence regions to distinguish reliable from unreliable reasoning paths, aiding in effective debugging and model refinement.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决标准最终答案评估的局限性，提高大型视觉-语言模型在数学和科学推理方面的可靠性。TRACE，一种透明推理和一致性评估框架，引入了辅助推理集来分解复杂问题并评估中间步骤。实验表明，这些集中的一致性与正确的最终答案相关，并有助于识别特定推理步骤中的失败，提供模型改进的行动指南。此外，TRACE 定义了可信区间，以区分可靠的和不可靠的推理路径，支持有效的过滤、调试和模型优化。</div>
</details>
</div>
<div class="card">
<div class="title">Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</div>
<div class="meta-line">Authors: Zhiyuan Jiang, Shenghao Xie, Wenyi Li, Wenqiang Zu, Peihang Li, Jiahao Qiu, Siqi Pei, Lei Ma, Tiejun Huang, Mengdi Wang, Shilong Liu</div>
<div class="meta-line">First: 2025-12-05T18:39:12+00:00 · Latest: 2025-12-05T18:39:12+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/Princeton-AI2-Lab/ZoomClick</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05941v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05941v1">PDF</a> · <a href="https://github.com/Princeton-AI2-Lab/ZoomClick">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>放大缩小，点击退出：解锁和评估缩放技术在GUI语义理解中的潜力</div>
<div class="mono" style="margin-top:8px">语义理解是构建图形用户界面（GUI）代理的基本能力。尽管现有方法依赖于大规模边界框监督，但仍面临各种挑战，如跨平台通用性、复杂布局分析和细粒度元素定位。在本文中，我们研究了缩放作为GUI语义理解中强大但尚未充分探索的先验，并提出了一种无需训练的方法ZoomClick。通过描述缩放的四个关键属性（即预缩放、深度、缩小尺寸、最小裁剪尺寸），我们解锁了其动态空间聚焦和自适应上下文切换的全部能力。实验表明，我们的方法显著提升了通用视觉-语言模型和专门的GUI语义理解模型的性能，在多个主流基准上取得了最先进的结果；例如，UI-Venus-72B在ScreenSpot-Pro上的成功率为73.1%。此外，我们提出了GUIZoom-Bench，这是一个用于评估模型对缩放适应性的基准，旨在激发未来研究以提高缩放在GUI语义理解任务中的进一步训练和测试时缩放能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the use of zoom as a powerful yet underexplored technique for GUI grounding, proposing a training-free method called ZoomClick. By leveraging four key properties of zoom, the method enhances the dynamic spatial focusing and adaptive context switching capabilities. Experimental results show that ZoomClick significantly improves the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several benchmarks. Additionally, the paper introduces GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to advance future research in this area.</div>
<div class="mono" style="margin-top:8px">本文探索了将缩放作为GUI定位中强大但尚未充分探索的技术，并提出了一种无需训练的方法ZoomClick。通过利用缩放的四个关键属性，该方法增强了动态空间聚焦和自适应上下文切换的能力。实验结果表明，ZoomClick 显著提高了通用视觉-语言模型和专门的GUI定位模型的性能，并在多个基准上达到了最先进的结果。此外，本文还引入了GUIZoom-Bench，一个用于评估模型对缩放适应性的基准，旨在推动该领域的未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation</div>
<div class="meta-line">Authors: Shima Imani, Seungwhan Moon, Adel Ahmadyan, Lu Zhang, Kirmani Ahmed, Babak Damavandi</div>
<div class="meta-line">First: 2025-12-05T18:14:55+00:00 · Latest: 2025-12-05T18:14:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05930v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05930v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRiSM：一种基于Python评估的科学推理多模态基准</div>
<div class="mono" style="margin-top:8px">在数学和物理学等科学领域评估视觉-语言模型（VLMs）带来了独特的挑战，远超出了预测最终答案的范围。这些领域需要概念理解、符号推理和遵守正式法则，而现有的大多数基准未能解决这些问题。特别是，当前的数据集往往是静态的，缺乏中间推理步骤、对变化的鲁棒性或验证科学正确性的机制。为了解决这些局限性，我们引入了PRiSM，这是一种合成的、完全动态的、基于多模态的评估科学推理的基准，通过嵌入的Python代码进行评估。PRiSM 包含超过24,750个大学级别的物理和数学问题，并利用我们的可扩展的基于代理的管道PrismAgent生成结构良好的问题实例。每个问题包含动态的文本和视觉输入、生成的图表，以及丰富的结构化输出：用于生成和验证真实情况的可执行Python代码，以及详细的逐步推理。我们的基准的动态特性和基于Python的自动化真实情况生成允许对多模态VLMs进行精细的实验审计，揭示其失败模式、不确定性行为和科学推理的局限性。为此，我们提出了五个有针对性的评估任务，涵盖泛化、符号程序合成、扰动鲁棒性、推理纠正和歧义解决。通过全面评估现有的VLMs，我们指出了它们的局限性，并展示了PRiSM如何使我们更深入地了解它们的科学推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PRiSM is designed to evaluate vision-language models in scientific domains by introducing a dynamic, multimodal benchmark grounded in Python code. It addresses the limitations of existing benchmarks by including detailed intermediate reasoning steps and mechanisms for verifying scientific correctness. Key findings show that current VLMs struggle with symbolic reasoning and robustness to variations, while PRiSM provides deeper insights into their scientific reasoning capabilities through five targeted evaluation tasks.</div>
<div class="mono" style="margin-top:8px">PRiSM 是一个基于 Python 代码的动态多模态基准，旨在评估科学领域的视觉-语言模型。它通过提供执行的 Python 代码来生成和验证地面真相，解决了现有基准的局限性。主要发现表明，当前的 VLM 在泛化、符号程序合成、扰动鲁棒性、推理纠正和歧义解决方面存在困难，突显了需要改进的科学推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning</div>
<div class="meta-line">Authors: Negin Baghbanzadeh, Mohammed Saidul Islam, Sajad Ashkezari, Elham Dolatabadi, Arash Afkanpour</div>
<div class="meta-line">First: 2025-06-03T10:53:19+00:00 · Latest: 2025-12-05T17:47:02+00:00</div>
<div class="meta-line">Comments: 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.02738v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.02738v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In biomedical vision-language modeling, datasets are typically mined from scientific literature, pairing compound figures with captions that are short, context-dependent, and oftern partially informative. Prior work on subfigure extraction has been limited in both dataset size and generalizability. In addition, no existing effort has incorporated rich medical context in image-text pairs. We revisit data curation as a foundational component of effective biomedical representation learning. Our data curation process integrates transformer-based subfigure detection, subcaption extraction, and contextual text enrichment derived from inline references. Our subfigure extraction model, trained on a corpus of 500,000 compound figures, achieves state-of-the-art performance on real and synthetic benchmarks. Using this process, we curate and release Open-PMC-18M, a large-scale high-fidelity biomedical dataset comprising 18 million image-text pairs, spanning radiology, microscopy, and visible light photography. We train vision-language models on our dataset and perform extensive evaluation on 6 retrieval and 19 zero-shot classification tasks across three major modalities. The models trained on our dataset set a new state-of-the-art results in medical representation learning. We release our dataset, models, and code to support reproducible benchmarks and further study into biomedical vision-language modeling and representation learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Open-PMC-18M：大规模多模态医学数据集用于高保真表示学习</div>
<div class="mono" style="margin-top:8px">在生物医学视觉-语言建模中，数据通常是从科学文献中挖掘出来的，将复合图与短、上下文依赖且经常部分信息的描述配对。先前的子图提取工作在数据集规模和泛化能力上都受到限制。此外，现有的努力没有在图像-文本对中融入丰富的医学上下文。我们重新审视数据收集作为有效生物医学表示学习基础组件的重要性。我们的数据收集过程结合了基于变换器的子图检测、子描述提取以及来自内联参考的上下文文本丰富。我们的子图提取模型在50万复合图的语料库上训练，实现了在真实和合成基准上的最佳性能。通过这一过程，我们收集并发布了Open-PMC-18M，这是一个包含1800万图像-文本对的大规模高保真生物医学数据集，涵盖了放射学、显微镜和可见光摄影。我们在该数据集上训练视觉-语言模型，并在三个主要模态的六个检索和十九个零样本分类任务上进行了广泛的评估。在该数据集上训练的模型在医学表示学习中取得了新的最佳结果。我们发布了数据集、模型和代码，以支持可重复的基准测试并进一步研究生物医学视觉-语言建模和表示学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the limitations of existing biomedical vision-language datasets by creating a large-scale, high-fidelity dataset called Open-PMC-18M. The method involves using transformer-based models for subfigure detection and subcaption extraction, along with contextual text enrichment from inline references. The dataset includes 18 million image-text pairs from radiology, microscopy, and visible light photography. Models trained on this dataset achieve state-of-the-art results in six retrieval and 19 zero-shot classification tasks across three major modalities, setting new benchmarks in medical representation learning.</div>
<div class="mono" style="margin-top:8px">研究旨在通过创建一个大规模、高质量的数据集来解决现有生物医学数据集的局限性，用于多模态表示学习。方法包括使用基于变换器的模型进行子图检测、子图描述提取和上下文文本丰富。最终生成的Open-PMC-18M数据集包含1800万幅图像-文本对，来自放射学、显微镜和可见光摄影领域。在该数据集上训练的模型在各种任务中达到了最先进的结果，包括三个主要模态的检索和零样本分类任务。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering Grounding IDs: How External Cues Shape Multimodal Binding</div>
<div class="meta-line">Authors: Hosein Hasani, Amirmohammad Izadi, Fatemeh Askari, Mobin Bagherian, Sadegh Mohammadian, Mohammad Izadi, Mahdieh Soleymani Baghshah</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-28T21:15:07+00:00 · Latest: 2025-12-05T17:19:01+00:00</div>
<div class="meta-line">Comments: Under review as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24072v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.24072v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the internal mechanisms underlying these gains remain unclear. We investigate this phenomenon and propose the concept of Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. Through representation analysis, we find that these identifiers emerge as consistent within-partition alignment in embedding space and reduce the modality gap between image and text. Causal interventions further confirm that these identifiers mediate binding between objects and symbolic cues. We show that Grounding IDs strengthen attention between related components, which in turn improves cross-modal grounding and reduces hallucinations. Taken together, our results identify Grounding IDs as a key symbolic mechanism that explains how external cues enhance multimodal binding and offer both interpretability and practical improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示接地ID：外部线索如何塑造多模态绑定</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）在多模态基准测试中表现出色，但在结构化推理和精确接地方面仍有限制。近期研究表明，添加简单的视觉结构，如分区和注释，可以提高准确性，但这些改进背后的内部机制尚不清楚。我们研究了这一现象，并提出了接地ID的概念，即由外部线索诱导的潜在标识符，这些标识符在不同模态中将对象与其指定的分区绑定在一起。通过表示分析，我们发现这些标识符在嵌入空间中表现为一致的分区内部对齐，并减少了图像和文本之间的模态差距。因果干预进一步证实这些标识符在对象与符号线索之间起中介作用。我们展示了接地ID增强了相关组件之间的注意力，从而提高了跨模态接地并减少了幻觉。综上所述，我们的结果将接地ID识别为一个关键的符号机制，解释了外部线索如何增强多模态绑定，并提供了可解释性和实际改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how external visual cues improve the performance of large vision-language models in multimodal tasks, particularly in structured reasoning and precise grounding. By proposing the concept of Grounding IDs, latent identifiers induced by external cues, the research finds that these identifiers enhance cross-modal alignment and reduce the gap between image and text representations. The study confirms that these identifiers mediate the binding between objects and symbolic cues, leading to improved cross-modal grounding and reduced hallucinations.</div>
<div class="mono" style="margin-top:8px">研究探讨了外部线索如何提高大型视觉-语言模型在多模态任务中的表现，特别是在结构化推理和精确定位方面。通过提出Grounding IDs的概念，即由外部线索诱导的隐含标识符，研究发现这些标识符在嵌入空间内一致地对齐对象于分区，减少模态差异，并增强跨模态定位。实验确认Grounding IDs在对象与符号线索之间起到中介作用，从而改善注意力并减少幻觉。</div>
</details>
</div>
<div class="card">
<div class="title">Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling</div>
<div class="meta-line">Authors: Saurav Jha, M. Jehanzeb Mirza, Wei Lin, Shiqi Yang, Sarath Chandar</div>
<div class="meta-line">First: 2025-12-05T15:30:08+00:00 · Latest: 2025-12-05T15:30:08+00:00</div>
<div class="meta-line">Comments: Extended abstract at World Modeling Workshop 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05809v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05809v1">PDF</a> · <a href="https://github.com/chandar-lab/visa-for-mindjourney">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney&#x27;s verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究世界模型在空间推理中的有效性通过测试时缩放</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在需要多视角理解和身体视角转换的空间推理任务中仍然受到限制。最近的方法如MindJourney试图通过测试时缩放来缓解这一差距，其中世界模型想象基于动作的轨迹，启发式验证器从中选择有用的观点。在本研究中，我们系统地考察了此类测试时验证器在基准测试中的表现，揭示了它们的潜力和局限性。基于不确定性的分析表明，MindJourney的验证器几乎没有提供有意义的校准，随机评分往往与减少答案熵的效果相当，从而暴露出系统性的动作偏差和不可靠的奖励信号。为缓解这些问题，我们引入了一种基于空间断言的验证框架（ViSA），将测试时的奖励与可验证的、帧锚定的微断言联系起来。这种原理性的验证器在SAT-Real基准测试中一致地提高了空间推理能力，并通过更平衡的探索行为纠正了轨迹选择偏差。然而，在具有挑战性的MMSI-Bench上，包括我们的验证器在内的所有验证器都无法实现一致的缩放，这表明当前的世界模型形成了一个信息瓶颈，想象中的视图未能丰富精细的推理。这些发现共同描绘了基于世界模型推理的测试时验证的优劣之处。我们的代码可在https://github.com/chandar-lab/visa-for-mindjourney/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates the effectiveness of test-time scaling in Vision-Language Models for spatial reasoning, focusing on the MindJourney approach. The study reveals that the heuristic verifier in MindJourney does not provide meaningful calibration and that random scoring can be equally effective, highlighting biases and unreliable reward signals. To address these issues, the authors propose a Verification through Spatial Assertions (ViSA) framework, which improves spatial reasoning on the SAT-Real benchmark and reduces trajectory selection biases. However, the current world models still form an information bottleneck, failing to enrich fine-grained reasoning on more challenging benchmarks like MMSI-Bench.</div>
<div class="mono" style="margin-top:8px">该研究探讨了测试时缩放在视觉-语言模型中进行空间推理的有效性，以MindJourney为例。研究发现，MindJourney中的启发式验证器未能提供有意义的校准，随机评分可以达到类似的效果，表明存在动作偏差和不可靠的奖励信号。作者引入了基于可验证的微断言的验证框架ViSA，以将奖励与可验证的帧锚定的微断言联系起来，这在SAT-Real上提高了空间推理能力，但在更具挑战性的MMSI-Bench上未能实现一致的缩放，表明当前的世界模型形成了一个信息瓶颈，想象的视图未能丰富精细的推理。这项工作揭示了测试时验证在基于世界模型的推理中的优缺点。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone</div>
<div class="meta-line">Authors: Antonio Bărbălau, Cristian Daniel Păduraru, Teodor Poncu, Alexandru Tifrea, Elena Burceanu</div>
<div class="meta-line">First: 2025-09-13T06:36:07+00:00 · Latest: 2025-12-05T13:23:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10809v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.10809v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sparse Autoencoders (SAEs) are widely employed for mechanistic interpretability and model steering. Within this context, steering is by design performed by means of decoding altered SAE intermediate representations. This procedure essentially rewrites the original activations as a weighted sum of decoder features. In contrast to existing literature, we forward an encoder-centric alternative to model steering which demonstrates a stronger cross-modal performance. We introduce S&amp;P Top-K, a retraining-free and computationally lightweight Selection and Projection framework that identifies Top-K encoder features aligned with a sensitive attribute or behavior, optionally aggregates them into a single control axis, and computes an orthogonal projection to be subsequently applied directly in the model&#x27;s native embedding space. In vision-language models, it improves fairness metrics on CelebA and FairFace by up to 3.2 times over conventional SAE usage, and in large language models, it substantially reduces aggressiveness and sycophancy in Llama-3 8B Instruct, achieving up to 3.6 times gains over masked reconstruction. These findings suggest that encoder-centric interventions provide a general, efficient, and more effective mechanism for shaping model behavior at inference time than the traditional decoder-centric use of SAEs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考稀疏自编码器：仅从编码器特征选择和投影实现公平性和控制</div>
<div class="mono" style="margin-top:8px">稀疏自编码器（SAEs）广泛应用于机制可解释性和模型引导。在此背景下，引导设计上是通过解码修改的SAE中间表示来实现的。这一过程本质上是将原始激活重新写为解码器特征的加权和。与现有文献不同，我们提出了一种以编码器为中心的模型引导替代方案，该方案展示了更强的跨模态性能。我们引入了S&amp;P Top-K，这是一种无需重新训练且计算量轻的选优和投影框架，该框架识别与敏感属性或行为对齐的Top-K编码器特征，可选地将它们聚合到一个控制轴上，并计算一个正交投影，随后直接应用于模型的原生嵌入空间中。在视觉-语言模型中，它在CelebA和FairFace上的公平性指标上比传统SAE使用方法提高了3.2倍，而在大型语言模型中，它显著减少了Llama-3 8B Instruct的攻击性和奉承性，实现了高达3.6倍的改进。这些发现表明，以编码器为中心的干预措施提供了一种更通用、更高效且更有效的机制，在推理时塑造模型行为，比传统的以解码器为中心的SAE使用方法更为有效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper rethinks the use of Sparse Autoencoders (SAEs) for model steering, proposing an encoder-centric approach called S&amp;P Top-K. This method selects and projects top K encoder features aligned with sensitive attributes or behaviors, optionally aggregating them into a single control axis, and applies an orthogonal projection directly in the model&#x27;s embedding space. The approach improves fairness metrics by up to 3.2 times in vision-language models on CelebA and FairFace, and reduces aggressiveness and sycophancy in Llama-3 8B Instruct by up to 3.6 times compared to conventional SAE usage.</div>
<div class="mono" style="margin-top:8px">论文重新思考了使用稀疏自编码器（SAEs）进行模型控制的方法，强调了对编码器特征的干预而非解码器。它引入了S&amp;P Top-K框架，选择并投影与敏感属性对齐的前K个编码器特征，可将它们聚合为单一控制轴。该方法在CelebA和FairFace上的公平性指标上提高了最多3.2倍，在Llama-3 8B Instruct上显著减少了攻击性和奉承性，实现了最高3.6倍的改进，超过传统的掩码重构方法。</div>
</details>
</div>
<div class="card">
<div class="title">Concept-Guided Backdoor Attack on Vision Language Models</div>
<div class="meta-line">Authors: Haoyu Shen, Weimin Lyu, Haotian Xu, Tengfei Ma</div>
<div class="meta-line">First: 2025-11-30T03:24:23+00:00 · Latest: 2025-12-05T13:17:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00713v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00713v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing &quot;cat&quot; with &quot;dog&quot;), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>概念引导的视觉语言模型后门攻击</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在多模态文本生成方面取得了显著进展，但其快速采用引发了越来越多关于安全漏洞的担忧。现有针对VLMs的后门攻击主要依赖于显式的像素级触发器或注入图像中的不可感知扰动。虽然这些方法有效，但它们降低了隐蔽性，并且仍然容易受到图像防御的攻击。我们引入了概念引导的后门攻击，这是一种在语义概念层面而非像素层面操作的新范式。我们提出了两种不同的攻击方法。第一种，概念阈值中毒（CTP），使用自然图像中的显式概念作为触发器：只有包含目标概念的样本才会被中毒，导致模型在其他情况下正常工作，但在概念出现时始终注入恶意输出。第二种，CBL引导的未见后门（CGUB），在训练过程中利用概念瓶颈模型（CBM）干预内部概念激活，而在推理时丢弃CBM分支以保持VLM不变。这种设计使得即使在训练数据中从未出现替换行为，也可以系统地替换生成文本中的目标标签（例如，将“猫”替换为“狗”）。在多个VLM架构和数据集上的实验表明，CTP和CGUB均能实现高攻击成功率，同时对干净任务性能的影响适中。这些发现突显了概念层面的漏洞作为VLMs的一个关键新的攻击面。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address security vulnerabilities in Vision-Language Models (VLMs) by introducing concept-guided backdoor attacks. Two methods are proposed: Concept-Thresholding Poisoning (CTP) uses explicit concepts in images as triggers, while CBL-Guided Unseen Backdoor (CGUB) leverages a Concept Bottleneck Model during training to modify internal concept activations without altering the model at inference time. Experiments demonstrate that both methods achieve high attack success rates with minimal impact on clean-task performance, highlighting the need to address concept-level vulnerabilities in VLMs.</div>
<div class="mono" style="margin-top:8px">本文提出了针对视觉语言模型（VLMs）的概念引导后门攻击，以解决这些模型的安全漏洞问题。提出了两种方法：概念阈值中毒（CTP）和CBL引导未见后门（CGUB）。CTP使用图像中的显式概念作为触发器，而CGUB在训练过程中使用概念瓶颈模型修改内部概念激活，但在推理时不改变模型。实验表明，这两种方法在保持对干净任务性能影响最小的情况下，实现了高攻击成功率，突显了VLMs中的概念级漏洞作为新的关键攻击面。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</div>
<div class="meta-line">Authors: Lingdong Kong, Xiang Xu, Jiawei Ren, Wenwei Zhang, Liang Pan, Kai Chen, Wei Tsang Ooi, Ziwei Liu</div>
<div class="meta-line">First: 2024-05-08T17:59:53+00:00 · Latest: 2025-12-05T12:51:41+00:00</div>
<div class="meta-line">Comments: IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.05258v3">Abs</a> · <a href="https://arxiv.org/pdf/2405.05258v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态数据高效自主驾驶3D场景理解</div>
<div class="mono" style="margin-top:8px">在自主驾驶中，3D场景理解的进步依赖于高效的数据利用，而对大量人工标注的LiDAR点云的依赖挑战了完全监督方法。为解决这一问题，我们的研究扩展到LiDAR语义分割的半监督学习，利用驾驶场景的固有空间先验和多传感器互补来增强未标注数据的有效性。我们引入了LaserMix++，这是一种集成不同LiDAR扫描的激光束操作并结合LiDAR-相机对应关系的框架，进一步辅助数据高效学习。该框架通过结合多模态，包括1）多模态LaserMix操作以实现细粒度的跨传感器交互；2）相机到LiDAR特征蒸馏以增强LiDAR特征学习；3）语言驱动的知识指导，使用开放词汇模型生成辅助监督。LaserMix++的灵活性使其适用于各种LiDAR表示，使其成为一种通用解决方案。我们的框架通过理论分析和在流行驾驶感知数据集上的广泛实验得到了严格验证。结果表明，LaserMix++显著优于完全监督方法，仅需五分之一的标注即可达到相当的准确性，并且显著提高了仅监督基线。这一重大进展突显了半监督方法在减少基于LiDAR的3D场景理解系统对大量标注数据依赖方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve 3D scene understanding in autonomous driving by addressing the challenge of heavy reliance on human-annotated LiDAR data. The study introduces LaserMix++, a semi-supervised learning framework that enhances data efficiency through multi-modal operations and LiDAR-camera correspondences. Experiments on popular driving perception datasets show that LaserMix++ significantly outperforms fully supervised methods, achieving comparable accuracy with five times fewer annotations and improving supervised baselines.</div>
<div class="mono" style="margin-top:8px">研究提出了一种半监督学习框架LaserMix++，通过利用多模态数据和场景的内在空间先验来解决自主驾驶中3D场景理解的数据效率问题。该框架整合了激光束操作和LiDAR-相机对应关系，以增强未标注数据的利用，并引入了多模态操作、特征蒸馏和基于语言的知识指导。实验结果表明，LaserMix++在更少的标注数据下显著优于全监督方法，展示了半监督方法在减少对大量标注数据依赖方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM</div>
<div class="meta-line">Authors: Sunghyun Ahn, Youngwan Jo, Kijung Lee, Sein Kwon, Inpyo Hong, Sanghyun Park</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-03-06T14:52:34+00:00 · Latest: 2025-12-05T11:34:09+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.04504v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.04504v4">PDF</a> · <a href="http://github.com/SkiddieAhn/Paper-AnyAnomaly">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive results on VAD benchmarks, achieving state-of-the-art performance on UBnormal and UCF-Crime and surpassing other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyAnomaly: LVLM驱动的零样本可定制视频异常检测</div>
<div class="mono" style="margin-top:8px">视频异常检测（VAD）对于计算机视觉中的视频分析和监控至关重要。然而，现有的VAD模型依赖于学习到的正常模式，这使得它们难以应用于多种环境。因此，用户需要重新训练模型或为新环境开发单独的AI模型，这需要机器学习专业知识、高性能硬件和大量数据收集，限制了VAD的实际可用性。为了解决这些挑战，本研究提出了可定制视频异常检测（C-VAD）技术和AnyAnomaly模型。C-VAD将用户定义的文本视为异常事件，并检测视频中包含指定事件的帧。我们通过上下文感知的视觉问答有效实现了AnyAnomaly，无需微调大型视觉语言模型。为了验证所提模型的有效性，我们构建了C-VAD数据集，并展示了AnyAnomaly的优越性。此外，我们的方法在VAD基准测试中表现出竞争力，分别在UBnormal和UCF-Crime上达到了最先进的性能，并在所有数据集上的泛化能力上超过了其他方法。我们的代码已在线发布在github.com/SkiddieAhn/Paper-AnyAnomaly上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of applying existing video anomaly detection models to diverse environments by proposing a customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. AnyAnomaly uses a context-aware visual question answering approach without fine-tuning a large vision language model, allowing users to define abnormal events through text. Experimental results show that AnyAnomaly outperforms other methods on VAD benchmarks, achieving state-of-the-art performance on UBnormal and UCF-Crime and demonstrating strong generalization across datasets.</div>
<div class="mono" style="margin-top:8px">研究通过提出可定制视频异常检测（C-VAD）技术和AnyAnomaly模型来解决将视频异常检测（VAD）模型应用于不同环境的挑战。AnyAnomaly利用上下文感知的视觉问答方法，无需微调大型视觉语言模型，来检测视频中的指定异常事件。实验结果表明，AnyAnomaly在VAD基准测试中优于其他方法，在UBnormal和UCF-Crime上达到最先进的性能，并且在所有数据集上具有很强的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation</div>
<div class="meta-line">Authors: Ruoxuan Zhang, Bin Wen, Hongxia Xie, Yi Yao, Songhan Zuo, Jian-Yu Jiang-Lin, Hong-Han Shuai, Wen-Huang Cheng</div>
<div class="meta-line">First: 2025-12-03T08:01:48+00:00 · Latest: 2025-12-05T10:31:39+00:00</div>
<div class="meta-line">Comments: Accepted by ACM Multimedia 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03540v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.03540v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CookAnything：一种灵活且一致的多步骤食谱图像生成框架</div>
<div class="mono" style="margin-top:8px">烹饪是一种顺序性和视觉导向的活动，其中每一步如切菜、搅拌或炒菜都包含程序逻辑和视觉语义。虽然最近的扩散模型在文本到图像生成方面表现出强大的能力，但在处理如食谱插图这样的结构化多步骤场景时却力不从心。此外，当前的食谱插图方法无法适应食谱长度的自然变化，无论实际指令结构如何，都会生成固定数量的图像。为了解决这些限制，我们提出了CookAnything，一种灵活且一致的基于扩散的框架，可以从任意长度的文本烹饪说明中生成连贯且语义上不同的图像序列。该框架引入了三个关键组件：(1) 步骤区域控制(SRC)，在单个去噪过程中将文本步骤与相应的图像区域对齐；(2) 灵活RoPE，一种步骤感知的位置编码机制，增强了时间连贯性和空间多样性；(3) 跨步骤一致性控制(CSCC)，在步骤之间保持细粒度的食材一致性。在食谱插图基准上的实验结果表明，CookAnything在基于训练和非基于训练的设置中都优于现有方法。所提出的框架支持复杂多步骤指令的可扩展、高质量的视觉合成，并在教学媒体和程序化内容创作方面具有广泛的应用潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CookAnything is a framework designed to generate coherent image sequences from cooking instructions, addressing the limitations of existing diffusion models in handling multi-step scenarios. It introduces SRC for aligning textual steps with image regions, Flexible RoPE for enhancing temporal coherence and spatial diversity, and CSCC for maintaining ingredient consistency across steps. Experimental results demonstrate that CookAnything outperforms existing methods in both training-based and training-free settings, supporting the visual synthesis of complex multi-step instructions.</div>
<div class="mono" style="margin-top:8px">CookAnything 是一个框架，用于从任意长度的烹饪文本指令生成连贯且语义上不同的图像序列。它引入了三个关键组件：步骤区域控制 (SRC)、灵活 RoPE 和跨步骤一致性控制 (CSCC)。该框架在训练依赖和非依赖设置中均优于现有方法，展示了其处理结构化多步骤场景和自然食谱长度变化的能力。实验结果表明，CookAnything 在生成复杂多步骤指令的高质量视觉内容方面表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">3D Question Answering via only 2D Vision-Language Models</div>
<div class="meta-line">Authors: Fengyun Wang, Sicheng Yu, Jiawei Wu, Jinhui Tang, Hanwang Zhang, Qianru Sun</div>
<div class="meta-line">First: 2025-05-28T09:04:39+00:00 · Latest: 2025-12-05T10:05:57+00:00</div>
<div class="meta-line">Comments: ICML2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22143v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.22143v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (LVLMs) have significantly advanced numerous fields. In this work, we explore how to harness their potential to address 3D scene understanding tasks, using 3D question answering (3D-QA) as a representative example. Due to the limited training data in 3D, we do not train LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a 3D point cloud and feed them into 2D models to answer a given question. When the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters the most. We propose cdViews, a novel approach to automatically selecting critical and diverse Views for 3D-QA. cdViews consists of two key components: viewSelector prioritizing critical views based on their potential to provide answer-specific information, and viewNMS enhancing diversity by removing redundant views based on spatial overlap. We evaluate cdViews on the widely-used ScanQA and SQA benchmarks, demonstrating that it achieves state-of-the-art performance in 3D-QA while relying solely on 2D models without fine-tuning. These findings support our belief that 2D LVLMs are currently the most effective alternative (of the resource-intensive 3D LVLMs) for addressing 3D tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>仅通过2D视觉-语言模型实现3D问答</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）在众多领域取得了显著进展。在本文中，我们探讨如何利用它们的潜力来解决3D场景理解任务，以3D问答（3D-QA）为例。由于3D数据的训练数据有限，我们没有训练LVLMs，而是以零样本的方式进行推理。具体来说，我们从3D点云中采样2D视图，并将它们输入2D模型以回答给定的问题。当选择2D模型时，例如LLAVA-OV，采样的视图质量最为重要。我们提出了cdViews，这是一种新颖的方法，用于自动选择关键且多样的视图以进行3D-QA。cdViews由两个关键组件组成：viewSelector，基于其提供答案特定信息的潜力来优先选择关键视图；以及viewNMS，通过基于空间重叠去除冗余视图来增强多样性。我们在广泛使用的ScanQA和SQA基准上评估了cdViews，证明它在3D-QA中实现了最先进的性能，仅依赖于2D模型而无需微调。这些发现支持我们相信2D LVLMs目前是解决3D任务最有效的替代方案（相对于资源密集型的3D LVLMs）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work explores using large vision-language models (LVLMs) for 3D question answering (3D-QA) in a zero-shot manner, by sampling 2D views from 3D point clouds and feeding them into 2D models. The proposed cdViews method selects critical and diverse views to improve the quality of answers. Evaluations on ScanQA and SQA benchmarks show that cdViews achieves state-of-the-art performance without fine-tuning, supporting the use of 2D LVLMs for 3D tasks.</div>
<div class="mono" style="margin-top:8px">本文探讨了使用大型视觉-语言模型（LVLMs）通过从3D点云中采样2D视图并输入2D模型来进行3D问答（3D-QA）的方法。提出的cdViews方法选择关键且多样的视图以提高答案质量。在ScanQA和SQA基准上的评估表明，cdViews在无需微调的情况下达到了最先进的性能，表明2D LVLMs对于3D任务是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning</div>
<div class="meta-line">Authors: Ting-Ting Xie, Yixin Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-05T09:56:58+00:00 · Latest: 2025-12-05T09:56:58+00:00</div>
<div class="meta-line">Comments: 2nd Place Solution to the CURE-Bench Competition @ NeurIPS 2025. Code available at https://github.com/June01/CureAgent</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05576v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05576v1">PDF</a> · <a href="https://github.com/June01/CureAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CureAgent：一种无需训练的执行-分析师框架用于临床推理</div>
<div class="mono" style="margin-top:8px">当前基于小型LLM的临床代理，如TxAgent，遭受了“上下文利用失败”的问题，模型虽然由于监督微调成功检索到生物医学证据，但在将这些信息应用于诊断方面却失败了。在本文中，我们提出了执行-分析师框架，这是一种模块化架构，将工具执行的句法精确性与临床推理的语义稳健性分离。通过协调专门的TxAgents（执行器）与长上下文基础模型（分析师），我们减轻了单体模型中观察到的推理缺陷。除了简单的模块化之外，我们证明了分层集成策略显著优于全局聚合，因为它保留了证据多样性，有效地解决了信息瓶颈问题。此外，我们的压力测试揭示了关键的扩展见解：（1）“上下文-性能悖论”，其中将推理上下文扩展到12k个标记以上引入了噪声，降低了准确性；（2）在操作空间中的“维度灾难”，其中扩展工具集需要分层检索策略。至关重要的是，我们的方法强调了无需训练的架构工程的潜力，无需昂贵的端到端微调即可在CURE-Bench上实现最先进的性能。代码已发布在https://github.com/June01/CureAgent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CureAgent addresses the Context Utilization Failure in clinical reasoning agents by proposing an Executor-Analyst Framework. This framework uses specialized TxAgents (Executors) and long-context foundation models (Analysts) to improve semantic robustness. The Stratified Ensemble strategy outperforms global pooling by preserving evidentiary diversity. Stress tests revealed a Context-Performance Paradox and the Curse of Dimensionality, highlighting the importance of hierarchical retrieval strategies. CureAgent achieves state-of-the-art performance on CURE-Bench without end-to-end finetuning, providing a scalable foundation for trustworthy AI-driven therapeutics.</div>
<div class="mono" style="margin-top:8px">研究旨在解决基于小型语言模型（如TxAgent）的临床代理在上下文利用方面的失败，这些模型虽然可以检索生物医学证据，但在应用这些信息时却存在问题。研究引入了执行者-分析师框架，将工具执行的语法精确性与临床推理的语义稳健性分离。通过使用专门的TxAgents（执行者）和长上下文基础模型（分析师），该框架缓解了推理缺陷。实验证明，分层集成策略优于全局聚合，因为它保留了证据多样性。压力测试显示，将推理上下文扩展到12k个标记以上会引入噪声，而扩展工具集需要分层检索策略。该方法在CURE-Bench上实现了最先进的性能，无需昂贵的端到端微调，为可信的人工智能驱动疗法提供了可扩展的基础。</div>
</details>
</div>
<div class="card">
<div class="title">MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging</div>
<div class="meta-line">Authors: Xingyu Zhang, Anna Reithmeir, Fryderyk Kögl, Rickmer Braren, Julia A. Schnabel, Daniel M. Lang</div>
<div class="meta-line">First: 2025-12-05T09:53:07+00:00 · Latest: 2025-12-05T09:53:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05571v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05571v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT achieves correspondence accuracy comparable to the state-of-the-art learning-based UniGradICON model and surpasses conventional B-spline-based registration, without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedDIFT：基于多尺度扩散的3D医学成像配准</div>
<div class="mono" style="margin-top:8px">医学图像之间的准确空间对应对于纵向分析、病灶追踪和图像引导干预至关重要。医学图像配准方法依赖于局部强度相似性度量，但无法捕捉全局语义结构，在低对比度或解剖结构变化区域常导致配准错误。最近在扩散模型方面的进展表明，其中间表示蕴含丰富的几何和语义信息。我们提出MedDIFT，这是一种无需训练的3D对应框架，利用预训练的医学扩散模型的多尺度特征作为体素描述符。MedDIFT将扩散激活融合到丰富的体素级描述符中，并通过余弦相似度进行匹配，可选地加入局部搜索先验。在公开的肺部CT数据集上，MedDIFT的对应精度与基于学习的UniGradICON模型相当，并优于传统的B样条配准方法，无需任何特定任务的模型训练。消融实验表明，多级特征融合和适度的扩散噪声可提高性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MedDIFT is a training-free 3D correspondence framework that uses multi-scale features from a pretrained latent medical diffusion model to generate voxel descriptors. It matches these descriptors using cosine similarity and optionally includes a local-search prior. On a lung CT dataset, MedDIFT achieves comparable correspondence accuracy to the state-of-the-art UniGradICON model and outperforms traditional B-spline-based registration methods. Ablation studies show that multi-level feature fusion and moderate diffusion noise enhance performance.</div>
<div class="mono" style="margin-top:8px">MedDIFT 是一个无需训练的 3D 对应框架，利用预训练的医疗扩散模型的多尺度特征生成体素描述符。通过余弦相似性进行匹配，可选地加入局部搜索先验。在肺部 CT 数据集上，MedDIFT 的对应精度与最先进的 UniGradICON 模型相当，并优于传统的 B-样条基于的注册方法。消融实验表明，多级特征融合和扩散噪声的引入可以提升性能。</div>
</details>
</div>
<div class="card">
<div class="title">ProPhy: Progressive Physical Alignment for Dynamic World Simulation</div>
<div class="meta-line">Authors: Zijun Wang, Panwen Hu, Jing Wang, Terry Jingchen Zhang, Yuhao Cheng, Long Chen, Yiqiang Yan, Zutao Jiang, Hanhui Li, Xiaodan Liang</div>
<div class="meta-line">First: 2025-12-05T09:39:26+00:00 · Latest: 2025-12-05T09:39:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05564v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05564v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProPhy：渐进物理对齐以实现动态世界模拟</div>
<div class="mono" style="margin-top:8px">近期视频生成技术的发展展示了构建世界模拟器的巨大潜力。然而，当前模型仍然难以生成物理上一致的结果，尤其是在处理大规模或复杂动力学时。这一限制主要源于现有方法对物理提示的各向同性响应以及生成内容与局部物理线索之间细粒度对齐的忽视。为解决这些挑战，我们提出了一种渐进物理对齐框架ProPhy，该框架能够实现显式的物理感知条件和各向异性生成。ProPhy采用了一种两阶段物理专家混合机制（MoPE）来提取判别性物理先验，其中语义专家从文本描述中推断语义级物理原理，细化专家捕捉标记级物理动态。该机制使模型能够学习更精细、物理感知的视频表示，更好地反映基本物理定律。此外，我们引入了一种物理对齐策略，将视觉语言模型（VLMs）的物理推理能力转移到细化专家中，从而更准确地表示动态物理现象。在物理感知视频生成基准上的广泛实验表明，ProPhy生成的结果更加真实、动态且物理上一致。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ProPhy is a Progressive Physical Alignment Framework designed to improve the physical consistency of dynamic world simulations. It uses a two-stage Mixture-of-Physics-Experts (MoPE) mechanism to extract discriminative physical priors, with Semantic Experts inferring physical principles from text and Refinement Experts capturing detailed physical dynamics. ProPhy also incorporates a physical alignment strategy from vision-language models to enhance the accuracy of dynamic physical phenomena. Experiments show that ProPhy generates more realistic and physically coherent results compared to existing methods.</div>
<div class="mono" style="margin-top:8px">ProPhy 是一种渐进物理对齐框架，旨在提高动态世界模拟的物理一致性。它通过引入两阶段的物理专家混合机制（MoPE），提取区分性的物理先验，并结合视觉语言模型的物理推理能力，以更准确地表示动态物理现象。实验结果表明，ProPhy 生成的视频更加真实、动态且物理上更一致，优于当前最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image</div>
<div class="meta-line">Authors: Ziqian Wang, Yonghao He, Licheng Yang, Wei Zou, Hongxuan Ma, Liu Liu, Wei Sui, Yuxin Guo, Hu Su</div>
<div class="meta-line">First: 2025-12-01T02:38:52+00:00 · Latest: 2025-12-05T09:22:27+00:00</div>
<div class="meta-line">Comments: Project page: https://d-robotics-ai-lab.github.io/TabletopGen.project/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01204v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.01204v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://d-robotics-ai-lab.github.io/TabletopGen.project/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI -- especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TabletopGen：从文本或单张图像生成实例级交互式3D桌面场景</div>
<div class="mono" style="margin-top:8px">生成高保真、物理交互的3D模拟桌面场景对于体态AI至关重要，尤其是在机器人操作策略学习和数据合成方面。然而，当前基于文本或图像的3D场景生成方法主要关注大规模场景，难以捕捉桌面场景中高密度布局和复杂的空间关系。为解决这些挑战，我们提出TabletopGen，这是一种无需训练、全自动的框架，能够生成多样化的实例级交互式3D桌面场景。TabletopGen接受参考图像作为输入，该图像可以通过文本到图像模型合成以增强场景多样性。然后我们对参考图像进行实例分割和完成，以获得每个实例的图像。每个实例随后被重建为3D模型，并进行标准坐标对齐。对齐后的3D模型进行姿态和尺度估计，然后组装成一个无碰撞、可模拟的桌面场景。我们框架的关键组件是一种新颖的姿态和尺度对齐方法，将复杂的空间推理分解为两个阶段：可微旋转优化器用于精确的旋转恢复，以及俯视图空间对齐机制用于稳健的平移和尺度估计，从而实现从2D参考图像到3D重建的准确重建。广泛的实验和用户研究显示，TabletopGen达到了最先进的性能，显著超越现有方法在视觉保真度、布局准确性和物理合理性方面，能够生成具有丰富风格和空间多样性的逼真桌面场景。我们的代码将公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">TabletopGen is a training-free framework for generating diverse, interactive 3D tabletop scenes from a reference image. It uses instance segmentation and completion to create per-instance images, which are then reconstructed into 3D models and aligned in canonical coordinates. A novel pose and scale alignment approach ensures accurate 3D reconstruction. Experiments show that TabletopGen outperforms existing methods in visual fidelity, layout accuracy, and physical plausibility, generating realistic and diverse tabletop scenes.</div>
<div class="mono" style="margin-top:8px">TabletopGen 是一个无需训练的框架，可以从参考图像生成多样化的实例级交互式 3D 桌面场景。它使用实例分割和完成来获取每个实例的图像，将它们重建为 3D 模型，并使用一种新颖的姿态和尺度对齐方法进行对齐。实验表明，TabletopGen 在视觉保真度、布局准确性和物理合理性方面超越了现有方法，能够生成逼真且多样的桌面场景。</div>
</details>
</div>
<div class="card">
<div class="title">Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models</div>
<div class="meta-line">Authors: Weijue Bu, Guan Yuan, Guixian Zhang</div>
<div class="meta-line">First: 2025-12-05T09:07:55+00:00 · Latest: 2025-12-05T09:07:55+00:00</div>
<div class="meta-line">Comments: 6 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05546v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05546v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of object hallucinations in Vision-Language Models (VLMs) due to text inertia, where attention shifts from visual evidence to linguistic priors. It introduces Conscious Gaze (CG-VLM), a training-free framework that uses a Cognitive Demand Sensor to estimate the need for visual grounding and a Focused Consensus Induction module to reorient mid-layer attention towards visual tokens. CG-VLM outperforms existing methods on POPE and CHAIR benchmarks across various VLMs, showing that token-level sensing can enable precise, context-aware intervention without losing foundational knowledge.</div>
<div class="mono" style="margin-top:8px">研究旨在解决大型视觉-语言模型（VLM）中由于注意力偏向语言先验而导致的物体幻觉问题。提出的Conscious Gaze（CG-VLM）框架在推理时无需训练，使用基于Harsanyi交互的认知需求传感器来估计需要视觉接地的时刻。该传感器触发聚焦共识诱导模块，重新定向中间层注意力指向视觉标记，从而减轻幻觉。CG-VLM在不同的VLM上在POPE和CHAIR基准测试中表现出色，表明标记级感知可以实现精确、上下文相关的干预，同时不丧失基础知识。</div>
</details>
</div>
<div class="card">
<div class="title">Enabling Validation for Robust Few-Shot Recognition</div>
<div class="meta-line">Authors: Hanxin Wang, Tian Liu, Shu Kong</div>
<div class="meta-line">First: 2025-06-05T07:37:15+00:00 · Latest: 2025-12-05T08:56:48+00:00</div>
<div class="meta-line">Comments: Project website: https://hannawang09.github.io/projects/vest/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04713v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.04713v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hannawang09.github.io/projects/vest/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-Shot Recognition (FSR) tackles classification tasks by training with minimal task-specific labeled data. Prevailing methods adapt or finetune a pretrained Vision-Language Model (VLM) and augment the scarce training data by retrieving task-relevant but noisy samples from open data sources. The finetuned VLM generalizes decently well to the task-specific in-distribution (ID) test data but struggles with out-of-distribution (OOD) test data. This motivates our study of robust FSR with VLM finetuning. The core challenge of FSR is data scarcity, extending beyond limited training data to a complete lack of validation data. We identify a key paradox as a potential solution: repurposing the retrieved open data for validation. As such retrieved data are inherently OOD compared with the task-specific ID training data, finetuned VLMs yield degraded performance on the retrieved data. This causes the validation logic to favor the pretrained model without any finetuning, hindering improvements w.r.t generalization. To resolve this dilemma, we introduce a novel validation strategy that harmonizes performance gain and degradation on the few-shot ID data and the retrieved data, respectively. Our validation enables parameter selection for partial finetuning and checkpoint selection, mitigating overfitting and improving test-data generalization. We unify this strategy with robust learning into a cohesive framework: Validation-Enabled Stage-wise Tuning (VEST). Extensive experiments on the established ImageNet OOD benchmarks show that VEST significantly outperforms existing VLM adaptation methods, achieving state-of-the-art FSR performance on both ID and OOD data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>启用验证以实现稳健的少样本识别</div>
<div class="mono" style="margin-top:8px">少样本识别（FSR）通过使用少量任务特定标记数据进行训练来解决分类任务。现有方法通过调整或微调预训练的视觉-语言模型（VLM），并从开放数据源中检索相关但嘈杂的样本来扩充稀缺的训练数据。微调后的VLM在任务特定的分布内（ID）测试数据上表现良好，但在分布外（OOD）测试数据上表现不佳。这促使我们研究基于VLM微调的稳健FSR。FSR的核心挑战是数据稀缺性，不仅限于有限的训练数据，还包括完全缺乏验证数据。我们发现一个关键悖论可能是潜在的解决方案：重新利用检索到的开放数据进行验证。由于这些检索到的数据与任务特定的ID训练数据相比是固有的OOD，微调后的VLM在这些数据上的表现较差。这导致验证逻辑倾向于选择未微调的预训练模型，阻碍了泛化能力的提升。为了解决这一困境，我们提出了一种新的验证策略，该策略在少样本ID数据和检索数据上分别平衡性能提升和下降。我们的验证策略能够选择部分微调的参数和检查点，减轻过拟合并提高测试数据的泛化能力。我们将这种策略与稳健学习统一到一个综合框架中：验证启用阶段式调优（VEST）。在建立的ImageNet OOD基准测试上的广泛实验表明，VEST显著优于现有的VLM适应方法，在ID和OOD数据上均实现了最先进的FSR性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robust few-shot recognition (FSR) by proposing a novel validation strategy to mitigate overfitting. The motivation arises from the difficulty in validating models trained with scarce data, especially when dealing with out-of-distribution (OOD) test data. The method introduces a validation-enabled stage-wise tuning (VEST) framework that harmonizes performance on in-distribution (ID) and out-of-distribution (OOD) data. Experiments on ImageNet OOD benchmarks demonstrate that VEST significantly improves FSR performance, outperforming existing methods on both ID and OOD data.</div>
<div class="mono" style="margin-top:8px">该研究通过利用预训练的Vision-Language模型并重新利用开放数据进行验证，解决了少样本识别（FSR）的鲁棒性问题。核心方法是一种名为Validation-Enabled Stage-wise Tuning（VEST）的新验证策略，该策略在保持分布内和分布外数据性能之间取得平衡。实验表明，VEST在ImageNet分布外基准上优于现有方法，实现了在分布内和分布外数据上的最佳少样本识别性能。</div>
</details>
</div>
<div class="card">
<div class="title">RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs</div>
<div class="meta-line">Authors: Jonathan Geuter, Gregor Kornhardt</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-05T08:55:39+00:00 · Latest: 2025-12-05T08:55:39+00:00</div>
<div class="meta-line">Comments: 20 pages, 3 figures. 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Foundations of Reasoning in Language Models</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05542v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05542v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoBoN：路由在线最佳-n测试时缩放多LLM的方法</div>
<div class="mono" style="margin-top:8px">最佳-n是一种广泛用于LLM推理的测试时缩放方法。尽管有证据表明LLM在不同任务上表现出互补的优势，但传统上最佳-n依赖单一模型生成响应。我们提出了RoBoN（路由在线最佳-n），这是一种基于单一模型的最佳-n的顺序多LLM替代方案。给定一组模型$\{m_i\}_{i=1}^M$，RoBoN基于奖励模型和预测响应的一致信号，顺序地将生成任务路由到各个模型。这种在线路由无需额外训练，保持计算量一致，并且可以与任何插件奖励模型一起使用。在推理基准测试（MATH500，奥林匹克竞赛题库，MinervaMath，GSM8K，MMLU）中，RoBoN在较大n值时始终优于单独模型应用的标准最佳-n，绝对准确率提高了高达3.4%，并且也优于均匀多模型组合基线。我们的结果表明，在推理时可以利用模型之间的多样性来提高最佳-n性能，超过任何单一模型的表现，提供了一种简单且无需训练的多LLM测试时缩放路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoBoN is a new approach to test-time scaling for LLM inference by routing generations sequentially across multiple models based on scores from a reward model and an agreement signal. It consistently outperforms traditional best-of-$n$ methods, achieving up to 3.4% higher accuracy on reasoning benchmarks, and improves over a uniform multi-model portfolio baseline.</div>
<div class="mono" style="margin-top:8px">RoBoN 是一种通过顺序路由多个模型生成内容来扩展 LLM 测试时缩放的新方法，基于奖励模型评分和预测响应的一致性信号。它在推理基准测试中始终优于传统最佳-of-$n$ 方法，最高可提高 3.4% 的准确率，并优于均匀的多模型组合基线。</div>
</details>
</div>
<div class="card">
<div class="title">See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors</div>
<div class="meta-line">Authors: Kunyi Yang, Qingyu Wang, Cheng Yuan, Yutong Ban</div>
<div class="meta-line">First: 2025-12-05T08:41:42+00:00 · Latest: 2025-12-05T08:41:42+00:00</div>
<div class="meta-line">Comments: The first two authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05529v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05529v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深入解析：基于单目深度先验的无训练手术场景分割</div>
<div class="mono" style="margin-top:8px">腹腔镜场景的像素级分割对于计算机辅助手术至关重要，但由于密集注释成本高昂，难以扩展。我们提出了深度引导的手术场景分割（DepSeg），这是一种无需训练的框架，利用单目深度作为几何先验，结合预训练的视觉基础模型。DepSeg 首先使用预训练的单目深度估计网络估计相对深度图，并提出深度引导的点提示，SAM2 将其转换为类无差别掩码。每个掩码随后由预训练的视觉特征池化描述，并通过与注释帧构建的模板库进行模板匹配分类。在 CholecSeg8k 数据集上，DepSeg 在直接 SAM2 自动分割基线之上取得了改进（35.9% 对 14.7% 的 mIoU），即使仅使用 10-20% 的对象模板，其性能也保持竞争力。这些结果表明，深度引导提示和基于模板的分类提供了一种注释高效的分割方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a training-free method for pixel-wise segmentation of laparoscopic scenes, addressing the challenge of high annotation costs. The method, Depth-guided Surgical Scene Segmentation (DepSeg), leverages monocular depth as a geometric prior with pretrained vision models. DepSeg estimates a relative depth map and uses it to propose depth-guided point prompts, which are converted into class-agnostic masks by SAM2. These masks are then classified using template matching against a template bank. On the CholecSeg8k dataset, DepSeg outperforms a direct SAM2 auto segmentation baseline and maintains good performance with limited object templates, demonstrating an efficient segmentation approach with reduced annotation requirements.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种无需训练的方法，用于腹腔镜场景的像素级分割，以应对高标注成本的挑战。方法Depth-guided Surgical Scene Segmentation (DepSeg) 利用单目深度作为几何先验，并结合预训练视觉模型。DepSeg 首先估计一个相对深度图，并使用它提出深度引导的点提示，这些提示由 SAM2 转换为类无差别掩码。然后，这些掩码通过与注释帧构建的模板库进行模板匹配来进行分类。在 CholecSeg8k 数据集上，DepSeg 在直接 SAM2 自动分割基线之上表现出色，并且即使使用少量对象模板也能保持良好的性能，展示了减少标注需求的有效分割方法。</div>
</details>
</div>
<div class="card">
<div class="title">VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation</div>
<div class="meta-line">Authors: Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando</div>
<div class="meta-line">First: 2025-12-05T08:34:06+00:00 · Latest: 2025-12-05T08:34:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05524v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05524v1">PDF</a> · <a href="https://github.com/LUNAProject22/VOST">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at https://github.com/LUNAProject22/VOST.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VOST-SGG: VLM辅助的一阶段时空场景图生成</div>
<div class="mono" style="margin-top:8px">时空场景图生成（ST-SGG）旨在建模视频帧中对象及其随时间演变的关系，为下游推理任务如视频字幕生成和视觉问答提供可解释的表示。尽管最近在DETR风格的一阶段ST-SGG模型方面取得了进展，但它们仍然存在几个关键限制。首先，虽然这些模型依赖于基于注意力的学习查询作为核心组件，但这些学习查询在语义上是未受过训练的，并且实例无关地初始化。其次，这些模型完全依赖于单模态视觉特征进行谓词分类。为了解决这些挑战，我们提出了一种VLM辅助的一阶段ST-SGG框架，将视觉语言模型（VLM）的常识推理能力整合到ST-SGG管道中。首先，我们引入了双源查询初始化策略，将关注什么与关注哪里分离，实现语义导向的什么-在哪里推理。此外，我们提出了一种多模态特征库，将从VLM中提取的视觉、文本和空间线索融合起来，以提高谓词分类的准确性。在Action Genome数据集上的广泛实验表明，我们的方法达到了最先进的性能，验证了将VLM辅助的语义先验和多模态特征整合到ST-SGG中的有效性。我们将代码发布在https://github.com/LUNAProject22/VOST。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve spatio-temporal scene graph generation (ST-SGG) by addressing limitations in DETR-style models, such as semantically uninformed learnable queries and reliance on unimodal visual features. VOST-SGG integrates vision-language model (VLM) capabilities to enhance query initialization and predicate classification through a dual-source query strategy and a multi-modal feature bank. Experiments show that VOST-SGG outperforms existing methods on the Action Genome dataset, validating the benefits of VLM-aided semantic priors and multi-modal features for ST-SGG.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决DETR风格模型中的限制，如语义不明确的学习查询和依赖单一视觉特征，来改进时空场景图生成（ST-SGG）。VOST-SGG通过引入双源查询初始化策略和多模态特征库，将视觉语言模型（VLM）的能力整合到ST-SGG管道中，以增强查询初始化和谓词分类。实验表明，VOST-SGG在Action Genome数据集上的表现优于现有方法，验证了语义先验和多模态特征对ST-SGG的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning</div>
<div class="meta-line">Authors: Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando</div>
<div class="meta-line">First: 2025-12-05T08:15:49+00:00 · Latest: 2025-12-05T08:15:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05513v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05513v1">PDF</a> · <a href="https://github.com/LUNAProject22/Know-Show">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Video-Language Models (Video-LMs) have achieved impressive progress in multimodal understanding, yet their reasoning remains weakly grounded in space and time. We present Know-Show, a new benchmark designed to evaluate spatio-temporal grounded reasoning, the ability of a model to reason about actions and their semantics while simultaneously grounding its inferences in visual and temporal evidence. Know-Show unifies reasoning and localization within a single evaluation framework consisting of five complementary scenarios across spatial (person, object, person-object, and hand-object) and temporal dimensions. Built from Charades, Action Genome, and Ego4D with 2.5K human-authored questions, the benchmark exposes significant gaps between current Video-LMs and human reasoning. To bridge this gap, we propose GRAM, a training-free plug-in that augments Video-LMs with fine-grained grounding through attention-based video token selection and explicit timestamp encoding. Extensive experiments across open and closed Video-LMs (Qwen, VideoLLaVA, GPT-4o, and Gemini, etc.) reveal that existing models struggle to &quot;show what they know&quot; and vice versa, especially in fine-grained hand-object interactions. Know-Show establishes a unified standard for assessing grounded reasoning in video-language understanding and provides insights toward developing interpretable and reliable multimodal reasoning systems. We will release the code at https://github.com/LUNAProject22/Know-Show.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知行合一：视频语言模型在时空定位推理上的基准测试</div>
<div class="mono" style="margin-top:8px">大型视频语言模型（Video-LMs）在多模态理解方面取得了显著进展，但其推理在空间和时间上的定位仍然很弱。我们提出了知行合一，这是一个新的基准测试，旨在评估时空定位推理能力，即模型在同时将推理与视觉和时间证据联系起来的情况下，对动作及其语义进行推理的能力。知行合一在一个包含五个互补场景的单一评估框架中统一了推理和定位，这些场景跨越了空间（人物、物体、人物-物体和手-物体）和时间维度。该基准测试基于Charades、Action Genome和Ego4D，包含2500个人工撰写的问答，揭示了当前Video-LMs与人类推理之间的显著差距。为了弥合这一差距，我们提出了GRAM，这是一种无需训练的插件，通过基于注意力的视频标记选择和显式的时间戳编码，为Video-LMs增加细粒度的定位。广泛的实验表明，现有的模型在“展示它们所知道的”和“理解它们所展示的”方面都存在困难，尤其是在细粒度的手-物体交互方面。知行合一为评估视频语言理解中的定位推理建立了一个统一的标准，并为开发可解释和可靠的多模态推理系统提供了见解。我们将在https://github.com/LUNAProject22/Know-Show/发布代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to evaluate the spatio-temporal grounded reasoning ability of Video-Language Models (Video-LMs) by introducing Know-Show, a new benchmark. The method involves five scenarios that test reasoning and localization in both spatial and temporal dimensions. Experiments show that current Video-LMs perform poorly in fine-grained hand-object interactions, indicating a significant gap between model performance and human reasoning. GRAM, a training-free plug-in, is proposed to improve grounding through attention-based video token selection and explicit timestamp encoding. The benchmark provides a unified standard for assessing grounded reasoning in video-language understanding and highlights areas for improvement in multimodal reasoning systems.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入Know-Show新基准来评估Video-Language Models (Video-LMs)的空间-时间定位推理能力。方法包括五个场景，测试推理和定位在空间和时间维度上的表现。实验表明，当前的Video-LMs在精细的手-物体交互方面存在困难，显示出推理能力的显著差距。提出了GRAM，一种无需训练的插件，通过基于注意力的视频标记选择和显式的时间戳编码来改善定位。该基准为评估视频-语言理解中的定位推理提供了一个统一的标准，并指出了多模态推理系统改进的领域。</div>
</details>
</div>
<div class="card">
<div class="title">InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity</div>
<div class="meta-line">Authors: Haoming Wang, Qiyao Xue, Wei Gao</div>
<div class="meta-line">First: 2025-11-22T22:05:39+00:00 · Latest: 2025-12-05T07:59:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18200v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18200v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiniBench：无限基准测试用于视觉空间推理，具有可定制的场景复杂度</div>
<div class="mono" style="margin-top:8px">现代视觉-语言模型（VLMs）被期望具有在不同场景复杂度下进行空间推理的能力，但由于缺乏既能多样化又能扩展且完全可定制的基准测试，评估这些能力是困难的。现有的基准测试在场景复杂度的可定制性方面有限，并且无法在不同的空间条件下隔离和分析特定的VLM故障模式。为了解决这一差距，而不是分别呈现不同场景复杂度的基准测试，本文提出了一种名为InfiniBench的全自动、可定制且用户友好的基准测试生成器，它可以合成理论上无限多样的3D场景，并通过参数化控制场景复杂度。InfiniBench独特地将自然语言中的场景描述转化为具有复杂且物理上合理的3D布局的逼真视频。这通过三个关键创新实现：1）基于LLM的代理框架，迭代细化从场景描述中获得的程序化场景约束；2）灵活的基于集群的布局优化器，生成先前无法通过程序化方法处理的密集和拥挤的场景；3）任务感知的摄像机轨迹优化方法，将场景渲染为VLM输入的具有全面物体覆盖的视频。实验表明，InfiniBench在提示保真度和物理合理性方面优于最先进的程序化和基于LLM的3D生成方法，尤其是在高复杂度场景中。我们进一步展示了InfiniBench的实用性，通过生成代表性的空间推理任务基准测试，包括测量、视角转换和时空跟踪。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">InfiniBench is a benchmark generator designed to evaluate visual spatial reasoning abilities of vision-language models (VLMs) across diverse and customizable scene complexities. It uses a LLM-based agentic framework, a flexible cluster-based layout optimizer, and a task-aware camera trajectory optimization method to synthesize photo-realistic 3D scenes. Experiments show that InfiniBench outperforms existing methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios, and is useful for generating benchmarks for spatial reasoning tasks such as measurement, perspective-taking, and spatiotemporal tracking.</div>
<div class="mono" style="margin-top:8px">InfiniBench 是一个用于评估视觉语言模型在不同多样性和可定制场景复杂性下空间推理能力的基准生成器。它通过基于LLM的代理框架、灵活的集群布局优化器和任务感知的摄像机轨迹优化方法来合成逼真的3D场景。实验表明，InfiniBench 在提示保真度和物理合理性方面优于现有方法，尤其是在高复杂性场景中。此外，它还通过生成测量、视角转换和时空跟踪等空间推理任务的基准来展示其实用性。</div>
</details>
</div>
<div class="card">
<div class="title">iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning</div>
<div class="meta-line">Authors: Manyi Yao, Bingbing Zhuang, Sparsh Garg, Amit Roy-Chowdhury, Christian Shelton, Manmohan Chandraker, Abhishek Aich</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-23T20:25:53+00:00 · Latest: 2025-12-05T07:58:34+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19552v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.19552v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues -- object pose, lane positions, and object trajectories -- which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM&#x27;s outputs and provide accurate reasoning. Evaluations on four public dash-cam video benchmarks show that iFinder&#x27;s proposed grounding with domain-specific cues, especially object orientation and global context, significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>iFinder：面向后置行车记录视频分析的结构化零样本视觉基础大语言模型对接</div>
<div class="mono" style="margin-top:8px">将大语言模型（LLMs）对接到如后置行车记录视频分析等特定领域任务中具有挑战性，因为它们是通用训练的，缺乏结构化的归纳偏置。由于此类分析通常仅依赖视觉模态（即没有LiDAR、GPS等），现有的基于视频的视觉-语言模型（V-VLMs）在空间推理、因果推理和事件解释方面存在困难。为此，我们提出了iFinder，这是一种结构化语义对接框架，通过将行车记录视频转换为层次化、可解释的数据结构来解耦感知与推理。iFinder 作为模块化、无需训练的流水线，利用预训练的视觉模型提取关键线索——物体姿态、车道位置和物体轨迹，并将这些线索按层次组织成帧级和视频级结构。结合三块提示策略，它使LLM能够逐步、基于上下文地进行推理，以细化V-VLM的输出并提供准确的推理。在四个公开的后置行车记录视频基准测试上的评估表明，iFinder 提出的基于特定领域线索的对接，尤其是物体方向和全局上下文，显著优于端到端的V-VLMs，在四个零样本驾驶基准测试中，事故推理准确性提高了高达39%。通过使用驾驶领域特定的表示对接LLM，iFinder 提供了一种零样本、可解释且可靠的替代方案，用于后置行车记录视频理解，与端到端的V-VLMs相比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">iFinder is a structured semantic grounding framework designed to enhance zero-shot vision-based large language model (LLM) reasoning for post-hoc dash-cam video analysis. It decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure. iFinder uses pretrained vision models to extract critical cues such as object pose, lane positions, and object trajectories, which are organized into frame- and video-level structures. This approach, combined with a three-block prompting strategy, enables step-wise, grounded reasoning for LLMs to refine outputs from existing vision-language models. Experiments on four public dash-cam video benchmarks demonstrate that iFinder significantly outperforms end-to-end vision-language models, particularly in accident reasoning accuracy, with up to 39% gains.</div>
<div class="mono" style="margin-top:8px">iFinder 是一种结构化的语义接地框架，旨在增强大型语言模型（LLMs）在后处理分析驾驶记录仪视频时的零样本推理能力。它通过将视频转换为层次化、可解释的数据结构来分离感知和推理。实验结果表明，iFinder 在四个零样本驾驶基准测试中优于端到端的视觉-语言模型，特别是在事故推理准确性方面，最高提高了 39%。</div>
</details>
</div>
<div class="card">
<div class="title">V-CECE: Visual Counterfactual Explanations via Conceptual Edits</div>
<div class="meta-line">Authors: Nikolaos Spanos, Maria Lymperaiou, Giorgos Filandrianos, Konstantinos Thomas, Athanasios Voulodimos, Giorgos Stamou</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-20T07:53:06+00:00 · Latest: 2025-12-05T07:24:35+00:00</div>
<div class="meta-line">Comments: Accepted in NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16567v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.16567v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-CECE: 视觉概念编辑的可视化反事实解释</div>
<div class="mono" style="margin-top:8px">近期的黑盒反事实生成框架未能考虑所提编辑的语义内容，而主要依赖训练来指导生成过程。我们提出了一种新颖的、即插即用的黑盒反事实生成框架，该框架基于最优编辑的理论保证，逐步建议编辑以生成与人类水平相当的反事实解释，无需训练。该框架利用预训练的图像编辑扩散模型，并在不访问分类器内部结构的情况下运行，从而实现可解释的反事实生成过程。在我们的实验中，通过使用卷积神经网络（CNN）、视觉变换器（ViT）和大型视觉语言模型（LVLM）分类器，并通过全面的人类评估，展示了人类推理与神经模型行为之间的解释差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of existing black-box counterfactual generation frameworks that do not consider the semantic content of edits. It introduces V-CECE, a novel framework that suggests step-by-step edits based on theoretical guarantees, enabling the generation of human-level counterfactual explanations without training. The framework uses a pre-trained image editing diffusion model and operates without access to the classifier&#x27;s internals, providing an explainable counterfactual generation process. Experiments with CNN, ViT, and LVLM classifiers demonstrate the explanatory gap between human reasoning and neural model behavior, supported by human evaluations.</div>
<div class="mono" style="margin-top:8px">研究旨在解决现有黑盒反事实生成方法不考虑语义内容且依赖训练的局限性。提出的V-CECE框架基于理论保证建议逐步编辑，生成无需训练的人类级反事实解释。实验展示了该框架在不同分类器（包括卷积神经网络、视觉变换器和大型视觉语言模型）中解释人类推理与神经网络行为差异的有效性，通过人类评估验证了这一点。</div>
</details>
</div>
<div class="card">
<div class="title">Concept-based Explainable Data Mining with VLM for 3D Detection</div>
<div class="meta-line">Authors: Mai Tsujimoto</div>
<div class="meta-line">First: 2025-12-05T07:18:45+00:00 · Latest: 2025-12-05T07:18:45+00:00</div>
<div class="meta-line">Comments: 28 pages including appendix. Code: https://github.com/mm1129/concept_based_rare_detector_2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05482v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.05482v1">PDF</a> · <a href="https://github.com/mm1129/concept_based_rare_detector_2025">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于概念的可解释数据挖掘与VLM在3D检测中的应用</div>
<div class="mono" style="margin-top:8px">在自主驾驶系统中，基于点云数据的稀有物体检测仍然是一个具有挑战性的任务，尤其是在仅依赖点云数据的情况下。尽管视觉-语言模型（VLMs）在图像理解方面表现出强大的能力，但它们通过智能数据挖掘增强3D物体检测的潜力尚未得到充分探索。本文提出了一种新颖的跨模态框架，利用2D VLMs从驾驶场景中识别和挖掘稀有物体，从而提高3D物体检测性能。我们的方法将物体检测、语义特征提取、降维和多方面离群点检测等互补技术综合成一个系统、可解释的管道，系统地识别驾驶场景中的稀有但关键物体。通过结合孤立森林和基于t-SNE的离群点检测方法与基于概念的过滤，该框架有效地识别了具有语义意义的稀有物体。该方法的一个关键优势在于能够提取和标注针对性的稀有物体概念，如施工车辆、摩托车和障碍物。这大大减少了标注负担，并仅关注最有价值的训练样本。在nuScenes数据集上的实验表明，这种基于概念的数据挖掘策略在使用少量训练数据的情况下提高了3D物体检测模型的性能，特别是在拖车和自行车等具有挑战性的物体类别上，与相同数量的随机数据相比，表现尤为突出。这一发现对安全关键的自主系统中数据集的高效整理具有重要意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of detecting rare objects in autonomous driving systems using a novel cross-modal framework that integrates 2D Vision-Language Models (VLMs) with techniques like object detection, semantic feature extraction, and outlier detection. The framework effectively identifies semantically meaningful rare objects, reducing annotation burden and improving 3D object detection performance, especially for challenging categories like trailers and bicycles. Experiments on the nuScenes dataset show significant performance enhancements with a fraction of the training data compared to random data.</div>
<div class="mono" style="margin-top:8px">本文针对自主驾驶系统中基于3D点云数据的稀有物体检测难题，提出了一种结合2D视觉语言模型的新型跨模态框架，以识别和挖掘稀有物体，提升3D物体检测性能。该框架综合了目标检测、语义特征提取和离群点检测等技术，系统地识别关键的稀有物体。实验结果表明，这种概念引导的数据挖掘策略在nuScenes数据集上提升了3D物体检测性能，特别是在拖车和自行车等具有挑战性的类别上，使用更少的训练数据相比随机数据。</div>
</details>
</div>
<div class="card">
<div class="title">Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</div>
<div class="meta-line">Authors: Tao Lin, Yilei Zhong, Yuxin Du, Jingjing Zhang, Jiting Liu, Yinxinyu Chen, Encheng Gu, Ziyan Liu, Hongyi Cai, Yanwen Zou, Lixing Zou, Zhaoye Zhou, Gen Li, Bo Zhao</div>
<div class="meta-line">First: 2025-11-06T17:07:49+00:00 · Latest: 2025-12-05T06:57:07+00:00</div>
<div class="meta-line">Comments: Github: https://github.com/MINT-SJTU/Evo-1</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04555v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.04555v2">PDF</a> · <a href="https://github.com/MINT-SJTU/Evo-1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Evo-1：轻量级视觉-语言-行动模型，保留语义对齐</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型已成为一种强大的框架，能够统一感知、语言和控制，使机器人能够通过多模态理解执行多种任务。然而，当前的VLA模型通常包含大量参数，并且高度依赖大规模机器人数据预训练，导致训练时计算成本高昂，部署时实时推理能力有限。此外，大多数训练范式往往会降低视觉-语言主干的感知表示，导致过拟合和下游任务泛化能力差。在本工作中，我们提出了Evo-1，这是一种轻量级的VLA模型，能够减少计算量并提高部署效率，同时保持强大的性能，无需使用机器人数据进行预训练。Evo-1基于原生多模态视觉-语言模型（VLM），结合了一种新颖的跨模态扩散变换器以及优化的集成模块，共同形成了有效的架构。我们进一步引入了一种两阶段训练范式，逐步将行动与感知对齐，保留了VLM的表示。值得注意的是，仅包含0.77亿个参数的Evo-1在Meta-World和RoboTwin套件上达到了最先进的结果，分别超越了之前最佳模型12.4%和6.9%，并在LIBERO上也取得了竞争力的结果，达到94.8%。在实际世界评估中，Evo-1以高推理频率和低内存开销实现了78%的成功率，超越了所有基线方法。我们发布了代码、数据和模型权重，以促进轻量级和高效VLA模型的未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evo-1 is a lightweight VLA model that reduces computational costs and improves deployability without pretraining on robot data. It uses a cross-modulated diffusion transformer and an optimized integration module, along with a two-stage training paradigm to preserve perceptual representations. Evo-1 achieves state-of-the-art results on Meta-World and RoboTwin suite, and a competitive result on LIBERO, with a 78% success rate in real-world evaluations.</div>
<div class="mono" style="margin-top:8px">Evo-1 是一种轻量级的 VLA 模型，减少了计算成本并提高了部署效率，同时保持了强大的性能。它基于一个原生的多模态视觉-语言模型，并结合了一个交叉调制扩散变换器和一个优化的集成模块。Evo-1 在 Meta-World 和 RoboTwin 任务集上取得了最先进的结果，分别超越了之前的模型 12.4% 和 6.9%，并在 LIBERO 上也达到了 94.8% 的成功率。在实际应用中，Evo-1 达到了 78% 的成功率，具有高推理频率和低内存开销。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251208_0317.html">20251208_0317</a>
<a href="archive/20251207_0317.html">20251207_0317</a>
<a href="archive/20251206_0318.html">20251206_0318</a>
<a href="archive/20251205_0320.html">20251205_0320</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
