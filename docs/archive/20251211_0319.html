<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-11 03:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251211_0319</div>
    <div class="row"><div class="card">
<div class="title">Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration</div>
<div class="meta-line">Authors: Jin Hyeon Kim, Paul Hyunbin Cho, Claire Kim, Jaewon Min, Jaeeun Lee, Jihye Park, Yeji Choi, Seungryong Kim</div>
<div class="meta-line">First: 2025-12-09T18:56:54+00:00 · Latest: 2025-12-09T18:56:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08922v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08922v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-Aware Image Restoration (TAIR) aims to recover high-quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong generative priors for general image restoration, they often produce text hallucinations in text-centric tasks due to the absence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that integrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an iterative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermediate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine-grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一扩散变换器用于高保真文本感知图像恢复</div>
<div class="mono" style="margin-top:8px">文本感知图像恢复（TAIR）旨在从包含退化文本内容的低质量输入中恢复高质量图像。虽然扩散模型为通用图像恢复提供了强大的生成先验知识，但在以文本为中心的任务中，它们往往会由于缺乏显式的语言知识而产生文本幻觉。为了解决这个问题，我们提出了一种统一的文本恢复框架UniT，该框架以迭代方式结合了扩散变换器（DiT）、视觉语言模型（VLM）和文本检测模块（TSM），以实现高保真文本恢复。在UniT中，VLM从退化图像中提取文本内容，提供显式的文本指导。同时，TSM在去噪的每一步生成中间的OCR预测，使VLM能够在去噪过程中逐步细化其指导。最后，DiT骨干利用其强大的表征能力，利用这些线索恢复细粒度的文本内容，同时有效抑制文本幻觉。在SA-Text和Real-Text基准测试上的实验表明，UniT能够忠实恢复退化文本，显著减少幻觉，并在TAIR任务中实现最先进的端到端F1分数性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes UniT, a unified text restoration framework for high-fidelity image restoration, integrating a Diffusion Transformer, a Vision-Language Model, and a Text Spotting Module. This framework iteratively refines text content by extracting textual guidance from degraded images and generating intermediate OCR predictions, which helps suppress text hallucinations. Experiments show that UniT outperforms existing methods in terms of reconstructing degraded text and reducing hallucinations, achieving state-of-the-art F1-score performance in the TAIR task.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种名为UniT的统一框架，结合了扩散变换器、视觉语言模型和文本检测模块，通过迭代提取文本信息和生成中间OCR预测来细化退化图像中的文本内容，从而减少文本幻觉。实验表明，UniT在忠实重建文本和减少幻觉方面优于现有方法，并实现了最先进的F1分数性能。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Evolving 3D Scene Generation from a Single Image</div>
<div class="meta-line">Authors: Kaizhi Zheng, Yue Fan, Jing Gu, Zishuo Xu, Xuehai He, Xin Eric Wang</div>
<div class="meta-line">First: 2025-12-09T18:44:21+00:00 · Latest: 2025-12-09T18:44:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08905v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08905v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从单张图像自演化生成3D场景</div>
<div class="mono" style="margin-top:8px">从单张图像生成高质量、纹理化的3D场景仍然是视觉和图形学中的一个基本挑战。最近的图像到3D生成器可以从单个视角恢复合理的几何结构，但它们以对象为中心的训练限制了其在复杂、大规模场景中的泛化能力，这些场景需要忠实的结构和纹理。我们提出了EvoScene，这是一种无需训练的自演化框架，可以逐步从单张图像中重建完整的3D场景。关键思想是结合现有模型的互补优势：3D生成模型的几何推理能力和视频生成模型的视觉知识。通过三个迭代阶段——空间先验初始化、视觉引导的3D场景网格生成和空间引导的新视角生成——EvoScene 在2D和3D领域之间交替，逐步提高结构和外观。在多种场景上的实验表明，与强大的基线相比，EvoScene 在几何稳定性、视图一致的纹理以及未见区域的完成方面表现出优越性，生成了可以直接用于实际应用的3D网格。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to generate high-quality 3D scenes from a single image, addressing the limitations of existing object-centric models in handling complex scenes. EvoScene, a self-evolving framework, combines geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three stages, it progressively reconstructs complete 3D scenes, improving both structure and appearance. Experiments show that EvoScene outperforms strong baselines in geometric stability, view-consistent textures, and unseen-region completion, producing ready-to-use 3D meshes.</div>
<div class="mono" style="margin-top:8px">研究旨在从单张图像生成高质量的3D场景，解决现有以对象为中心的模型在处理复杂场景时的局限性。EvoScene是一种自我进化的框架，结合了3D生成模型的几何推理和视频生成模型的视觉知识。通过三个阶段，它逐步重建完整的3D场景，实现了优于强基线的几何稳定性、视图一致的纹理和未见区域的完成。</div>
</details>
</div>
<div class="card">
<div class="title">TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Shima Imani, Seungwhan Moon, Lambert Mathias, Lu Zhang, Babak Damavandi</div>
<div class="meta-line">First: 2025-12-05T18:40:18+00:00 · Latest: 2025-12-09T18:19:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05943v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.05943v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRACE：一种分析和增强视觉语言模型逐步推理的框架</div>
<div class="mono" style="margin-top:8px">可靠地进行数学和科学推理仍然是大型视觉语言模型面临的开放挑战。标准的最终答案评估往往掩盖了推理错误，允许无声失败持续存在。为了解决这一差距，我们引入了TRACE，一种透明推理和一致性评估框架，该框架诊断推理轨迹而非仅关注最终结果。核心上，TRACE 利用辅助推理集，这是一种分解复杂问题的紧凑子问题答案对，通过基于一致性的度量评估中间步骤，并暴露标准评估中忽略的失败。我们的实验表明，辅助推理集（ARS）的一致性与最终答案的正确性相关，并有助于定位失败出现的推理步骤，提供可操作的信号以改进模型。此外，TRACE 定义了置信区域，区分可靠和不可靠的推理路径，支持有效的过滤、调试和模型细化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the reliability of mathematical and scientific reasoning in large vision-language models by addressing the limitations of standard final-answer evaluation. TRACE, a framework for Transparent Reasoning And Consistency Evaluation, introduces Auxiliary Reasoning Sets to decompose complex problems and evaluate intermediate steps. Experiments show that consistency across these sets correlates with final-answer correctness and helps identify specific reasoning steps where failures occur, providing actionable insights for model improvement. Additionally, TRACE defines confidence regions to distinguish reliable from unreliable reasoning paths, aiding in effective filtering and debugging.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决标准最终答案评估的局限性，提高大型视觉-语言模型在数学和科学推理方面的可靠性。TRACE，一种透明推理和一致性评估框架，引入了辅助推理集来分解复杂问题并评估中间步骤。实验表明，这些集中的一致性与正确的最终答案相关，并有助于识别推理步骤中的失败点，提供改进模型的行动指南。此外，TRACE 定义了置信区域来区分可靠和不可靠的推理路径，有助于有效的过滤、调试和模型优化。</div>
</details>
</div>
<div class="card">
<div class="title">SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing</div>
<div class="meta-line">Authors: Aysim Toker, Andreea-Maria Oncescu, Roy Miles, Ismail Elezi, Jiankang Deng</div>
<div class="meta-line">First: 2025-12-09T18:15:43+00:00 · Latest: 2025-12-09T18:15:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08881v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08881v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model&#x27;s ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SATGround：一种针对遥感领域视觉定位的空间感知方法</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）正在成为遥感领域强大的通用工具，能够跨多种任务整合信息，并通过聊天界面实现灵活的指令式交互。在本文中，我们通过提出一种新颖的结构化定位机制，增强了基于VLM的卫星图像视觉定位。我们的方法包括在多样化的指令遵循任务上微调预训练的VLM，并通过专门的控制标记接口连接一个专用的定位模块。该方法促进了语言和空间信息的联合推理，显著增强了模型在复杂卫星场景中精确定位物体的能力。我们在几个遥感基准上评估了我们的框架，始终优于现有方法，包括在视觉定位上的24.8%的相对改进。我们的结果突显了将结构化空间推理集成到VLM中的好处，为更可靠的遥感数据分析铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve visual grounding in satellite imagery using vision-language models (VLMs) by introducing a structured localization mechanism. This method involves fine-tuning a pretrained VLM on various instruction-following tasks and integrating a specialized grounding module through control tokens. The approach enhances joint reasoning over language and spatial information, leading to significant improvements in object localization accuracy, with a 24.8% relative improvement on visual grounding benchmarks compared to previous methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入结构化定位机制来增强卫星图像中的视觉定位，使用视觉语言模型（VLMs）。该方法包括对预训练的VLM进行各种指令跟随任务的微调，并通过控制标记集成专门的定位模块。该方法提高了语言和空间信息的联合推理能力，相比之前的方法在遥感基准测试中视觉定位性能提高了24.8%。</div>
</details>
</div>
<div class="card">
<div class="title">The Missing Point in Vision Transformers for Universal Image Segmentation</div>
<div class="meta-line">Authors: Sajjad Shahabodini, Mobina Mansoori, Farnoush Bayatmakou, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi</div>
<div class="meta-line">First: 2025-05-26T10:29:13+00:00 · Latest: 2025-12-09T17:56:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19795v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.19795v2">PDF</a> · <a href="https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P">Code1</a> · <a href="https://github.com/sajjad-sh33/ViT-P">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉变换器在通用图像分割中的缺失点</div>
<div class="mono" style="margin-top:8px">图像分割仍然是计算机视觉中的一个挑战性任务，需要稳健的掩码生成和精确的分类。基于掩码的方法通过捕捉全局上下文来生成高质量的掩码。然而，在存在模糊边界和类别分布不平衡的情况下，准确地对这些掩码进行分类仍然是一个开放的挑战。在本文中，我们引入了ViT-P，这是一种新颖的两阶段分割框架，将掩码生成与分类脱钩。第一阶段使用提案生成器生成无类别的掩码提案，而第二阶段则利用基于视觉变换器(ViT)的基于点的分类模型，通过关注掩码的中心点来细化预测。ViT-P 作为一种无需预训练的适配器，允许将各种预训练的视觉变换器无缝集成到其架构中，确保其对密集预测任务的适应性。此外，我们证明粗略和边界框注释可以有效增强分类，而无需在精细注释数据集上进行额外训练，从而降低注释成本并保持强大的性能。在COCO、ADE20K和Cityscapes数据集上的广泛实验验证了ViT-P的有效性，分别在ADE20K全景分割中达到54.0 PQ，在Cityscapes语义分割中达到87.4 mIoU，在ADE20K语义分割中达到63.6 mIoU。代码和预训练模型可在：https://github.com/sajjad-sh33/ViT-P 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of accurate mask classification in image segmentation by proposing ViT-P, a two-stage framework that decouples mask generation from classification. The first stage generates class-agnostic mask proposals, and the second stage uses a point-based classification model based on Vision Transformer to refine predictions by focusing on mask central points. ViT-P achieves state-of-the-art results on ADE20K panoptic segmentation, Cityscapes semantic segmentation, and ADE20K semantic segmentation without requiring additional pre-training, demonstrating its effectiveness and adaptability to dense prediction tasks.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种两阶段框架ViT-P，通过将掩码生成与分类分离来解决图像分割中准确掩码分类的挑战。第一阶段生成无类别的掩码提案，第二阶段使用基于Vision Transformer的点分类模型来细化预测。ViT-P在ADE20K全景分割（54.0 PQ）、Cityscapes语义分割（87.4 mIoU）和ADE20K语义分割（63.6 mIoU）上取得了最先进的结果，且无需额外预训练，展示了其在密集预测任务中的有效性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference</div>
<div class="meta-line">Authors: Amit Bendkhale</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-09T17:52:57+00:00 · Latest: 2025-12-09T17:52:57+00:00</div>
<div class="meta-line">Comments: 6 pages, 3 figures. Code and data: https://github.com/Amiton7/Tri-Bench. Accepted to the AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08860v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08860v1">PDF</a> · <a href="https://github.com/Amiton7/Tri-Bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Tri-Bench：在相机倾斜和物体干扰下对视觉-语言模型在空间推理可靠性的压力测试</div>
<div class="mono" style="margin-top:8px">可验证的几何推理是值得信赖和可控的代理人工智能的关键组成部分。尽管具有令人印象深刻的性能，但在现实场景变化下，视觉-语言模型（VLMs）经常失败。我们提出了Tri-Bench，这是一个紧凑的基准测试，专注于平面三角形问题，以隔离相对几何推理，同时强调两个关键部署因素：相机姿态（平面 vs. 倾斜）和通过物体干扰（10种日常生活中的物体）的场景上下文。为了测试可验证性和可控性，我们使用一个单一的固定提示来评估四个最近的VLMs，该提示的护栏明确描述了一个周围的正方形边界，通过齐次变换使正确答案成为可能。我们评估了六个简单的任务，涉及二进制和连续目标，观察到相对于3D地面真实值的整体准确性较低，平均约为69%（最佳约为75%，最差约为64%）。同样的响应在图像平面的二维投影中与2D投影的准确性更加一致，平均准确性约为72%。所有四个VLMs在识别少数形状类别（等边、等腰、直角三角形）时都表现一致不佳，准确率降至约0%。此外，总体VLM准确性在相机倾斜下下降了约4.1%。这表明模型未能正确利用提示中提供的明确框架参考提示，而是默认使用2D图像平面线索。最后，我们发现物体干扰对VLM准确性没有显著影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to evaluate the reliability of Vision-Language Models (VLMs) in geometric reasoning under realistic scene changes. Tri-Bench, a benchmark focusing on planar triangle problems, tests VLMs under two critical factors: camera pose and object interference. Four recent VLMs were evaluated using a fixed prompt with a guardrail, and results showed an average accuracy of ~69% with respect to 3D ground truth, dropping to ~0% for recognizing minority shape classes under camera tilt. Object interference had no significant impact on accuracy.</div>
<div class="mono" style="margin-top:8px">研究旨在评估Vision-Language模型在现实场景变化下的几何推理可靠性。Tri-Bench基准专注于平面三角形问题，测试模型在相机倾斜和物体干扰下的表现。使用固定提示和描述方形边界的边界提示，模型在3D地面真实度下的平均准确率为~69%，在2D图像平面上则为~72%。模型在少数形状类别上表现不佳，几乎无法识别，且在相机倾斜时性能下降了~4.1%，表明模型依赖于2D图像平面线索而非提供的3D框架参考提示。物体干扰对模型的准确性没有显著影响。</div>
</details>
</div>
<div class="card">
<div class="title">InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</div>
<div class="meta-line">Authors: Hongyuan Tao, Bencheng Liao, Shaoyu Chen, Haoran Yin, Qian Zhang, Wenyu Liu, Xinggang Wang</div>
<div class="meta-line">First: 2025-12-09T17:18:32+00:00 · Latest: 2025-12-09T17:18:32+00:00</div>
<div class="meta-line">Comments: 16 pages, 8 figures, conference or other essential info</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08829v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08829v1">PDF</a> · <a href="https://github.com/hustvl/InfiniteVL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiniteVL：结合线性和稀疏注意机制的高效无限输入视觉-语言模型</div>
<div class="mono" style="margin-top:8px">窗口注意和线性注意是缓解视觉-语言模型（VLM）中二次复杂度和不断增长的KV缓存的两种主要策略。然而，我们发现基于窗口的VLM在序列长度超过窗口大小时会性能下降，而线性注意在OCR和文档理解等信息密集型任务上表现不佳。为克服这些限制，我们提出了InfiniteVL，这是一种结合滑动窗口注意（SWA）和门控DeltaNet的线性复杂度VLM架构。为了在资源受限的情况下实现竞争性的多模态性能，我们设计了三阶段训练策略，包括蒸馏预训练、指令调优和长序列SFT。令人惊讶的是，使用领先VLM所需训练数据的不到2%，InfiniteVL不仅大幅优于之前的线性复杂度VLM，还与领先的基于Transformer的VLM性能相当，同时展示了有效的长期记忆保留。与通过FlashAttention-2加速的类似规模的基于Transformer的VLM相比，InfiniteVL实现了超过3.6倍的推理加速，同时保持了恒定的延迟和内存占用。在流式视频理解场景中，它保持了稳定的每秒24帧实时预填充速度，同时保留了长期记忆缓存。代码和模型可在https://github.com/hustvl/InfiniteVL获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">InfiniteVL is a linear-complexity Vision-Language Model that combines sliding window attention with Gated DeltaNet to address the limitations of window-based and linear attention approaches. It employs a three-stage training strategy and achieves performance comparable to leading Transformer-based models while offering significant inference speedup and maintaining long-term memory retention. Using less than 2% of the training data, InfiniteVL outperforms previous linear-complexity models and matches the performance of state-of-the-art Transformer-based models.</div>
<div class="mono" style="margin-top:8px">InfiniteVL 是一种结合滑动窗口注意力和 Gated DeltaNet 的线性复杂度视觉-语言模型，旨在克服窗口基模型和线性注意力方法的局限性。它采用三阶段训练策略，并且使用不到 2% 的训练数据就达到了与领先 Transformer 基础模型相当的性能。InfiniteVL 在推理速度上比类似大小的 Transformer 基础模型快 3.6 倍，同时保持了恒定的延迟和内存占用。在流式视频理解场景中，它还能保持每秒 24 帧的实时预填充速度，同时保留长期记忆缓存。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning</div>
<div class="meta-line">Authors: Yi Zhang, Chun-Wun Cheng, Junyi He, Ke Yu, Yushun Tang, Carola-Bibiane Schönlieb, Zhihai He, Angelica I. Aviles-Rivero</div>
<div class="meta-line">First: 2025-12-09T17:12:22+00:00 · Latest: 2025-12-09T17:12:22+00:00</div>
<div class="meta-line">Comments: Accepted in IEEE Transactions on Multimedia (TMM)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08820v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需训练的双超曲面适配器以实现更好的跨模态推理</div>
<div class="mono" style="margin-top:8px">近期在视觉-语言模型（VLMs）方面的研究显著提升了我们在跨模态推理方面的能力。然而，现有方法在领域变化时性能会下降，或者需要大量的计算资源进行新领域的微调。为了解决这个问题，我们开发了一种新的大型视觉-语言模型的适应方法，称为“无需训练的双超曲面适配器”（T-DHA）。我们以超曲面空间而不是传统的欧几里得空间来表征视觉-语言之间的语义概念关系，这种关系通常具有层次树结构。超曲面空间的体积增长呈指数级，而欧几里得空间的增长呈多项式级。我们发现，这种独特性质特别适合使用Poincaré球模型嵌入层次数据结构，从而显著提高了表示能力和区分能力。结合负学习，它可以在更少的特征维度下提供更准确和稳健的分类。我们在各种数据集上的广泛实验结果表明，T-DHA方法在少样本图像识别和领域泛化任务中显著优于现有最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the performance of Vision-Language Models (VLMs) in cross-modal reasoning, especially in new domains without requiring fine-tuning. The authors introduce Training-free Dual Hyperbolic Adapters (T-DHA), which embed hierarchical data structures in hyperbolic space, offering better representation and discrimination power. Experiments show that T-DHA outperforms existing methods in few-shot image recognition and domain generalization tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决领域变化导致的性能下降和需要大量计算资源进行微调的问题，提高视觉-语言模型（VLMs）的跨模态推理能力。作者提出了一种名为Training-free Dual Hyperbolic Adapters (T-DHA)的方法，该方法在双曲空间中嵌入层次数据结构，相较于传统的欧几里得空间，能够提供更好的表示能力和区分能力。该方法在少量样本图像识别和领域泛化任务中表现出色，优于现有最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images</div>
<div class="meta-line">Authors: Kaiyu Li, Shengqi Zhang, Yupeng Deng, Zhi Wang, Deyu Meng, Xiangyong Cao</div>
<div class="meta-line">First: 2025-12-09T15:42:28+00:00 · Latest: 2025-12-09T15:42:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08730v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08730v1">PDF</a> · <a href="https://github.com/earth-insights/SegEarth-OV-3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3&#x27;s semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SegEarth-OV3：探索SAM 3在遥感图像中的开放词汇语义分割</div>
<div class="mono" style="margin-top:8px">大多数现有的训练免费开放词汇语义分割（OVSS）方法基于CLIP。尽管这些方法取得了进展，但在精确定位方面仍面临挑战，或者需要复杂的管道将单独的模块结合起来，特别是在遥感场景中，存在大量密集且小型的目标。最近，提出了Segment Anything Model 3（SAM 3），在一个可提示框架中统一了分割和识别。在本文中，我们初步探索了在没有任何训练的情况下将SAM 3应用于遥感OVSS任务。首先，我们实现了一种掩码融合策略，将SAM 3的语义分割头和Transformer解码器（实例头）的输出结合起来。这使我们能够利用两个头的优点，以更好地覆盖土地。其次，我们利用存在头的出现分数来过滤掉场景中不存在的类别，减少由于地理空间场景中的大规模词汇和像素级处理引起的假阳性。我们在广泛的遥感数据集上评估了我们的方法。实验表明，这种简单的适应取得了令人鼓舞的性能，展示了SAM 3在遥感OVSS中的潜力。我们的代码发布在https://github.com/earth-insights/SegEarth-OV-3。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the application of Segment Anything Model 3 (SAM 3) for training-free Open-Vocabulary Semantic Segmentation (OVSS) in remote sensing images. It introduces a mask fusion strategy combining SAM 3&#x27;s semantic segmentation head and Transformer decoder, and uses the presence score to filter out non-existent categories, reducing false positives. Experiments on various remote sensing datasets show promising results, indicating SAM 3&#x27;s potential for this task.</div>
<div class="mono" style="margin-top:8px">本文探索了使用Segment Anything Model 3 (SAM 3) 在遥感图像中进行无训练的Open-Vocabulary Semantic Segmentation (OVSS)。引入了一种掩码融合策略，结合SAM 3的语义分割头和Transformer解码器，并利用存在分数过滤掉不存在的类别，减少假阳性。在多种遥感数据集上的实验显示了有希望的结果，表明SAM 3在该任务中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</div>
<div class="meta-line">Authors: Yuhan Liu, Lianhui Qin, Shengjie Wang</div>
<div class="meta-line">First: 2025-10-23T17:59:21+00:00 · Latest: 2025-12-09T15:09:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20812v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.20812v3">PDF</a> · <a href="https://github.com/Tinaliu0123/speculative-verdict">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小草图，大裁决：基于推测的信息密集型视觉推理</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）在多模态理解方面取得了显著进展，但在处理密集交织了文本注释和细粒度图形元素的信息密集型图像时，它们面临挑战。主要挑战在于在密集布局中精确定位关键线索以及进行多跳推理以整合分散的证据。我们提出了一种名为推测裁决（SV）的无训练框架，该框架受到推测解码的启发，结合了多个轻量级草图专家和一个大型裁决模型。在草图阶段，小型VLMs作为草图专家生成提供多样化定位候选的推理路径；在裁决阶段，强大的VLM综合这些路径以生成最终答案，同时降低计算成本并恢复正确答案。为了进一步提高效率和准确性，SV引入了一种共识专家选择机制，仅将高一致性的推理路径转发到裁决阶段。实验证明，SV在InfographicVQA、ChartMuseum、ChartQAPro和HR-Bench 4K等具有挑战性的信息密集型和高分辨率视觉问答基准测试中取得了持续的改进。通过综合多个部分准确推理路径中的正确见解，SV在错误纠正和成本效率方面优于大型专有模型或训练管道。代码可在https://github.com/Tinaliu0123/speculative-verdict/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of visual reasoning in information-intensive images, where large VLMs struggle due to dense layouts and multi-hop reasoning. It introduces Speculative Verdict (SV), a training-free framework that uses lightweight draft experts to generate diverse localization candidates and a strong VLM to synthesize these paths for the final answer. SV includes a consensus expert selection mechanism to forward only high-agreement paths, achieving consistent gains on benchmarks like InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K, demonstrating both error correction and cost-efficiency.</div>
<div class="mono" style="margin-top:8px">研究旨在解决大型VLM在处理密集文本和图形元素的复杂图像时的推理难题。提出的Speculative Verdict (SV)框架使用多个轻量级的草案专家生成多样化的推理路径，然后由强大的VLM在判决阶段综合这些路径以生成最终答案。这种方法降低了计算成本同时保持了准确性。实验证明，SV在InfographicVQA、ChartMuseum、ChartQAPro和HR-Bench 4K等基准测试上表现出一致的改进，展示了与大型专有模型或训练管道相比的错误纠正和成本效率优势。</div>
</details>
</div>
<div class="card">
<div class="title">Trajectory Densification and Depth from Perspective-based Blur</div>
<div class="meta-line">Authors: Tianchen Qiu, Qirun Zhang, Jiajian He, Zhengyue Zhuge, Jiahui Xu, Yueting Chen</div>
<div class="meta-line">First: 2025-12-09T14:11:43+00:00 · Latest: 2025-12-09T14:11:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08627v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08627v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视角模糊的轨迹稠化与深度估计</div>
<div class="mono" style="margin-top:8px">在缺乏机械稳定器的情况下，相机在拍摄过程中不可避免地会经历旋转动态，这会在长时间曝光场景中引起视角模糊。从光学角度来看，视角模糊是深度位置相关的：即使在相同的成像设置下，处于不同空间位置的物体也会产生不同的模糊程度。受此启发，我们提出了一种新颖的方法，通过联合光学设计算法检查视频流和稠密轨迹的模糊模式来估计度量深度。具体来说，我们使用现成的视觉编码器和点跟踪器来提取视频信息。然后，我们通过窗口嵌入和多窗口聚合估计深度图，并使用视觉语言模型稠密化光学算法生成的稀疏轨迹。在多个深度数据集上的评估表明，我们的方法在大深度范围内表现出强大的性能，同时保持了良好的泛化能力。与手持拍摄设置中的真实轨迹相比，我们的光学算法在精度上表现出优越性，而稠密重建保持了很强的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a method to estimate metric depth and densify trajectories by analyzing perspective-based blur in video streams. It uses off-the-shelf vision encoder and point tracker to extract video information, and then estimates depth maps through windowed embedding and multi-window aggregation. The method also densifies the sparse trajectory using a vision-language model. Evaluations show that the proposed method performs well over a large depth range and maintains good generalization. Compared to the real trajectory in handheld shooting, the optical algorithm achieves higher precision, and the dense reconstruction maintains strong accuracy.</div>
<div class="mono" style="margin-top:8px">论文提出了一种通过分析视频流中的透视模糊来估计深度和稠密化轨迹的方法。它使用现成的视觉编码器和点跟踪器来提取视频信息，然后通过窗口嵌入和多窗口聚合来估计深度图。该方法还使用视觉语言模型稠密化稀疏轨迹。实验表明，所提出的方法在大深度范围内表现良好，保持了良好的泛化能力，并且在手持拍摄场景中与真实轨迹相比具有更高的精度。</div>
</details>
</div>
<div class="card">
<div class="title">OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics</div>
<div class="meta-line">Authors: Jisang Yoo, Gyeongjin Kang, Hyun-kyu Ko, Hyeonwoo Yu, Eunbyung Park</div>
<div class="meta-line">First: 2025-12-09T14:10:23+00:00 · Latest: 2025-12-09T14:10:23+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08625v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenMonoGS-SLAM: 单目高斯点云SLAM与开放语义</div>
<div class="mono" style="margin-top:8px">同时定位与建图（SLAM）是机器人学、AR/VR和自主系统中的基础组件。近年来，随着对空间AI的关注增加，将SLAM与语义理解相结合变得越来越重要，以实现智能感知和交互。最近的研究已经探索了这种集成，但它们通常依赖于深度传感器或封闭语义模型，限制了其在开放环境中的可扩展性和适应性。在本文中，我们提出了OpenMonoGS-SLAM，这是第一个将3D高斯点云（3DGS）与开放语义理解统一的单目SLAM框架。为了实现这一目标，我们利用了视觉基础模型（VFMs）的最新进展，包括MASt3R用于视觉几何和SAM及CLIP用于开放词汇语义。这些模型在多种任务上提供了稳健的泛化能力，使我们能够实现准确的单目相机跟踪和建图，以及对开放环境中的语义有丰富的理解。我们的方法不依赖任何深度输入或3D语义地面真值，仅依赖于自我监督学习目标。此外，我们提出了一种专门设计的记忆机制，用于管理高维语义特征，有效地构建了高斯语义特征图，从而实现了强大的整体性能。实验结果表明，我们的方法在封闭语义和开放语义分割任务中均能达到或超过现有基线的性能，且不依赖于深度图或语义注释等辅助传感器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">OpenMonoGS-SLAM is a monocular SLAM framework that integrates 3D Gaussian Splatting with open-set semantic understanding, leveraging Visual Foundation Models for robust generalization. It achieves accurate monocular camera tracking and mapping, as well as rich semantic understanding in open-world environments, without requiring depth sensors or 3D semantic ground truth. The method outperforms existing baselines in both closed-set and open-set segmentation tasks, demonstrating strong overall performance.</div>
<div class="mono" style="margin-top:8px">OpenMonoGS-SLAM 是一种结合了 3D 高斯点积和开放集语义理解的单目 SLAM 框架，利用了视觉基础模型如 MASt3R、SAM 和 CLIP。该方法实现了准确的单目相机跟踪和建图，并在开放世界环境中提供了丰富的语义理解，无需深度输入或 3D 语义标注。该方法在封闭集和开放集分割任务中均优于现有基线，展示了强大的整体性能。</div>
</details>
</div>
<div class="card">
<div class="title">OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</div>
<div class="meta-line">Authors: Qiushi Sun, Mukai Li, Zhoumianze Liu, Zhihui Xie, Fangzhi Xu, Zhangyue Yin, Kanzhi Cheng, Zehao Li, Zichen Ding, Qi Liu, Zhiyong Wu, Zhuosheng Zhang, Ben Kao, Lingpeng Kong</div>
<div class="meta-line">First: 2025-10-28T13:22:39+00:00 · Latest: 2025-12-09T13:57:50+00:00</div>
<div class="meta-line">Comments: work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24411v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24411v2">PDF</a> · <a href="https://github.com/OS-Copilot/OS-Sentinel">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents. Our code and data are available at https://github.com/OS-Copilot/OS-Sentinel.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OS-Sentinel：通过混合验证在现实工作流中提升移动GUI代理的安全性</div>
<div class="mono" style="margin-top:8px">由视觉-语言模型（VLMs）驱动的计算机使用代理在操作如移动平台等数字环境方面展示了类似人类的能力。尽管这些代理在推进数字自动化方面具有巨大潜力，但它们进行不安全操作的可能性，如系统破坏和隐私泄露，引发了重大担忧。在移动环境复杂且庞大的操作空间中检测这些安全问题是一项艰巨的挑战，目前仍严重未被探索。为建立移动代理安全性研究的基础，我们引入了MobileRisk-Live，一种动态沙盒环境，附带一个包含现实轨迹和细粒度注释的安全检测基准。在此基础上，我们提出了OS-Sentinel，一种新颖的混合安全性检测框架，该框架结合了形式验证器以检测明确的系统级违规行为，以及基于VLM的上下文评估器以评估上下文风险和代理行为。实验结果显示，OS-Sentinel在多个指标上比现有方法提高了10%-30%。进一步的分析提供了关键见解，促进了更安全、更可靠的自主移动代理的发展。我们的代码和数据可在https://github.com/OS-Copilot/OS-Sentinel获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the safety of mobile GUI agents powered by Vision-Language Models (VLMs) by addressing the risk of unsafe operations such as system compromise and privacy leakage. The study introduces OS-Sentinel, a hybrid safety detection framework that combines a Formal Verifier for explicit system-level violations and a VLM-based Contextual Judge for assessing contextual risks. Experiments demonstrate that OS-Sentinel outperforms existing approaches by 10%-30% across multiple metrics, providing critical insights for developing safer autonomous mobile agents.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决系统妥协和隐私泄露等安全风险，增强由视觉-语言模型（VLM）驱动的移动GUI代理的安全性。研究引入了OS-Sentinel，这是一种结合形式验证器和基于VLM的上下文评估器的混合安全检测框架。实验表明，OS-Sentinel在多个指标上比现有方法提高了10%-30%，为开发更安全的移动代理提供了宝贵见解。代码和数据已公开可用。</div>
</details>
</div>
<div class="card">
<div class="title">Mind to Hand: Purposeful Robotic Control via Embodied Reasoning</div>
<div class="meta-line">Authors: Peijun Tang, Shangjin Xie, Binyan Sun, Baifu Huang, Kuncheng Luo, Haotian Yang, Weiqi Jin, Jianan Wang</div>
<div class="meta-line">First: 2025-12-09T13:19:37+00:00 · Latest: 2025-12-09T13:19:37+00:00</div>
<div class="meta-line">Comments: 49 pages, 25 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08580v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08580v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (&quot;mind&quot;) with robot action (&quot;hand&quot;). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>心手相连：通过具身推理实现目的性机器人控制</div>
<div class="mono" style="margin-top:8px">人类在行动中具有上下文和意图，并且推理起着核心作用。尽管互联网规模的数据使AI系统具备了广泛的推理能力，但在物理行动中实现这些能力仍然是一个重大挑战。我们引入了Lumo-1，这是一种统一了机器人推理（“心”）与机器人行动（“手”）的通用视觉-语言-行动（VLA）模型。我们的方法基于预训练的多模态推理能力，逐步扩展到具身推理和动作预测，并最终实现结构化推理和推理-行动对齐。这形成了一个三阶段的预训练管道：（1）在精选的视觉-语言数据上继续预训练VLM，以增强具身推理技能，如规划、空间理解和轨迹预测；（2）与视觉-语言数据一起进行跨具身机器人数据的协同训练；（3）在收集于具有类人灵巧性和敏捷性的双臂移动操作器Astribot S1上的轨迹上进行动作训练，结合推理过程。最后，我们整合强化学习以进一步提高推理-行动一致性，并在语义推理和运动控制之间形成闭环。广泛的实验表明，Lumo-1在具身视觉-语言推理方面取得了显著的性能提升，这是通用机器人控制的关键组成部分。实际评估进一步表明，Lumo-1在一系列具有挑战性的机器人任务中超越了强大的基线，具有强大的泛化能力，特别是在长时任务和需要推理策略、概念和空间的人类自然指令方面表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</div>
<div class="meta-line">Authors: Qiwei Tian, Chenhao Lin, Zhengyu Zhao, Chao Shen</div>
<div class="meta-line">First: 2025-12-08T07:05:18+00:00 · Latest: 2025-12-09T12:48:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07222v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07222v2">PDF</a> · <a href="https://github.com/michaeltian108/FDA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>减少关注功能词以提高视觉-语言模型的鲁棒性</div>
<div class="mono" style="margin-top:8px">为了解决鲁棒性和性能之间的权衡问题，我们观察到功能词可能会使视觉-语言模型在跨模态对抗攻击中变得脆弱，并据此提出了功能词去注意（FDA）方法以减轻功能词的影响。类似差分放大器，我们的FDA在注意力头内计算原始的和功能词的交叉注意力，并从前者中差分地减去后者，从而获得更对齐和鲁棒的视觉-语言模型。全面的实验包括在2个下游任务、3个数据集和3个模型上的2个SOTA基线，在6种不同攻击下的2个下游任务。总体而言，我们的FDA在3个测试模型上的检索任务中平均导致18/13/53%的ASR下降，仅带来0.2/0.3/0.6%的性能下降；在视觉定位任务中，ASR下降90%，但性能提高了0.3%。我们通过实验展示了FDA的可扩展性、泛化能力和零样本性能，并进行了深入的消融研究和分析。代码将在https://github.com/michaeltian108/FDA公开。</div>
</details>
</div>
<div class="card">
<div class="title">MVP: Multiple View Prediction Improves GUI Grounding</div>
<div class="meta-line">Authors: Yunzhu Zhang, Zeyu Pan, Zhengwen Zeng, Shuheng Shen, Changhua Meng, Linchao Zhu</div>
<div class="meta-line">First: 2025-12-09T12:19:00+00:00 · Latest: 2025-12-09T12:19:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08529v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08529v1">PDF</a> · <a href="https://github.com/ZJUSCL/MVP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP&#x27;s effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MVP：多视图预测提高GUI定位</div>
<div class="mono" style="margin-top:8px">GUI定位，即将自然语言指令转化为精确的像素坐标，对于开发实用的GUI代理至关重要。然而，我们发现现有的定位模型在坐标预测方面存在显著的不稳定性，轻微的视觉扰动（例如裁剪几个像素）可以大幅改变预测结果，导致结果在正确和错误之间翻转。这种不稳定性严重削弱了模型性能，尤其是在高分辨率和小UI元素的样本中。为了解决这一问题，我们提出了多视图预测（MVP），这是一种无需训练的框架，通过多视图推理来提升定位性能。我们的核心见解是，虽然单视图预测可能不稳定，但通过聚合多个精心裁剪视图的预测结果，可以有效区分正确的坐标和异常值。MVP 包含两个组件：（1）注意力引导的视图提案，该组件根据指令到图像的注意力分数生成多样化的视图；（2）多坐标聚类，该组件通过选择最密集空间簇的质心来聚合预测结果。广泛的实验表明，MVP 在各种模型和基准测试中都表现出有效性。值得注意的是，在ScreenSpot-Pro上，MVP 将UI-TARS-1.5-7B 提升至56.1%，GTA1-7B 提升至61.7%，Qwen3VL-8B-Instruct 提升至65.3%，Qwen3VL-32B-Instruct 提升至74.0%。代码可在 https://github.com/ZJUSCL/MVP 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability in GUI grounding models, where minor visual perturbations can significantly alter predictions. It proposes Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. MVP uses Attention-Guided View Proposal to derive diverse views and Multi-Coordinates Clustering to ensemble predictions by selecting the densest spatial cluster. Experiments show MVP improves performance on various models and benchmarks, notably increasing accuracy on ScreenSpot-Pro for different models to 56.1%, 61.7%, 65.3%, and 74.0% respectively.</div>
<div class="mono" style="margin-top:8px">论文针对GUI接地模型中存在的微小视觉扰动会导致预测结果大幅变化的问题，提出了MVP（多视角预测）方法，通过多视角推理聚合多个精心裁剪视角的预测结果，提高模型性能。实验结果显示，MVP在ScreenSpot-Pro基准测试中显著提升了各种模型的准确性，最高提升了74.0%。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Real Weights: Hypercomplex Representations for Stable Quantization</div>
<div class="meta-line">Authors: Jawad Ibn Ahad, Maisha Rahman, Amrijit Biswas, Muhammad Rafsan Kabir, Robin Krambroeckers, Sifat Momen, Nabeel Mohammed, Shafin Rahman</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2025-12-09T12:10:57+00:00 · Latest: 2025-12-09T12:10:57+00:00</div>
<div class="meta-line">Comments: Accepted in Winter Conference on Applications of Computer Vision (WACV) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08524v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08524v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越真实权重：超复数表示法在稳定量化中的应用</div>
<div class="mono" style="margin-top:8px">多模态语言模型（MLLMs）需要大量的参数容量来对齐高维视觉特征与语言表示，使其在计算上非常沉重且难以高效部署。我们提出了一种渐进重参数化策略，通过逐步用紧凑的参数化超复数乘法（PHM）层替换密集的前馈网络块来压缩这些模型。残差插值调度，结合轻量级重构和知识蒸馏损失，确保在训练过程中PHM模块继承其密集对应物的功能行为。这种转变在保持强大的多模态对齐的同时，实现了参数和FLOP的大幅减少，同时保持了输出质量，使推理速度更快。我们在多个视觉语言模型（VLMs）上进行了评估。我们的方法在保持与基础模型相当的性能的同时，实现了显著的模型大小和推理延迟减少。渐进的PHM替换因此提供了一种与架构兼容的路径，以实现更高效的多模态推理，并补充现有的低比特量化技术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to reduce the computational burden of multimodal language models (MLLMs) by introducing a progressive reparameterization strategy that replaces dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. This method uses a residual interpolation schedule and lightweight losses to ensure functional consistency during training. Key findings show that this approach achieves substantial parameter and FLOP reductions while maintaining performance comparable to the base models, enabling faster inference without degrading output quality. The method offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.</div>
<div class="mono" style="margin-top:8px">研究旨在通过逐步用Parameterized Hypercomplex Multiplication (PHM) 层替换密集的前馈网络块来减轻多模态语言模型（MLLMs）的计算负担。该方法使用残差插值调度和轻量级损失来确保训练过程中的功能一致性。该方法在保持多模态对齐的同时实现了参数和FLOP的大幅减少，从而实现更快的推理而不会降低输出质量。在各种视觉语言模型上的评估表明，该方法在保持性能的同时显著减少了模型大小和推理延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Wukong&#x27;s 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models</div>
<div class="meta-line">Authors: Minghao Yin, Yukang Cao, Kai Han</div>
<div class="meta-line">First: 2025-11-27T13:03:57+00:00 · Latest: 2025-12-09T11:45:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.22425v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.22425v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>悟空的72变：基于流模型的无训练高保真纹理3D变形</div>
<div class="mono" style="margin-top:8px">我们提出了一种名为WUKONG的新型无训练框架，用于高保真纹理3D变形，该框架以一对源和目标提示（图像或文本）作为输入。与传统的依赖于手动对应匹配和变形轨迹估计的方法（限制了泛化能力并需要昂贵的预处理）不同，WUKONG利用基于流的生成器的生成先验来生成具有丰富纹理细节的高保真3D过渡。为了确保形状过渡的平滑性，我们利用基于流的生成过程的内在连续性，并将变形问题形式化为最优传输重心问题。我们进一步引入了一种顺序初始化策略，以防止突然的几何失真并保持身份一致性。为了忠实保留纹理，我们提出了一种基于相似性的语义一致性机制，该机制选择性地保留高频细节并允许对混合动力学进行精确控制。这避免了常见的过度平滑等伪影，同时保持语义保真度。广泛的定量和定性评估表明，WUKONG显著优于最先进的方法，在各种几何和纹理变化中取得了更好的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">WUKONG is a training-free framework for high-fidelity 3D morphing that uses flow-based transformers to generate detailed 3D transitions from source and target prompts. It formulates morphing as an optimal transport barycenter problem to ensure smooth shape transitions and introduces a sequential initialization strategy to preserve identity coherence. WUKONG also proposes a similarity-guided semantic consistency mechanism to retain high-frequency details, avoiding oversmoothing while maintaining semantic fidelity. Experimental results show that WUKONG outperforms existing methods in handling diverse geometry and texture variations.</div>
<div class="mono" style="margin-top:8px">WUKONG 是一个无需训练的框架，用于基于图像或文本提示的高保真纹理 3D 变形。它利用流基变换器生成平滑且细节丰富的 3D 过渡，避免了手动对应和昂贵预处理的需要。关键发现表明，WUKONG 在定量和定性评估中均优于现有最佳方法，能够有效处理多样化的几何和纹理变化。</div>
</details>
</div>
<div class="card">
<div class="title">Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions</div>
<div class="meta-line">Authors: Ada Gorgun, Fawaz Sammani, Nikos Deligiannis, Bernt Schiele, Jonas Fischer</div>
<div class="meta-line">First: 2025-12-09T11:05:08+00:00 · Latest: 2025-12-09T11:05:08+00:00</div>
<div class="meta-line">Comments: Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08486v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08486v1">PDF</a> · <a href="https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过提示条件干预研究扩散模型中的时间概念动态</div>
<div class="mono" style="margin-top:8px">扩散模型通常通过其最终输出进行评估，逐步去除随机噪声以生成有意义的图像。然而，生成过程沿着一条轨迹展开，分析这一动态过程对于理解这些模型在成功/失败模式方面的可控性、可靠性和可预测性至关重要。在本文中，我们提出的问题是：何时噪声会转变为特定概念（例如年龄）并锁定去噪轨迹？我们提出了PCI（提示条件干预）来研究这一问题。PCI 是一个无需训练且模型无关的框架，用于通过扩散时间分析概念动态。核心思想是概念插入成功率（CIS）的分析，定义为在给定时间步插入的概念在最终图像中被保留和反映的概率，提供了一种表征概念形成时间动态的方法。将 PCI 应用于多个最先进的文本到图像扩散模型和广泛的概念分类，揭示了扩散模型中概念的多样时间行为，在同一概念类型内，某些轨迹阶段对特定概念更为有利。这些发现还为文本驱动的图像编辑提供了可操作的见解，无需访问模型内部或进行训练，从而获得比强基线更强的编辑，这些编辑在语义准确性和内容保留之间达到了平衡。代码可在：https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates the temporal dynamics of concept formation in diffusion models by proposing PCI (Prompt-Conditioned Intervention), which analyzes the probability of a concept being preserved throughout the denoising process. The study reveals that certain phases of the trajectory are more favorable for specific concepts, providing insights for text-driven image editing. These findings show that interventions can be more effective without needing model internals or training, leading to better semantic accuracy and content preservation compared to strong baselines.</div>
<div class="mono" style="margin-top:8px">该研究通过提出PCI（Prompt-Conditioned Intervention）来分析扩散模型中概念形成的时序动态，该方法通过分析不同时间步的概念插入成功率来进行分析。研究发现，扩散轨迹的某些阶段更适合特定概念，为基于文本的图像编辑提供了行动指南。这些发现无需访问模型内部或进行训练即可实现更有效的编辑，并且在语义准确性和内容保留之间达到了更好的平衡，优于强基线。</div>
</details>
</div>
<div class="card">
<div class="title">SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos</div>
<div class="meta-line">Authors: Mingqi Gao, Yunqi Miao, Jungong Han</div>
<div class="meta-line">First: 2025-12-09T09:37:31+00:00 · Latest: 2025-12-09T09:37:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08406v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08406v1">PDF</a> · <a href="https://github.com/gaomingqi/sam-body4d">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAM-Body4D：无需训练的视频中4D人体网格恢复</div>
<div class="mono" style="margin-top:8px">人体网格恢复（HMR）旨在从2D观察中重建3D人体姿态和形状，是现实场景中以人体为中心理解的基础。尽管最近基于图像的HMR方法如SAM 3D Body在野外图像上表现出强大的鲁棒性，但在应用于视频时依赖于逐帧推理，导致时间不一致并在遮挡下性能下降。我们通过利用视频中固有的人体连续性，无需额外训练来解决这些问题。我们提出了SAM-Body4D，这是一种无需训练的框架，用于从视频中实现时间一致性和遮挡鲁棒性的HMR。我们首先使用可提示的视频分割模型生成身份一致的掩模，然后使用遮挡感知模块细化它们以恢复缺失区域。细化后的掩模引导SAM 3D Body生成一致的全身网格轨迹，而基于填充的并行策略使多人推理变得高效。实验结果表明，SAM-Body4D在具有挑战性的野外视频中实现了更好的时间稳定性和鲁棒性，无需任何重新训练。我们的代码和演示可在：https://github.com/gaomingqi/sam-body4d 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the robustness and temporal consistency of 3D human body mesh recovery from videos. SAM-Body4D proposes a training-free framework that uses identity-consistent masklets generated by a promptable video segmentation model, which are then refined with an Occlusion-Aware module to recover missing regions. This approach guides SAM 3D Body to produce consistent full-body mesh trajectories and enables efficient multi-human inference. The results show that SAM-Body4D enhances temporal stability and robustness in challenging in-the-wild videos without retraining.</div>
<div class="mono" style="margin-top:8px">研究解决了现有基于图像的人体网格恢复（HMR）方法在应用于视频时存在的时间不一致性和遮挡下的性能下降问题。SAM-Body4D 提出了一种无需训练的框架，使用可提示的视频分割模型生成身份一致的掩码，然后使用遮挡感知模块进行细化，指导 SAM 3D Body 生成一致的全身网格轨迹。该方法在具有挑战性的野外视频中展示了更好的时间稳定性和鲁棒性，无需重新训练。</div>
</details>
</div>
<div class="card">
<div class="title">Enabling Validation for Robust Few-Shot Recognition</div>
<div class="meta-line">Authors: Hanxin Wang, Tian Liu, Shu Kong</div>
<div class="meta-line">First: 2025-06-05T07:37:15+00:00 · Latest: 2025-12-09T09:22:13+00:00</div>
<div class="meta-line">Comments: Project website: https://hannawang09.github.io/projects/vest/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04713v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.04713v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hannawang09.github.io/projects/vest/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-Shot Recognition (FSR) tackles classification tasks by training with minimal task-specific labeled data. Prevailing methods adapt or finetune a pretrained Vision-Language Model (VLM) and augment the scarce training data by retrieving task-relevant but noisy samples from open data sources. The finetuned VLM generalizes decently well to the task-specific in-distribution (ID) test data but struggles with out-of-distribution (OOD) test data. This motivates our study of robust FSR with VLM finetuning. The core challenge of FSR is data scarcity, extending beyond limited training data to a complete lack of validation data. We identify a key paradox as a potential solution: repurposing the retrieved open data for validation. As such retrieved data are inherently OOD compared with the task-specific ID training data, finetuned VLMs yield degraded performance on the retrieved data. This causes the validation logic to favor the pretrained model without any finetuning, hindering improvements w.r.t generalization. To resolve this dilemma, we introduce a novel validation strategy that harmonizes performance gain and degradation on the few-shot ID data and the retrieved data, respectively. Our validation enables parameter selection for partial finetuning and checkpoint selection, mitigating overfitting and improving test-data generalization. We unify this strategy with robust learning into a cohesive framework: Validation-Enabled Stage-wise Tuning (VEST). Extensive experiments on the established ImageNet OOD benchmarks show that VEST significantly outperforms existing VLM adaptation methods, achieving state-of-the-art FSR performance on both ID and OOD data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>启用验证以实现稳健的少样本识别</div>
<div class="mono" style="margin-top:8px">少样本识别（FSR）通过使用少量任务特定标记数据进行训练来解决分类任务。现有方法通过调整或微调预训练的视觉-语言模型（VLM），并从开放数据源中检索相关但嘈杂的样本来扩充稀缺的训练数据。微调后的VLM在任务特定的分布内（ID）测试数据上表现良好，但在分布外（OOD）测试数据上表现不佳。这促使我们研究基于VLM微调的稳健FSR。FSR的核心挑战是数据稀缺性，不仅限于有限的训练数据，还包括完全缺乏验证数据。我们发现一个关键悖论可能是潜在的解决方案：重新利用检索到的开放数据进行验证。由于这些检索到的数据与任务特定的ID训练数据相比是固有的OOD，因此微调后的VLM在这些数据上的表现较差。这导致验证逻辑倾向于选择未微调的预训练模型，阻碍了泛化能力的提升。为解决这一困境，我们提出了一种新的验证策略，该策略在少样本ID数据和检索数据上分别平衡性能提升和下降。我们的验证策略能够选择部分微调的参数和检查点，从而减轻过拟合并提高测试数据的泛化能力。我们将这种策略与稳健学习统一到一个综合框架中：验证启用阶段式调优（VEST）。在建立的ImageNet OOD基准测试上的广泛实验表明，VEST显著优于现有的VLM适应方法，在ID和OOD数据上均实现了最先进的FSR性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of robust few-shot recognition by developing a validation strategy that uses retrieved open data for both training and validation. This approach helps mitigate overfitting and improves generalization to out-of-distribution data. The proposed Validation-Enabled Stage-wise Tuning (VEST) framework significantly outperforms existing methods on both in-distribution and out-of-distribution data in ImageNet benchmarks.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Validation-Enabled Stage-wise Tuning (VEST)的新颖验证策略，以解决少样本识别（FSR）的鲁棒性问题。动机来自于在有限任务特定数据下验证模型的困难，这些模型往往在测试的非分布数据上表现不佳。该方法涉及将检索到的开放数据用于验证，这些数据本质上是非分布数据。这种方法有助于选择在分布内和分布外数据上平衡性能的模型参数和检查点，从而提高泛化能力。在ImageNet OOD基准测试上的实验表明，VEST在分布内和分布外设置中均显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</div>
<div class="meta-line">Authors: Dahyeon Kye, Jeahun Sung, Mingyu Jeon, Jihyong Oh</div>
<div class="meta-line">First: 2025-12-08T04:39:12+00:00 · Latest: 2025-12-09T07:35:43+00:00</div>
<div class="meta-line">Comments: Please visit our project page at https://cmlab-korea.github.io/CHIMERA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07155v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07155v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cmlab-korea.github.io/CHIMERA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHIMERA：自适应缓存注入与语义锚点提示的零样本图像形态变换框架</div>
<div class="mono" style="margin-top:8px">扩散模型展示了卓越的生成能力，但在实现平滑且语义一致的图像形态变换方面仍面临挑战。现有方法往往由于缺乏自适应的结构和语义对齐而产生突兀的过渡或过度饱和的外观。我们提出CHIMERA，一种基于扩散的零样本框架，将形态变换形式化为缓存反演引导的去噪过程。为处理大规模的语义和外观差异，我们提出了自适应缓存注入和语义锚点提示。自适应缓存注入（ACI）在DDIM反演过程中缓存输入的低、中、高层特征，并在去噪过程中适当地重新注入，从而在深度和时间自适应的方式下实现空间和语义对齐，并实现自然特征融合和平滑过渡。语义锚点提示（SAP）利用视觉-语言模型生成共享的锚点提示，作为语义锚点，连接不相似的输入，并引导去噪过程向一致的结果发展。最后，我们引入全局-局部一致性评分（GLCS），这是一种形态变换导向的度量标准，同时评估两个输入的全局协调性和局部形态变换的平滑度。广泛的实验和用户研究显示，CHIMERA实现了比现有方法更平滑且更语义对齐的过渡，建立了图像形态变换的新基准。代码和项目页面将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CHIMERA is a zero-shot diffusion-based framework for image morphing that addresses the challenges of abrupt transitions and over-saturated appearances by proposing Adaptive Cache Injection and Semantic Anchor Prompting. ACI caches and re-injects features from both inputs during the denoising process, enabling spatial and semantic alignment. SAP uses a vision-language model to generate a shared anchor prompt, guiding the denoising process towards coherent results. GLCS, a new morphing-oriented metric, evaluates global harmonization and local smoothness. Experiments show CHIMERA outperforms existing methods in achieving smoother and more semantically aligned transitions.</div>
<div class="mono" style="margin-top:8px">CHIMERA 是一种零样本扩散基础框架，用于解决图像变形中的突变过渡和过度饱和问题。它通过自适应缓存注入和语义锚点提示来对齐空间和语义特征，并引入全局-局部一致性评分来评估结果。实验表明，CHIMERA 产生的过渡更加平滑且语义对齐，达到了图像变形的新前沿。</div>
</details>
</div>
<div class="card">
<div class="title">OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation</div>
<div class="meta-line">Authors: Yexin Liu, Manyuan Zhang, Yueze Wang, Hongyu Li, Dian Zheng, Weiming Zhang, Changsheng Lu, Xunliang Cai, Yan Feng, Peng Pei, Harry Yang</div>
<div class="meta-line">First: 2025-12-09T06:49:33+00:00 · Latest: 2025-12-09T06:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08294v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08294v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenSubject：利用视频衍生的身份和多样性先验进行主题驱动的图像生成和操控</div>
<div class="mono" style="margin-top:8px">尽管在主题驱动的图像生成方面取得了令人鼓舞的进展，但当前的模型往往偏离参考身份，并且在包含多个主题的复杂场景中难以应对。为了解决这一挑战，我们引入了OpenSubject，这是一个基于视频的大规模数据集，包含250万样本和435万张图像，用于主题驱动的生成和操控。该数据集通过一个四阶段管道构建，利用跨帧身份先验。 (i) 视频编目。我们应用分辨率和美学过滤以获得高质量的片段。 (ii) 跨帧主题挖掘和配对。我们利用基于视觉-语言模型（VLM）的类别共识、局部定位和多样性意识配对来选择图像对。 (iii) 身份保留参考图像合成。我们引入分割图引导的出画填充以合成用于主题驱动生成的输入图像，并通过边界引导的填充生成用于主题驱动操控的输入图像，同时包含几何感知增强和不规则边界侵蚀。 (iv) 验证和注释。我们利用VLM验证合成样本，基于阶段(iii)重新合成失败样本，然后构建短和长注释。此外，我们引入了一个涵盖主题驱动生成和操控的基准，并使用VLM评判员评估身份保真度、提示一致性、操控一致性和背景一致性。大量实验表明，使用OpenSubject进行训练可以提高生成和操控性能，特别是在复杂场景中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">OpenSubject is a large-scale dataset with 2.5 million samples and 4.35 million images, designed to improve subject-driven image generation and manipulation. It uses a four-stage pipeline to curate high-quality video clips, mine and pair cross-frame subjects, synthesize identity-preserving reference images, and verify and caption the results. Experiments show that training with OpenSubject enhances performance, especially in complex scenes, for both generation and manipulation tasks.</div>
<div class="mono" style="margin-top:8px">OpenSubject 是一个包含 2.5M 个样本和 4.35M 张图像的大规模数据集，通过四阶段管道创建，以解决主题驱动图像生成和操作中的挑战。管道包括视频筛选、跨帧主题挖掘、身份保留的参考图像合成以及验证。实验表明，使用 OpenSubject 训练可以提高复杂场景中的性能，特别是在身份保真度和操作一致性方面。</div>
</details>
</div>
<div class="card">
<div class="title">VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation</div>
<div class="meta-line">Authors: Md Selim Sarowar, Sungho Kim</div>
<div class="meta-line">First: 2025-12-08T06:54:16+00:00 · Latest: 2025-12-09T06:40:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07215v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07215v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VFM-VLM：基于视觉基础模型和视觉语言模型的3D姿态估计中的视觉比较</div>
<div class="mono" style="margin-top:8px">视觉基础模型（VFMs）和视觉语言模型（VLMs）通过提供丰富的语义和几何表示，已经革新了计算机视觉领域。本文介绍了CLIP基于和DINOv2基于的方法在手部物体抓取场景中的3D姿态估计中的全面视觉比较。我们在6D物体姿态估计任务上评估了这两种模型，并展示了它们的互补优势：CLIP在通过语言定位实现语义理解方面表现出色，而DINOv2提供了更优的密集几何特征。通过在基准数据集上的大量实验，我们表明基于CLIP的方法在语义一致性方面表现更好，而基于DINOv2的方法在几何精度方面表现出竞争性性能。我们的分析为选择合适的视觉模型用于机器人操作和抓取提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the use of Vision Foundation Models (VFMs) and Vision Language Models (VLMs) for 3D pose estimation in hand object grasping scenarios. It compares CLIP and DINOv2 approaches, showing that CLIP excels in semantic understanding while DINOv2 provides better geometric features. Experiments on benchmark datasets reveal that CLIP-based methods achieve better semantic consistency, whereas DINOv2-based approaches offer competitive geometric precision. These findings provide guidance for selecting appropriate vision models for robotic manipulation and grasping tasks.</div>
<div class="mono" style="margin-top:8px">该论文比较了基于CLIP和DINOv2的方法在手部抓取场景中的6D物体姿态估计。研究利用视觉基础模型（VFMs）和视觉语言模型（VLMs）提供丰富的语义和几何表示。研究表明，CLIP在通过语言接地实现语义理解方面表现出色，而DINOv2则提供了更优的密集几何特征。大量实验表明，基于CLIP的方法在语义一致性方面表现更好，而基于DINOv2的方法在几何精度方面表现出竞争力。这些发现为选择适用于机器人操作和抓取任务的视觉模型提供了指导。</div>
</details>
</div>
<div class="card">
<div class="title">PAVAS: Physics-Aware Video-to-Audio Synthesis</div>
<div class="meta-line">Authors: Oh Hyun-Bin, Yuhta Takida, Toshimitsu Uesaka, Tae-Hyun Oh, Yuki Mitsufuji</div>
<div class="meta-line">First: 2025-12-09T06:28:50+00:00 · Latest: 2025-12-09T06:28:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08282v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08282v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://physics-aware-video-to-audio-synthesis.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PAVAS：物理感知的视频到音频合成</div>
<div class="mono" style="margin-top:8px">近期视频到音频（V2A）生成技术在感知质量和时间同步方面取得了显著进展，但大多数模型仍以视觉驱动为主，捕捉视觉与听觉之间的关联而不考虑塑造现实世界声音的物理因素。我们提出了物理感知的视频到音频合成（PAVAS），这是一种通过物理驱动音频适配器（Phy-Adapter）将物理推理融入到基于潜在扩散的V2A生成方法。适配器接收物理参数估计器（PPE）估计的对象级物理参数，PPE使用视觉语言模型（VLM）推断移动物体的质量，并使用基于分割的动态3D重建模块恢复其运动轨迹以计算速度。这些物理线索使模型能够合成反映潜在物理因素的声音。为了评估物理现实性，我们创建了VGG-Impact基准，专注于物体-物体交互，并引入了音频-物理相关系数（APCC）作为评估指标，用于衡量物理和听觉属性之间的一致性。全面的实验表明，PAVAS生成的音频在物理合理性和感知一致性方面优于现有V2A模型，在定量和定性评估中均表现出色。访问https://physics-aware-video-to-audio-synthesis.github.io 查看演示视频。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PAVAS is a method that integrates physical reasoning into video-to-audio synthesis, using a Physics-Driven Audio Adapter (Phy-Adapter) to estimate physical parameters such as object mass and motion trajectory. This approach enhances the realism of synthesized audio by considering physical factors. PAVAS outperforms existing models in both quantitative and qualitative evaluations, as demonstrated by a new benchmark, VGG-Impact, and an evaluation metric, APCC, which measures consistency between physical and auditory attributes.</div>
<div class="mono" style="margin-top:8px">PAVAS 是一种将物理推理融入视频到音频合成的方法，使用潜扩散模型和物理驱动音频适配器。它估计物体的质量和运动轨迹等物理参数，以生成更真实的音频。PAVAS 在定量和定性评估中均优于现有模型，产生物理上合理且感知上一致的音频。该方法使用物理参数估计器和视觉语言模型来推断和计算物理线索以进行声音合成。全面的实验和新的基准 VGG-Impact 验证了模型的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Domain-RAG: Retrieval-Guided Compositional Image Generation for Cross-Domain Few-Shot Object Detection</div>
<div class="meta-line">Authors: Yu Li, Xingyu Qiu, Yuqian Fu, Jie Chen, Tianwen Qian, Xu Zheng, Danda Pani Paudel, Yanwei Fu, Xuanjing Huang, Luc Van Gool, Yu-Gang Jiang</div>
<div class="meta-line">First: 2025-06-06T08:41:09+00:00 · Latest: 2025-12-09T06:15:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05872v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.05872v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel objects with only a handful of labeled samples from previously unseen domains. While data augmentation and generative methods have shown promise in few-shot learning, their effectiveness for CD-FSOD remains unclear due to the need for both visual realism and domain alignment. Existing strategies, such as copy-paste augmentation and text-to-image generation, often fail to preserve the correct object category or produce backgrounds coherent with the target domain, making them non-trivial to apply directly to CD-FSOD. To address these challenges, we propose Domain-RAG, a training-free, retrieval-guided compositional image generation framework tailored for CD-FSOD. Domain-RAG consists of three stages: domain-aware background retrieval, domain-guided background generation, and foreground-background composition. Specifically, the input image is first decomposed into foreground and background regions. We then retrieve semantically and stylistically similar images to guide a generative model in synthesizing a new background, conditioned on both the original and retrieved contexts. Finally, the preserved foreground is composed with the newly generated domain-aligned background to form the generated image. Without requiring any additional supervision or training, Domain-RAG produces high-quality, domain-consistent samples across diverse tasks, including CD-FSOD, remote sensing FSOD, and camouflaged FSOD. Extensive experiments show consistent improvements over strong baselines and establish new state-of-the-art results. Codes will be released upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>域-RAG：检索引导的跨域少量样本对象检测的组合图像生成</div>
<div class="mono" style="margin-top:8px">跨域少量样本对象检测（CD-FSOD）旨在仅使用以前未见过的领域中少量标记样本来检测新型对象。尽管数据增强和生成方法在少量样本学习中显示出潜力，但它们在CD-FSOD中的有效性仍不清楚，因为需要同时具备视觉真实性和领域对齐。现有策略，如粘贴增强和文本到图像生成，往往无法保留正确的对象类别或产生与目标领域一致的背景，使得它们直接应用于CD-FSOD具有挑战性。为了解决这些挑战，我们提出了一种无需训练、检索引导的组合图像生成框架——域-RAG，专门用于CD-FSOD。域-RAG 包含三个阶段：领域感知背景检索、领域引导背景生成和前景背景组合。具体而言，输入图像首先被分解为前景和背景区域。然后，检索语义和风格上相似的图像以引导生成模型在同时考虑原始上下文和检索到的上下文的情况下合成新的背景。最后，保留的前景与新生成的领域对齐背景组合以形成生成图像。无需任何额外的监督或训练，域-RAG 能够在包括CD-FSOD、遥感少量样本检测和伪装少量样本检测等多种任务中生成高质量、领域一致的样本。大量实验表明，域-RAG 在强基线之上表现出一致的改进，并建立了新的最先进的结果。代码将在接受后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Domain-RAG is a retrieval-guided compositional image generation framework designed for Cross-Domain Few-Shot Object Detection (CD-FSOD). It decomposes input images into foreground and background, retrieves semantically and stylistically similar images for background generation, and composes them with the foreground to ensure domain consistency. Experiments demonstrate consistent improvements over strong baselines and set new state-of-the-art results in CD-FSOD and related tasks without requiring additional training or supervision.</div>
<div class="mono" style="margin-top:8px">Domain-RAG 是一种用于跨域少量样本目标检测（CD-FSOD）的检索引导式组成图像生成框架。它通过将输入图像分解为前景和背景，检索语义和风格相似的图像，然后将前景与新生成的背景进行组合，来解决生成视觉上真实且域对齐的图像的挑战。实验表明，Domain-RAG 在 CD-FSOD 及相关任务中优于强基线，并且在没有额外监督或训练的情况下建立了新的最先进结果。</div>
</details>
</div>
<div class="card">
<div class="title">CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning</div>
<div class="meta-line">Authors: Yanshu Li, Jianjiang Yang, Zhennan Shen, Ligong Han, Haoyan Xu, Ruixiang Tang</div>
<div class="meta-line">First: 2025-08-11T11:41:51+00:00 · Latest: 2025-12-09T06:05:33+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07871v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.07871v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern large vision-language models (LVLMs) convert each input image into a large set of tokens that far outnumber the text tokens. Although this improves visual perception, it also introduces severe image token redundancy. Because image tokens contain sparse information, many contribute little to reasoning but greatly increase inference cost. Recent image token pruning methods address this issue by identifying important tokens and removing the rest. These methods improve efficiency with only small performance drops. However, most of them focus on single-image tasks and overlook multimodal in-context learning (ICL), where redundancy is higher and efficiency is more important. Redundant tokens weaken the advantage of multimodal ICL for rapid domain adaptation and lead to unstable performance. When existing pruning methods are applied in this setting, they cause large accuracy drops, which exposes a clear gap and the need for new approaches. To address this, we propose Contextually Adaptive Token Pruning (CATP), a training-free pruning method designed for multimodal ICL. CATP uses two stages of progressive pruning that fully reflect the complex cross-modal interactions in the input sequence. After removing 77.8% of the image tokens, CATP achieves an average performance gain of 0.6% over the vanilla model on four LVLMs and eight benchmarks, clearly outperforming all baselines. At the same time, it improves efficiency by reducing inference latency by an average of 10.78%. CATP strengthens the practical value of multimodal ICL and lays the foundation for future progress in interleaved image-text settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CATP：上下文自适应的标记剪枝以实现高效和增强的多模态即席学习</div>
<div class="mono" style="margin-top:8px">现代大型视觉-语言模型（LVLMs）将每个输入图像转换为一个大型标记集，其数量远超过文本标记。虽然这提高了视觉感知，但也引入了严重的图像标记冗余。因为图像标记包含稀疏信息，许多标记对推理贡献甚微但大大增加了推理成本。最近的图像标记剪枝方法通过识别重要标记并移除其余标记来解决这一问题。这些方法在仅小幅性能下降的情况下提高了效率。然而，大多数方法专注于单图像任务，而忽略了多模态即席学习（ICL），在这种场景下冗余更高，效率更为重要。冗余标记削弱了多模态ICL在快速领域适应中的优势，导致性能不稳定。当现有剪枝方法应用于这种场景时，它们会导致较大的准确率下降，这暴露了一个明显的差距和对新方法的需求。为了解决这一问题，我们提出了上下文自适应的标记剪枝（CATP），这是一种为多模态ICL设计的无需训练的剪枝方法。CATP使用两个阶段的渐进剪枝，完全反映了输入序列中的复杂跨模态交互。在移除77.8%的图像标记后，CATP在四个LVLMs和八个基准上平均性能提高了0.6%，明显优于所有基线。同时，它通过平均减少10.78%的推理延迟提高了效率。CATP增强了多模态ICL的实际价值，并为未来交错图像-文本场景中的进展奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Contextually Adaptive Token Pruning (CATP), a training-free method for pruning image tokens in large vision-language models to enhance efficiency and performance in multimodal in-context learning. CATP addresses the redundancy issue in image tokens, which can weaken the effectiveness of multimodal in-context learning. After pruning 77.8% of image tokens, CATP improves performance by an average of 0.6% across four LVLMs and eight benchmarks, while reducing inference latency by 10.78%. This method outperforms existing pruning techniques in this context.</div>
<div class="mono" style="margin-top:8px">论文提出了CATP，一种无需训练的剪枝方法，用于解决视觉语言模型中图像令牌冗余问题，特别是在多模态上下文学习场景中。通过逐步剪枝图像令牌，CATP同时提升了性能和效率。在四种模型和八个基准上，通过剪枝77.8%的图像令牌，CATP平均提高了0.6%的性能，同时将推理延迟降低了10.78%，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">HybridToken-VLM: Hybrid Token Compression for Vision-Language Models</div>
<div class="meta-line">Authors: Jusheng Zhang, Xiaoyang Guo, Kaitong Cai, Qinhan Lv, Yijia Fan, Wenhao Chai, Jian Wang, Keze Wang</div>
<div class="meta-line">First: 2025-12-09T04:48:38+00:00 · Latest: 2025-12-09T04:48:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08240v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HybridToken-VLM：视觉语言模型的混合令牌压缩</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）已经改变了多模态推理，但将数百个视觉块令牌输入LLMs会带来二次计算成本，对内存和上下文窗口造成压力。传统方法面临权衡：连续压缩会稀释高层次语义，如对象身份，而离散量化会丢失细微的细节，如纹理。我们引入了HTC-VLM，这是一种通过双通道分离语义和外观的混合框架，即通过ViT块的连续路径传递细微细节，并通过MGVQ量化投影到四个令牌的符号锚点的离散路径。这些内容被融合成一个580个令牌的混合序列，并通过解耦注意力掩码和瓶颈压缩成一个voco令牌，确保高效且具地基的表示。HTC-VLM 在七个基准测试（GQA、VQAv2、MMBench、MME、POPE、SEED-Bench、ScienceQA-Image）中实现了87.2%的平均性能保留，压缩比为580比1，优于领先的连续基线81.0%。注意力分析表明，压缩令牌优先处理离散锚点，验证了其语义指导。我们的工作表明，简约的混合设计可以解决效率与准确性的矛盾，并推动可扩展的VLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the computational inefficiency of feeding numerous visual patch tokens into large language models in vision-language models (VLMs). It introduces HTC-VLM, a hybrid framework that uses a continuous pathway for fine-grained details and a discrete pathway for symbolic anchors, achieving 87.2% performance retention across seven benchmarks with a 580-to-1 compression ratio, outperforming the leading continuous baseline by 6.2 percentage points.</div>
<div class="mono" style="margin-top:8px">研究旨在解决将视觉补丁令牌输入大型语言模型中的计算和内存挑战。HTC-VLM 引入了一种混合框架，使用连续路径处理细粒度细节，并使用离散路径处理符号锚点，然后将这些信息融合并压缩为一个令牌。该方法在七个基准测试中保留了87.2%的性能，并且与领先的连续基线相比，具有580比1的压缩比，验证了压缩令牌中语义指导的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models</div>
<div class="meta-line">Authors: Jusheng Zhang, Kaitong Cai, Xiaoyang Guo, Sidi Liu, Qinhan Lv, Ruiqi Chen, Jing Yang, Yijia Fan, Xiaofei Sun, Jian Wang, Ziliang Chen, Liang Lin, Keze Wang</div>
<div class="meta-line">First: 2025-12-09T04:13:31+00:00 · Latest: 2025-12-09T04:13:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08228v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.08228v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MM-CoT：多模态模型视觉链式推理基准</div>
<div class="mono" style="margin-top:8px">链式推理（CoT）能力是多模态模型（MMs）的一个重要里程碑，使它们能够解决复杂的视觉推理问题。然而，一个关键问题仍然存在：这种推理是否真正基于视觉证据并且逻辑上连贯？现有的基准主要强调生成，而忽视了验证，即评估推理链是否在视觉上一致且逻辑上有效的能力。为了填补这一空白，我们引入了MM-CoT，这是一种专门设计的诊断基准，用于探测MMs中视觉接地和逻辑连贯性的CoT推理。模型必须选择唯一满足两个正交约束的事件链：（i）视觉一致性，确保每一步都基于可观察的证据，（ii）逻辑连贯性，确保因果和常识的有效性。对抗性干扰被设计成违反其中一个约束，从而暴露不同的推理失败。我们对MM-CoT进行了领先视觉语言模型的评估，并发现即使是最先进的系统也难以应对，揭示了生成流畅性和真实推理准确性的显著差异。MM-CoT与现有基准的相关性较低，证实了它衡量的是视觉接地和逻辑推理的独特组合。该基准为开发能够在视觉世界中不仅合理地推理，而且忠实和连贯地推理的未来模型奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MM-CoT is a benchmark designed to test the visual grounding and logical coherence of Chain-of-Thought reasoning in multimodal models. Models must select the only event chain that is both visually consistent and logically coherent, with adversarial distractors to test these aspects. Leading vision-language models struggle with this task, indicating a gap between their generative fluency and true reasoning fidelity. MM-CoT reveals that existing benchmarks do not adequately measure visual grounding and logical reasoning together.</div>
<div class="mono" style="margin-top:8px">论文提出了MM-CoT基准，用于评估多模态模型在视觉推理和逻辑连贯性方面的表现。模型需要选择一个同时具备视觉一致性和逻辑连贯性的单一事件链，对抗性干扰项被设计用来测试这些方面。评估结果显示，即使是最先进的模型也难以应对，突显了生成流畅性和真实推理准确度之间的差距。MM-CoT衡量的是视觉接地和逻辑推理的独特组合，不同于现有的基准测试。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</div>
<div class="meta-line">Authors: Yongxian Wei, Yilin Zhao, Li Shen, Xinrui Chen, Runxi Cheng, Sinan Du, Hao Yu, Gang Liu, Jiahong Yan, Chun Yuan, Dian Li</div>
<div class="meta-line">First: 2025-11-13T03:08:51+00:00 · Latest: 2025-12-09T02:09:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09907v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09907v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver&#x27;s ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver&#x27;s ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver&#x27;s feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver&#x27;s competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习提出问题：基于推理驱动和求解器自适应的数据合成方法</div>
<div class="mono" style="margin-top:8px">为大型推理模型训练的数据合成提供了比有限的人工精选数据集更具扩展性的替代方案，能够生成高质量的数据。然而，现有方法面临几个挑战：（i）无差别的生成忽略了解算器的能力，导致生成低价值的问题，或者依赖复杂的数据管道来平衡问题难度；（ii）问题生成缺乏推理，导致浅层的问题变体。在本文中，我们开发了一个问题生成器，该生成器在合成之前明确地进行推理以规划问题方向，并根据解算器的能力调整难度。具体来说，我们构建了相关的问题对，并通过推理模型生成中间的问题设计推理（CoT）进行增强。这些数据为生成器提供了问题设计策略的启动。然后，我们将解算器对合成问题的反馈作为奖励信号，使生成器能够校准难度并生成接近解算器能力边缘的互补问题。在10个数学和通用推理基准上的广泛实验表明，我们的方法平均提高了2.5%，并能够泛化到语言和视觉语言模型。此外，使用合成数据训练的解算器为生成器的持续训练提供了改进的奖励，促进了协同进化，并进一步提高了0.7%的性能。我们的代码将在此公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of data synthesis for training large reasoning models by developing a problem generator that reasons explicitly to plan problem directions and adapts difficulty based on the solver&#x27;s ability. The method constructs related problem pairs and augments them with intermediate problem-design CoT from a reasoning model. It uses solver feedback as a reward signal to calibrate difficulty and produce complementary problems near the edge of the solver&#x27;s competence. Experiments on 10 benchmarks show an average improvement of 2.5% and generalization to both language and vision-language models, with an additional 0.7% performance gain from co-evolution.</div>
<div class="mono" style="margin-top:8px">本文针对大规模推理模型训练中数据合成面临的挑战，如无差别生成和生成问题缺乏推理。作者提出了一种方法，该方法明确推理以规划问题方向，并根据求解器的能力调整难度。通过构建相关问题对，并使用推理模型生成中间问题设计的CoT，该方法提高了合成数据的质量。在10个基准测试上的实验显示，平均改进了2.5%，并且能够泛化到语言和视觉语言模型。此外，基于合成数据训练的求解器提供了更好的奖励信号，促进了生成器的协同进化，并进一步提高了0.7%的性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251210_0317.html">20251210_0317</a>
<a href="archive/20251209_0321.html">20251209_0321</a>
<a href="archive/20251208_0317.html">20251208_0317</a>
<a href="archive/20251207_0317.html">20251207_0317</a>
<a href="archive/20251206_0318.html">20251206_0318</a>
<a href="archive/20251205_0320.html">20251205_0320</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
