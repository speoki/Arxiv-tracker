<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-17 03:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251217_0321</div>
    <div class="row"><div class="card">
<div class="title">LitePT: Lighter Yet Stronger Point Transformer</div>
<div class="meta-line">Authors: Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner, Christian Rupprecht, Konrad Schindler</div>
<div class="meta-line">First: 2025-12-15T18:59:57+00:00 · Latest: 2025-12-15T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://litept.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13689v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13689v1">PDF</a> · <a href="https://github.com/prs-eth/LitePT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://litept.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LitePT：更轻量但更强大的点变换器</div>
<div class="mono" style="margin-top:8px">现代用于3D点云处理的神经架构包含卷积层和注意力模块，但它们的最佳组合方式尚不明确。我们分析了3D点云网络中不同计算模块的作用，并发现一种直观的行为：卷积在早期高分辨率层中适于提取低级几何信息，而此时注意力模块昂贵且无益；注意力在低分辨率的深层中更有效地捕捉高级语义和上下文。根据这一设计原则，我们提出了一种新的3D点云骨干网络，早期阶段使用卷积，深层使用注意力。为避免丢弃冗余卷积层时丢失空间布局信息，我们引入了一种新的、无需训练的3D位置编码，PointROPE。结果表明，LitePT模型参数量减少3.6倍，运行速度提高2倍，内存使用减少2倍，但在多种任务和数据集上与最先进的Point Transformer V3相比，性能相当甚至更好。代码和模型可在：https://github.com/prs-eth/LitePT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to optimize 3D point cloud processing networks by balancing the use of convolutional layers and attention blocks. The study proposes LitePT, which uses convolutions in early layers and switches to attention in deeper layers to preserve spatial layout information. The LitePT model has fewer parameters, runs faster, and uses less memory than Point Transformer V3 while matching or outperforming it on various tasks and datasets.</div>
<div class="mono" style="margin-top:8px">该论文提出LitePT，这是一种新的3D点云骨干网络，在早期层使用卷积，在更深的层使用注意力，并通过一种新型的3D位置编码PointROPE保留空间布局信息。该模型参数量少3.6倍，运行速度快2倍，内存使用少2倍，同时在各种任务和数据集上与Point Transformer V3相当或超越其性能。</div>
</details>
</div>
<div class="card">
<div class="title">LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</div>
<div class="meta-line">Authors: Tianye Ding, Yiming Xie, Yiqing Liang, Moitreya Chatterjee, Pedro Miraldo, Huaizu Jiang</div>
<div class="meta-line">First: 2025-12-15T18:59:04+00:00 · Latest: 2025-12-15T18:59:04+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13680v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13680v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for LASER is to enable the practical deployment of high-quality 4D reconstruction models for streaming videos, which are limited by the quadratic memory complexity of existing feed-forward models. LASER proposes a training-free framework that aligns predictions across consecutive temporal windows using layer-wise scale alignment, which segments depth predictions into layers and computes per-layer scale factors. Experiments show that LASER achieves state-of-the-art performance in camera pose estimation and point map reconstruction while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU.</div>
<div class="mono" style="margin-top:8px">论文提出了LASER，这是一种无需训练的框架，通过在连续时间窗口之间对齐预测来将离线重建模型转换为流媒体系统。它解决了由单目尺度不确定性引起的层深度对齐问题，并引入了层间尺度对齐来计算并传播每层的尺度因子。实验表明，LASER在相机姿态估计和点云重建方面达到了最先进的性能，同时在RTX A6000 GPU上以每秒14帧的速度运行，峰值内存使用量为6 GB，适用于大规模流媒体视频的实用部署。</div>
</details>
</div>
<div class="card">
<div class="title">AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</div>
<div class="meta-line">Authors: Junwen Miao, Penghui Du, Yi Liu, Yu Wang, Yan Wang</div>
<div class="meta-line">First: 2025-12-15T18:57:04+00:00 · Latest: 2025-12-15T18:57:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13671v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13671v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentIAD：工具增强的单智能体工业异常检测</div>
<div class="mono" style="margin-top:8px">工业异常检测（IAD）由于正常参考样本稀缺和许多缺陷的细微、局部性质而具有挑战性。单次视图-语言模型（VLMs）往往忽视小异常，缺乏与典型正常模式进行对比的明确机制。我们提出了一种工具驱动的代理框架AgentIAD，以实现多阶段视觉检查。该代理配备了感知放大器（PZ）进行局部精细分析，以及比较检索器（CR）在证据模糊时查询正常示例。为了教授这些检查行为，我们从MMAD数据集中构建了结构化的感知和比较轨迹，并分两阶段训练模型：监督微调后进行强化学习。两部分奖励设计驱动这一过程：感知奖励监督分类准确性、空间对齐和类型正确性，行为奖励鼓励高效使用工具。这些组件共同使模型能够通过逐步观察、放大和验证来精炼其判断。AgentIAD在MMAD上实现了新的97.62%分类准确率，超越了基于先前MLLM的方法，同时生成透明且可解释的检查轨迹。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AgentIAD is a tool-driven framework for industrial anomaly detection that uses a multi-stage visual inspection process. It includes a Perceptive Zoomer for detailed analysis and a Comparative Retriever for comparing against normal patterns. The model is trained in two stages: supervised fine-tuning and reinforcement learning, with a reward system that promotes accurate classification and efficient tool use. AgentIAD achieves 97.62% classification accuracy, outperforming previous methods and providing transparent inspection traces.</div>
<div class="mono" style="margin-top:8px">AgentIAD 是一种工具驱动的工业异常检测框架，采用多阶段视觉检查过程。它包括一个感知放大器进行详细分析，以及一个比较检索器进行与正常模式的比较。模型通过两阶段训练：监督微调和强化学习，并设计奖励以提高分类准确性和工具效率。AgentIAD 达到了 97.62% 的分类准确率，超过了先前的方法，并提供了透明的检查轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</div>
<div class="meta-line">Authors: Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang</div>
<div class="meta-line">First: 2025-12-15T18:52:43+00:00 · Latest: 2025-12-15T18:52:43+00:00</div>
<div class="meta-line">Comments: Project page: https://zhoues.github.io/RoboTracer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13660v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13660v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhoues.github.io/RoboTracer">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboTracer：在视觉语言模型中通过推理掌握空间跟踪</div>
<div class="mono" style="margin-top:8px">空间跟踪是机器人基本的具身交互能力之一，由于需要多步度量导向的推理和复杂的空间指代以及现实世界的度量测量，因此本质上具有挑战性。然而，现有方法在处理这种组合任务时存在困难。为此，我们提出RoboTracer，这是一种3D感知的VLM，首先通过通用空间编码器和回归监督解码器实现3D空间指代和测量，增强监督微调（SFT）期间的尺度意识。此外，RoboTracer通过度量敏感的过程奖励进行强化微调（RFT），监督关键中间感知提示，以准确生成空间跟踪。为了支持SFT和RFT训练，我们引入了TraceSpatial，这是一个包含3000万QA对的大规模数据集，涵盖了户外/室内/桌面场景，并支持复杂的推理过程（多达9步）。我们还提出了TraceSpatial-Bench，这是一个具有挑战性的基准，填补了空间跟踪评估的空白。实验结果表明，RoboTracer在空间理解、测量和指代方面超越了基线，平均成功率达到了79.1%，并且在TraceSpatial-Bench上也以显著优势超越了Gemini-2.5-Pro，准确率高出36%。值得注意的是，RoboTracer可以与各种控制策略结合，执行跨不同机器人（UR5，G1人形机器人）的复杂场景中的长期动态任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboTracer is designed to address the challenges of spatial tracing in robotics by integrating 3D spatial reasoning into vision-language models. It uses a universal spatial encoder and regression-supervised decoder for spatial referring and measuring, and reinforcement fine-tuning to enhance multi-step metric-grounded reasoning. The model is trained on the TraceSpatial dataset, which includes 30M QA pairs for complex reasoning. RoboTracer outperforms existing methods with an average success rate of 79.1% and achieves state-of-the-art performance on the TraceSpatial-Bench benchmark, surpassing Gemini-2.5-Pro by 36% accuracy. It can be applied to various robots for long-horizon tasks in complex environments.</div>
<div class="mono" style="margin-top:8px">RoboTracer 通过结合 3D 空间推理和视觉语言模型来提升机器人的空间跟踪能力。它使用通用的空间编码器和回归监督解码器进行 3D 空间引用和测量，并通过强化微调增强多步度量导向的推理。该模型在 TraceSpatial 大型数据集上进行训练，并且在平均成功率达到了 79.1% 的情况下超越了现有方法，在 TraceSpatial-Bench 上实现了最先进的性能，比 Gemini-2.5-Pro 高出 36% 的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models</div>
<div class="meta-line">Authors: Shweta Mahajan, Shreya Kadambi, Hoang Le, Munawar Hayat, Fatih Porikli</div>
<div class="meta-line">First: 2025-12-15T18:03:42+00:00 · Latest: 2025-12-15T18:03:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13609v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>执行与撤销：生成和逆转视觉语言模型中的物理动作</div>
<div class="mono" style="margin-top:8px">我们引入了执行与撤销任务及基准测试，以解决视觉语言模型中的关键问题：理解并生成由真实世界动作驱动的物理上合理的场景变换。与以往专注于对象级编辑的工作不同，执行与撤销要求模型模拟物理动作的结果，然后准确地逆转它，反映视觉世界中的真正因果关系。我们从真实世界的视频中整理了一个大规模的可逆动作数据集，并设计了一种训练策略，以确保动作的稳健定位。我们的实验表明，当前的模型在物理可逆性方面存在困难，突显了该任务对于具身人工智能、机器人技术和物理感知生成建模的重要性。执行与撤销为评估和推进多模态系统中的物理推理提供了一个直观的测试平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Do-Undo task and benchmark are introduced to address the gap in vision-language models for understanding and generating physically plausible scene transformations. Unlike previous work focusing on object-level edits, Do-Undo requires models to simulate and reverse physical actions, reflecting true cause-and-effect. Experiments show that current models struggle with physical reversibility, highlighting the importance of this task for embodied AI, robotics, and physics-aware generative modeling. The dataset consists of reversible actions from real-world videos, and a training strategy is designed to enforce consistency for robust action grounding.</div>
<div class="mono" style="margin-top:8px">提出了Do-Undo任务和基准，旨在提升视觉-语言模型对物理可实现场景变换的理解和生成能力。与以往专注于对象级编辑的工作不同，该任务要求模型模拟物理动作的结果并准确地逆转它。作者从真实视频中收集了一个大规模的可逆动作数据集，并开发了一种一致性训练策略。实验结果显示，当前模型在物理可逆性方面存在困难，突显了该任务对于具身AI和物理感知生成建模的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Image Diffusion Preview with Consistency Solver</div>
<div class="meta-line">Authors: Fu-Yun Wang, Hao Zhou, Liangzhe Yuan, Sanghyun Woo, Boqing Gong, Bohyung Han, Ming-Hsuan Yang, Han Zhang, Yukun Zhu, Ting Liu, Long Zhao</div>
<div class="meta-line">First: 2025-12-15T17:47:49+00:00 · Latest: 2025-12-15T17:47:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13592v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13592v1">PDF</a> · <a href="https://github.com/G-U-N/consolver">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像扩散预览与一致性求解器</div>
<div class="mono" style="margin-top:8px">图像扩散模型的缓慢推理过程显著降低了交互式用户体验。为解决这一问题，我们引入了扩散预览，这是一种新颖的范式，通过快速、低步数采样生成初步输出供用户评估，直到预览被判定满意后再进行全步数细化。现有的加速方法，包括无训练求解器和后训练蒸馏，难以提供高质量的预览或确保预览与最终输出之间的一致性。我们提出了一致性求解器ConsistencySolver，这是一种源自通用线性多步法的轻量级、可训练的高阶求解器，通过强化学习优化，能够提升预览质量和一致性。实验结果表明，ConsistencySolver在低步数场景中显著提高了生成质量和一致性，使其成为高效的预览和细化工作流的理想选择。值得注意的是，它在使用47%更少的步骤的情况下，FID分数与Multistep DPM-Solver相当，同时优于蒸馏基线。此外，用户研究显示，我们的方法将总体用户交互时间减少了近50%，同时保持了生成质量。代码可在https://github.com/G-U-N/consolver/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the slow inference process of image diffusion models by proposing Diffusion Preview, which uses rapid, low-step sampling for initial output generation and defers full-step refinement. To enhance preview quality and consistency, the authors introduce ConsistencySolver, a lightweight, trainable solver optimized via Reinforcement Learning. Experimental results show that ConsistencySolver improves generation quality and consistency, achieving FID scores comparable to Multistep DPM-Solver with fewer steps and outperforming distillation baselines. User studies also indicate a significant reduction in user interaction time while maintaining generation quality.</div>
<div class="mono" style="margin-top:8px">论文通过引入Diffusion Preview，使用快速、低步数采样生成初步输出供用户评估，以解决图像扩散模型的慢推理问题。为提高预览质量和一致性，作者提出了基于一般线性多步法的轻量级可训练求解器ConsistencySolver，并通过强化学习进行优化。实验结果表明，ConsistencySolver在低步数场景下显著提高了生成质量和一致性，FID分数与Multistep DPM-Solver相当，但使用了47%更少的步骤，并且优于蒸馏基线。用户研究还表明，该方法将用户交互时间减少了近50%，同时保持了生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">Deep priors for satellite image restoration with accurate uncertainties</div>
<div class="meta-line">Authors: Biquard Maud, Marie Chabert, Florence Genin, Christophe Latry, Thomas Oberlin</div>
<div class="meta-line">Venue: IEEE Transactions on Geoscience and Remote Sensing, vol. 63, pp. 1-16, 2025, Art no. 5652916</div>
<div class="meta-line">First: 2024-12-05T12:56:03+00:00 · Latest: 2025-12-15T16:43:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.04130v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.04130v2">PDF</a> · <a href="https://github.com/MaudBqrd/VBLExz">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Satellite optical images, upon their on-ground receipt, offer a distorted view of the observed scene. Their restoration, including denoising, deblurring, and sometimes super-resolution, is required before their exploitation. Moreover, quantifying the uncertainties related to this restoration helps to reduce the risks of misinterpreting the image content. Deep learning methods are now state-of-the-art for satellite image restoration. Among them, direct inversion methods train a specific network for each sensor, and generally provide a point estimation of the restored image without the associated uncertainties. Alternatively, deep regularization (DR) methods learn a deep prior on target images before plugging it, as the regularization term, into a model-based optimization scheme. This allows for restoring images from several sensors with a single network and possibly for estimating associated uncertainties. In this paper, we introduce VBLE-xz, a DR method that solves the inverse problem in the latent space of a variational compressive autoencoder (CAE). We adapt the regularization strength by modulating the bitrate of the trained CAE with a training-free approach. Then, VBLE-xz estimates relevant uncertainties jointly in the latent and in the image spaces by sampling an explicit posterior estimated within variational inference. This enables fast posterior sampling, unlike state-of-the-art DR methods that use Markov chains or diffusion-based approaches. We conduct a comprehensive set of experiments on very high-resolution simulated and real Pléiades images, asserting the performance, robustness and scalability of the proposed method. They demonstrate that VBLE-xz represents a compelling alternative to direct inversion methods when uncertainty quantification is required. The code associated to this paper is available in https://github.com/MaudBqrd/VBLExz.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>卫星图像恢复的深度先验及其准确的不确定性</div>
<div class="mono" style="margin-top:8px">卫星光学图像在地面接收时会呈现出观测场景的失真视图。在利用这些图像之前，需要对其进行恢复，包括去噪、去模糊，有时还包括超分辨率。此外，量化恢复过程相关的不确定性有助于降低误读图像内容的风险。如今，深度学习方法已成为卫星图像恢复的前沿技术。其中，直接反演方法为每个传感器训练特定的网络，并通常提供恢复图像的点估计，而不提供相关的不确定性。相反，深度正则化（DR）方法在目标图像上学习一个深度先验，然后将其作为正则化项插入基于模型的优化方案中。这使得使用单个网络从多个传感器恢复图像，并可能估计相关不确定性。在本文中，我们引入了VBLE-xz，这是一种DR方法，它在变分压缩自编码器（CAE）的潜在空间中解决逆问题。我们通过训练免费的方法调节正则化强度。然后，VBLE-xz通过在变分推断中采样显式后验，在潜在空间和图像空间中联合估计相关不确定性。这使得后验采样速度快于最先进的DR方法，这些方法使用马尔可夫链或扩散方法。我们在非常高分辨率的模拟和实际Pléiades图像上进行了全面的实验，证明了所提方法的性能、鲁棒性和可扩展性。这些结果表明，当需要量化不确定性时，VBLE-xz是直接反演方法的一个有吸引力的替代方案。与本文相关的代码可在https://github.com/MaudBqrd/VBLExz/获得。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces VBLE-xz, a deep regularization method for satellite image restoration that learns a deep prior in the latent space of a variational compressive autoencoder. It estimates uncertainties jointly in the latent and image spaces through variational inference, allowing for fast posterior sampling. Experiments on high-resolution Pléiades images show that VBLE-xz outperforms direct inversion methods in terms of performance, robustness, and scalability, especially when uncertainty quantification is needed.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种名为VBLE-xz的深度正则化方法，用于卫星图像恢复，该方法在变分压缩自编码器的潜在空间中学习先验知识。通过调整比特率来调节正则化强度，并使用变分推断在潜在空间和图像空间中联合估计不确定性。实验结果表明，VBLE-xz在高分辨率的Pléiades图像上表现出色，特别是在需要不确定性量化时，其性能、鲁棒性和可扩展性优于直接反演方法。该方法能够快速进行后验采样，并适用于多种传感器。</div>
</details>
</div>
<div class="card">
<div class="title">HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</div>
<div class="meta-line">Authors: Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang</div>
<div class="meta-line">First: 2025-08-14T12:14:15+00:00 · Latest: 2025-12-15T16:23:44+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10576v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.10576v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://digital-avatar.github.io/ai/HumanSense/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks.Furthermore, grounded in the observation that appropriate feedback stems from a contextual analysis of the interlocutor&#x27;s needs and emotions, we posit that reasoning ability serves as the key to unlocking it. We devise a multi-stage, modality-progressive reinforcement learning approach, resulting in HumanSense-Omni-Reasoning, which substantially enhances performance on higher-level understanding and interactive tasks. Additionally, we observe that successful reasoning processes appear to exhibit consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner.Project page: \textcolor{brightpink}{https://digital-avatar.github.io/ai/HumanSense/}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumanSense：通过推理MLLM实现共情的上下文感知响应</div>
<div class="mono" style="margin-top:8px">尽管多模态大型语言模型（MLLMs）在实现真正的人类交互方面展现出巨大的潜力，但缺乏针对人类中心场景的精细评估框架，涵盖复杂的人类意图理解和提供共情的、上下文感知的响应，这阻碍了进展。在此，我们介绍了HumanSense，一个全面的基准，旨在评估MLLMs的人类中心感知和交互能力，特别关注对扩展多模态上下文的深刻理解以及合理反馈的形成。我们的评估表明，领先MLLMs在高级交互任务方面仍有很大的改进空间。将视觉输入补充以音频和文本信息可带来显著改进，而全模态模型在这些任务上表现出优势。此外，基于适当反馈源自对话者需求和情绪的上下文分析这一观察，我们认为推理能力是解锁这一能力的关键。我们设计了一种多阶段、模态渐进的强化学习方法，从而产生了HumanSense-Omni-Reasoning，显著提升了高层次理解和交互任务的性能。此外，我们观察到成功的推理过程似乎表现出一致的思维模式。通过设计相应的提示，我们还以无训练的方式增强了非推理模型的性能。项目页面：https://digital-avatar.github.io/ai/HumanSense/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">HumanSense is a benchmark for evaluating MLLMs&#x27; human-centered perception and interaction capabilities, focusing on understanding complex contexts and providing empathetic responses. The evaluation shows that current MLLMs need improvement, especially for advanced interaction tasks. HumanSense-Omni-Reasoning, which incorporates a multi-stage reinforcement learning approach, significantly enhances performance on higher-level understanding and interactive tasks by reasoning about the interlocutor&#x27;s needs and emotions. Grounding feedback in contextual analysis is crucial for effective interaction.</div>
<div class="mono" style="margin-top:8px">HumanSense 是一个用于评估 MLLMs 在人类中心感知和交互能力的基准，重点在于理解复杂的上下文并提供同理心的回应。评估显示，当前的 MLLMs 在高级交互任务上仍需改进。通过多阶段强化学习方法，HumanSense-Omni-Reasoning 显著提升了对更高层次理解和交互任务的性能，这需要对对话者的需要和情绪进行推理分析。基于上下文分析提供反馈是有效交互的关键。</div>
</details>
</div>
<div class="card">
<div class="title">Instance-Level Composed Image Retrieval</div>
<div class="meta-line">Authors: Bill Psomas, George Retsinas, Nikos Efthymiadis, Panagiotis Filntisis, Yannis Avrithis, Petros Maragos, Ondrej Chum, Giorgos Tolias</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-29T10:57:59+00:00 · Latest: 2025-12-15T14:45:31+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25387v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实例级组合图像检索</div>
<div class="mono" style="margin-top:8px">组合图像检索（CIR），一种流行的图像检索研究方向，由于缺乏高质量的训练和评估数据而受到限制。我们引入了一个新的评估数据集i-CIR，与现有数据集不同，它专注于实例级类定义。目标是检索包含与视觉查询相同特定对象的图像，这些图像在文本查询定义的多种修改下呈现。该数据集的设计和编纂过程使其保持紧凑，以促进未来研究，同时通过半自动选择难以否定样本，保持其挑战性，与在超过4000万随机干扰项中的检索相当。为了克服获得干净、多样且合适的训练数据的挑战，我们利用预训练的视觉-语言模型（VLMs）采用了一种无需训练的方法BASIC。该方法分别估计查询-图像到图像和查询-文本到图像的相似性，并进行后期融合，以增加同时满足两个查询的图像的权重，而降低仅与其中一个查询高度相似的图像的权重。每个单独的相似性进一步通过一组简单直观的组件得到改进。BASIC在i-CIR上以及遵循语义级类定义的现有CIR数据集上都达到了新的最佳性能。项目页面：https://vrg.fel.cvut.cz/icir/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance composed image retrieval (CIR) by addressing the lack of high-quality training and evaluation data. It introduces a new dataset, i-CIR, which focuses on instance-level class definitions to retrieve images containing the same specific object as the visual query, despite textual modifications. The method, BASIC, leverages pre-trained vision-and-language models to estimate and fuse query-image and query-text similarities, achieving state-of-the-art performance on i-CIR and existing CIR datasets with semantic-level class definitions.</div>
<div class="mono" style="margin-top:8px">论文通过引入一个新的数据集i-CIR，解决了综合图像检索（CIR）中的问题，该数据集侧重于实例级别的对象检索。数据集设计紧凑但具有挑战性，通过半自动化过程选择困难的负样本。为了训练模型，作者提出了一种无需训练的方法BASIC，该方法分别估计视觉和文本相似性，并融合它们以提高检索准确性。BASIC在i-CIR和其他CIR数据集上均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs</div>
<div class="meta-line">Authors: Anran Qi, Changjian Li, Adrien Bousseau, Niloy J. Mitra</div>
<div class="meta-line">First: 2025-12-15T14:45:05+00:00 · Latest: 2025-12-15T14:45:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13392v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://anranqi.github.io/beyondvisible.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address image-to-video generation with explicit user control over the final frame&#x27;s disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越可见：基于代理动态图的消隐感知编辑</div>
<div class="mono" style="margin-top:8px">我们解决了带有明确用户控制最终帧消隐区域的图像到视频生成问题。当前的图像到视频流水线能够生成可信的运动，但在生成可预测、有条理的运动同时确保新揭示区域中用户指定内容方面存在困难。我们的核心思想是将运动规范与外观合成分离：我们引入了一个轻量级、用户可编辑的代理动态图（PDG），它能够确定地但近似地驱动部分运动，而冻结的扩散先验用于合成遵循该运动的可信外观。在我们的无需训练的流水线中，用户对PDG进行粗略标注和重新定位，从中我们计算密集的运动流以利用扩散作为运动引导的着色器。然后，用户可以在图像的消隐区域编辑外观，并利用PDG编码的可见性信息在这些区域执行潜在空间合成，以在运动与用户意图之间达成一致。此设计实现了可控的有条理性和对消隐的用户控制，无需微调。我们展示了与最先进的替代方案相比，我们的方法在将图像转化为包含有条理对象、家具、车辆和变形体的短视频方面的明显优势。我们的方法结合了生成性控制（松散的姿态和结构）与最终帧中消隐区域外观规范的可预测控制，解锁了一种新的图像到视频工作流程。代码将在接受后发布。项目页面：https://anranqi.github.io/beyondvisible.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses image-to-video generation with user control over disoccluded regions. It introduces a Proxy Dynamic Graph (PDG) to drive part motion deterministically while using a frozen diffusion model for plausible appearance synthesis. The method allows users to edit the appearance in disoccluded areas and leverages PDG visibility information for latent-space compositing, achieving controllable articulation and user control without fine-tuning. Results show clear advantages over existing methods in generating articulated motions for objects, furniture, vehicles, and deformables in short videos.</div>
<div class="mono" style="margin-top:8px">研究解决了图像到视频生成问题，并允许用户控制未遮挡区域。引入了代理动态图（PDG）来驱动部分运动，同时使用冻结的扩散模型进行外观合成。该方法允许用户编辑未遮挡区域的外观，并利用PDG中的可见性信息进行潜空间合成，从而实现可控的运动控制和用户控制，无需微调。实验表明，该方法在将图像转换为包含可变形物体、家具、车辆和变形体的视频方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</div>
<div class="meta-line">Authors: Yongxian Wei, Yilin Zhao, Li Shen, Xinrui Chen, Runxi Cheng, Sinan Du, Hao Yu, Gang Liu, Jiahong Yan, Chun Yuan, Dian Li</div>
<div class="meta-line">First: 2025-11-13T03:08:51+00:00 · Latest: 2025-12-15T13:55:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09907v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.09907v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver&#x27;s ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver&#x27;s ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver&#x27;s feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver&#x27;s competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习提出问题：基于推理驱动和求解器自适应的数据合成方法</div>
<div class="mono" style="margin-top:8px">为大型推理模型训练的数据合成提供了比有限的人工精选数据集更具扩展性的替代方案，能够生成高质量的数据。然而，现有方法面临几个挑战：（i）无差别的生成忽略了解算器的能力，导致生成低价值的问题，或者依赖复杂的数据管道来平衡问题难度；（ii）问题生成缺乏推理，导致浅层的问题变体。在本文中，我们开发了一个问题生成器，该生成器在合成之前明确地进行推理以规划问题方向，并根据解算器的能力调整难度。具体来说，我们构建了相关的问题对，并通过推理模型生成中间的问题设计推理（CoT）进行增强。这些数据为生成器提供了问题设计策略的启动。然后，我们将解算器对合成问题的反馈作为奖励信号，使生成器能够校准难度并生成接近解算器能力边缘的互补问题。在10个数学和通用推理基准上的广泛实验表明，我们的方法平均提高了2.5%，并能够泛化到语言和视觉-语言模型。此外，使用合成数据训练的解算器为生成器的持续训练提供了改进的奖励，促进了协同进化，进一步提高了0.7%的性能。我们的代码将在此公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of data synthesis for training large reasoning models, such as indiscriminate generation and a lack of reasoning in problem generation. The authors propose a problem generator that reasons explicitly to plan problem directions and adapts difficulty based on the solver&#x27;s ability. By constructing related problem pairs and using a reasoning model to produce intermediate problem-design CoT, the generator can bootstrap problem-design strategies. The solver&#x27;s feedback is used as a reward signal to calibrate difficulty and produce complementary problems near the edge of the solver&#x27;s competence. Experiments on 10 benchmarks show an average improvement of 2.5% and generalization to both language and vision-language models, with an additional 0.7% performance gain from co-evolution.</div>
<div class="mono" style="margin-top:8px">本文通过开发一个显式推理规划问题方向并根据求解器能力调整难度的问题生成器，解决了大规模推理模型训练中的数据合成挑战。该方法构建相关问题对，并使用推理模型生成的中间问题设计CoT进行增强。同时，使用求解器反馈作为奖励信号来调整难度并生成接近求解器能力边缘的互补问题。在10个基准测试上的实验显示平均改进了2.5%，并且能够泛化到语言和视觉语言模型，进一步通过协同进化获得了0.7%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images</div>
<div class="meta-line">Authors: Bo Liu, Qiao Qin, Qinghui He</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-15T12:48:27+00:00 · Latest: 2025-12-15T12:48:27+00:00</div>
<div class="meta-line">Comments: 9 pages Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13285v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CausalCLIP：因果驱动的特征解缠与筛选以实现生成图像检测的泛化能力</div>
<div class="mono" style="margin-top:8px">生成模型的快速发展增加了对能够跨多种多样且不断演变的生成技术进行泛化的生成图像检测器的需求。然而，现有的方法，包括利用预训练的视觉-语言模型的方法，往往会产生高度缠结的表示，将与任务相关的法医线索（因果特征）与无关或不相关的模式（非因果特征）混合在一起，从而限制了泛化能力。为了解决这一问题，我们提出了CausalCLIP框架，该框架明确地解缠因果特征与非因果特征，并通过因果推理原则进行目标筛选，仅保留最具转移性和区分性的法医线索。通过使用结构因果模型建模生成过程，并通过Gumbel-Softmax基特征掩蔽和Hilbert-Schmidt独立性判据（HSIC）约束来强制统计独立性，CausalCLIP隔离了对分布偏移具有鲁棒性的稳定因果特征。当在不同系列的未见过的生成模型上进行测试时，CausalCLIP展示了强大的泛化能力，相对于最先进的方法，在准确性和平均精度上分别提高了6.83%和4.06%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CausalCLIP is designed to improve the generalization of generated image detectors by disentangling causal and non-causal features and applying causal inference-based filtering. It uses a structural causal model to isolate stable causal features and enforces statistical independence through feature masking and HSIC constraints. CausalCLIP shows strong generalization ability, achieving 6.83% higher accuracy and 4.06% higher average precision compared to state-of-the-art methods on unseen generative models.</div>
<div class="mono" style="margin-top:8px">CausalCLIP 是一种框架，旨在通过分离因果和非因果特征来提高生成图像检测器的泛化能力。它利用因果推理原则过滤掉不可移植的模式，并保留最具有区分性的因果特征。通过使用结构因果模型建模生成过程，并应用 Gumbel-Softmax 基础特征遮罩和 HSIC 约束，CausalCLIP 提高了检测器对分布变化的鲁棒性。该方法在未见过的生成模型上实现了 6.83% 的准确率提升和 4.06% 的平均精度提升，优于最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection</div>
<div class="meta-line">Authors: Juil Koo, Daehyeon Choi, Sangwoo Youn, Phillip Y. Lee, Minhyuk Sung</div>
<div class="meta-line">First: 2025-12-15T12:04:26+00:00 · Latest: 2025-12-15T12:04:26+00:00</div>
<div class="meta-line">Comments: Project page: https://active-view-selection.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13250v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://active-view-selection.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向移动视觉：学习基于视觉的主动视角选择</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在视觉问答（VQA）方面表现出色，但仍然局限于静态视觉，仅能从静态图像中进行推理。相比之下，具身智能体需要移动视觉，能够主动移动以获取更具信息量的视角。我们引入了基于视觉的主动视角选择（VG-AVS）任务，该任务仅使用当前图像中的视觉信息来选择最具有信息量的下一个视角，而不依赖于场景记忆或外部知识。为了支持这一任务，我们构建了一个合成数据集，其中包含自动生成的查询-目标视图配对以及问题-答案提示。我们还提出了一种框架，通过监督微调（SFT）后跟基于强化学习的策略优化来微调预训练的VLMs。我们的方法在基于视角选择的问题回答方面表现出色，并且能够稳健地泛化到未见过的合成和真实场景中。此外，将我们学习到的VG-AVS框架集成到现有的场景探索型问答系统中，可以提高下游问题回答的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance VLMs by enabling them to perform ambulatory vision, which involves selecting the most informative viewpoint based on visual information alone. The method involves creating a synthetic dataset for training and using a framework that fine-tunes pretrained VLMs with supervised fine-tuning followed by reinforcement learning. Key findings include strong performance in viewpoint selection and robust generalization to unseen scenes, with improvements in question-answering accuracy when integrated into existing systems.</div>
<div class="mono" style="margin-top:8px">研究旨在通过开发Visually Grounded Active View Selection (VG-AVS)使机器人具备主动选择最有信息量视角的能力，该任务仅依赖当前视图的视觉信息。方法包括构建包含查询-目标视图配对和问题的合成数据集，并使用监督微调（SFT）和基于强化学习的策略优化来微调预训练的Vision Language Models (VLMs)。该方法在基于视角选择的问题回答上表现出色，并且能够很好地泛化到合成和真实场景。将此框架集成到现有的场景探索型问答系统中可以提高其准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</div>
<div class="meta-line">Authors: Zihui Zhao, Zechang Li</div>
<div class="meta-line">First: 2025-12-15T11:55:55+00:00 · Latest: 2025-12-15T11:55:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13240v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反射偏好优化（RPO）：通过提示引导的反思增强策略对齐</div>
<div class="mono" style="margin-top:8px">直接偏好优化（DPO）已成为一种轻量级且有效的替代强化学习从人类反馈（RLHF）和强化学习与AI反馈（RLAIF）的方法，用于对齐大型语言和视觉-语言模型。然而，标准的DPO公式，其中选择和拒绝的响应均由同一策略生成，由于两者经常共享相似的错误并表现出较小的Kullback-Leibler（KL）散度，导致学习信号较弱，从而导致收敛缓慢且不稳定。为解决这一局限性，我们引入了反射偏好优化（RPO），这是一种新的框架，将提示引导的反思融入DPO范式中。RPO使用外部模型来识别幻觉来源并生成简洁的反思提示，从而能够构建具有更强对比度和更清晰偏好信号的策略对齐偏好对。我们从理论上证明，通过互信息条件化提示可以增加预期的偏好边际，提高样本效率，同时保持在策略分布家族内。实验上，RPO在较少的训练样本和迭代次数下实现了更好的对齐，显著降低了幻觉率，并在多模态基准测试中达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reflective Preference Optimization (RPO) addresses the limitations of Direct Preference Optimization (DPO) by incorporating hint-guided reflection. RPO uses external models to generate reflective hints, which help create more contrasting on-policy preference pairs. This method improves the learning signal and leads to faster and more stable convergence. Experiments show that RPO requires fewer training samples and iterations to achieve superior alignment, significantly reducing hallucination rates and outperforming existing methods on multimodal benchmarks.</div>
<div class="mono" style="margin-top:8px">Reflective Preference Optimization (RPO)通过引入基于提示的反思来改进直接偏好优化（DPO），以增强策略对齐。RPO使用外部模型生成反思提示，帮助构建更具对比度和更清晰偏好的策略对。实验表明，RPO使用更少的训练样本和迭代次数实现了更好的对齐，降低了幻觉率，并在多模态基准测试中达到最佳性能。</div>
</details>
</div>
<div class="card">
<div class="title">MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</div>
<div class="meta-line">Authors: Minghui Hou, Wei-Hsing Huang, Shaofeng Liang, Daizong Liu, Tai-Hao Wen, Gang Wang, Runwei Guan, Weiping Ding</div>
<div class="meta-line">First: 2025-12-15T10:37:59+00:00 · Latest: 2025-12-15T10:37:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13177v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13177v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMDrive：超越视觉的多表示融合交互场景理解</div>
<div class="mono" style="margin-top:8px">视觉语言模型通过多源信息融合，使复杂交通场景的理解和推理成为可能，成为自动驾驶的核心技术。然而，现有的视觉语言模型受限于二维图像理解范式，限制了其对三维空间信息的感知能力和深层次语义融合的能力，导致在复杂自动驾驶环境中表现不佳。本研究提出MMDrive，这是一种多模态视觉语言模型框架，将传统的图像理解扩展到一个通用的三维场景理解框架。MMDrive结合了三种互补的模态，包括占用地图、LiDAR点云和文本场景描述。为此，它引入了两种新的组件，用于自适应跨模态融合和关键信息提取。具体来说，文本导向的多模态调制器根据问题中的语义线索动态加权每个模态的贡献，引导上下文感知特征整合。跨模态抽象器使用可学习的抽象标记生成紧凑的跨模态摘要，突出关键区域和重要语义。在DriveLM和NuScenes-QA基准上的全面评估表明，MMDrive在自动驾驶中显著优于现有视觉语言模型，DriveLM上的BLEU-4得分为54.56，METEOR得分为41.78，NuScenes-QA上的准确率为62.7%。MMDrive有效地突破了传统的仅图像理解障碍，能够在复杂驾驶环境中实现稳健的多模态推理，并为可解释的自动驾驶场景理解提供新的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MMDrive is a multimodal vision-language model framework that extends traditional 2D image understanding to a 3D scene understanding framework, incorporating occupancy maps, LiDAR point clouds, and textual scene descriptions. It introduces two novel components: a Text-oriented Multimodal Modulator for adaptive cross-modal fusion based on semantic cues, and a Cross-Modal Abstractor for generating compact summaries. MMDrive achieves significant performance gains on DriveLM and NuScenes-QA benchmarks, with BLEU-4 scores of 54.56 and METEOR scores of 41.78, and an accuracy score of 62.7% on NuScenes-QA, demonstrating its effectiveness in complex autonomous driving environments.</div>
<div class="mono" style="margin-top:8px">MMDrive 是一种多模态视觉-语言模型框架，将传统的二维图像理解扩展到三维场景理解框架，结合了占用地图、LiDAR 点云和文本场景描述。它引入了两个新型组件：面向文本的多模态调制器，基于语义线索进行自适应跨模态融合，以及跨模态抽象器，生成紧凑的跨模态摘要。MMDrive 在 DriveLM 和 NuScenes-QA 基准测试中取得了显著的性能提升，BLEU-4 得分为 54.56，METEOR 得分为 41.78，NuScenes-QA 的准确率为 62.7%，展示了其在复杂自动驾驶环境中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MR-COSMO: Visual-Text Memory Recall and Direct CrOSs-MOdal Alignment Method for Query-Driven 3D Segmentation</div>
<div class="meta-line">Authors: Chade Li, Pengju Zhang, Yihong Wu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-06-26T04:10:33+00:00 · Latest: 2025-12-15T09:43:49+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026. Copyright (c) 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.20991v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.20991v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of vision-language models (VLMs) in 3D domains has accelerated research in text-query-guided point cloud processing, though existing methods underperform in point-level segmentation due to inadequate 3D-text alignment that limits local feature-text context linking. To address this limitation, we propose MR-COSMO, a Visual-Text Memory Recall and Direct CrOSs-MOdal Alignment Method for Query-Driven 3D Segmentation, establishing explicit alignment between 3D point clouds and text/2D image data through a dedicated direct cross-modal alignment module while implementing a visual-text memory module with specialized feature banks. This direct alignment mechanism enables precise fusion of geometric and semantic features, while the memory module employs specialized banks storing text features, visual features, and their correspondence mappings to dynamically enhance scene-specific representations via attention-based knowledge recall. Comprehensive experiments across 3D instruction, reference, and semantic segmentation benchmarks confirm state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MR-COSMO：基于视觉-文本记忆召回和直接跨模态对齐的查询驱动3D分割方法</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在3D领域的迅速发展加速了文本查询引导的点云处理研究，尽管现有方法在点级分割方面表现不佳，因为缺乏足够的3D-文本对齐，限制了局部特征-文本上下文链接。为了解决这一限制，我们提出了MR-COSMO，一种基于视觉-文本记忆召回和直接跨模态对齐的查询驱动3D分割方法，通过专用的直接跨模态对齐模块建立了3D点云和文本/2D图像数据之间的显式对齐，同时实现了具有专门特征库的视觉-文本记忆模块。这种直接对齐机制能够精确融合几何和语义特征，而记忆模块则通过基于注意力的知识召回，使用专门存储文本特征、视觉特征及其对应映射的特征库动态增强场景特定表示。全面的实验结果表明，该方法在3D指令、参考和语义分割基准测试中达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes MR-COSMO, a method for query-driven 3D segmentation that enhances 3D-text alignment through a direct cross-modal alignment module and a visual-text memory module. This approach improves the precision of geometric and semantic feature fusion and dynamically enhances scene-specific representations. Experiments show that MR-COSMO outperforms existing methods on 3D instruction, reference, and semantic segmentation benchmarks.</div>
<div class="mono" style="margin-top:8px">论文提出了MR-COSMO方法，通过直接跨模态对齐模块和视觉-文本记忆模块增强查询驱动的3D分割中的3D-文本对齐。这种方法提高了几何和语义特征融合的精确度，并动态增强了场景特定表示。实验表明，MR-COSMO在3D指令、参考和语义分割基准测试中优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model</div>
<div class="meta-line">Authors: Yumeng Li, Guang Yang, Hao Liu, Bowen Wang, Colin Zhang</div>
<div class="meta-line">First: 2025-12-02T07:42:38+00:00 · Latest: 2025-12-15T09:43:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.02498v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.02498v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world&#x27;s vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce $\text{dots.ocr}$, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, $\text{dots.ocr}$ establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>dots.ocr：单个视觉语言模型中的多语言文档布局解析</div>
<div class="mono" style="margin-top:8px">文档布局解析是人工智能（AI）访问和解释世界庞大结构化知识库的关键途径。这一过程包括布局检测、文本识别和关系理解，对于增强下一代视觉语言模型至关重要。然而，当前的方法依赖于分段的多阶段管道，容易产生错误传播，并且无法充分利用联合训练的协同效应。在本文中，我们介绍了$\text{dots.ocr}$，这是一种单个视觉语言模型，首次在统一的端到端框架中联合学习三个核心任务。这得益于一个高度可扩展的数据引擎，该引擎综合了一个庞大的多语言语料库，使模型能够在各种任务中提供稳健的表现，涵盖多种语言、布局和领域。我们通过在综合的OmniDocBench上取得最先进的性能验证了我们统一范式的有效性。此外，为了促进全球文档智能的研究，我们引入了XDocParse，这是一个涵盖126种语言的具有挑战性的新基准。在这一测试平台上，$\text{dots.ocr}$建立了新的基准，比第二好的竞争对手高出显著的+7.4分，并证明了其无与伦比的多语言能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the ability of AI to interpret structured knowledge from documents by introducing dots.ocr, a single Vision-Language Model that jointly learns layout detection, text recognition, and relational understanding in an end-to-end framework. This model, supported by a scalable data engine, achieves state-of-the-art performance on OmniDocBench and sets a new benchmark with a 7.4 point improvement on XDocParse, a new 126-language testbed, demonstrating its multilingual capabilities.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入dots.ocr，一个能够联合学习布局检测、文本识别和关系理解的单一体视语言模型，提高AI解读文档中结构化知识的能力。该模型利用可扩展的数据引擎处理多语言文档，并在全方位的OmniDocBench上表现出色，比下一个最佳竞争对手在跨126种语言的XDocParse基准测试中高出显著的+7.4分，证明了其无与伦比的多语言能力。</div>
</details>
</div>
<div class="card">
<div class="title">UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era</div>
<div class="meta-line">Authors: Ziqiang Zhu, Bowei Yang</div>
<div class="meta-line">First: 2025-12-15T08:42:23+00:00 · Latest: 2025-12-15T08:42:23+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13089v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13089v1">PDF</a> · <a href="https://github.com/Die-Xie/UniVCD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniVCD：开放词汇时代的无监督变化检测新方法</div>
<div class="mono" style="margin-top:8px">变化检测（CD）通过多时相观测识别场景变化，在城市开发和环境监测中广泛应用。现有大多数CD方法依赖于监督学习，性能高度依赖于数据集，导致标注成本高昂；它们通常专注于少数预定义类别，难以泛化到多样化的场景。随着SAM2和CLIP等视觉基础模型的兴起，出现了放松这些限制的新机会。我们提出了统一开放词汇变化检测（UniVCD），这是一种基于冻结的SAM2和CLIP构建的无监督、开放词汇变化检测方法。UniVCD能够在没有任何标注数据或配对变化图像的情况下，检测跨多样场景和成像几何的变化。引入了一个轻量级特征对齐模块，将SAM2的空间详细表示与CLIP的语义先验相结合，实现高分辨率、语义感知的变化估计，同时保持可训练参数数量较少。在此基础上，引入了一条简化的后处理流水线，以抑制噪声和伪变化，提高具有明确边界对象的检测准确性。在几个公开的二值变化检测（BCD）和语义变化检测（SCD）基准测试上进行的实验表明，UniVCD在F1和IoU等关键指标上表现出一致的强性能，并且在某些情况下超越了现有的开放词汇变化检测方法。结果表明，使用冻结的视觉基础模型和轻量级多模态对齐的无监督变化检测是一种实用且有效的开放词汇变化检测范式。代码和预训练模型将在https://github.com/Die-Xie/UniVCD上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniVCD is an unsupervised change detection method that leverages frozen SAM2 and CLIP to detect category-agnostic changes across various scenes and imaging geometries without labeled data. It introduces a lightweight feature alignment module to combine spatially detailed representations from SAM2 with semantic priors from CLIP, enabling high-resolution, semantically aware change estimation. Experiments on multiple benchmarks show that UniVCD outperforms existing open-vocabulary change detection methods in terms of F1 and IoU scores, demonstrating the practicality and effectiveness of this approach.</div>
<div class="mono" style="margin-top:8px">UniVCD 是一种无需标注数据的无监督变化检测方法，利用冻结的 SAM2 和 CLIP 来检测不同场景和成像几何下的无类别变化。它引入了一个轻量级的特征对齐模块，将 SAM2 的空间详细表示与 CLIP 的语义先验相结合，实现高分辨率、语义感知的变化估计。实验结果表明，UniVCD 在多个基准上的 F1 和 IoU 指标上优于现有开放词汇变化检测方法，证明了这种方法的实用性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</div>
<div class="meta-line">Authors: Dahyeon Kye, Jeahun Sung, Mingyu Jeon, Jihyong Oh</div>
<div class="meta-line">First: 2025-12-08T04:39:12+00:00 · Latest: 2025-12-15T08:33:55+00:00</div>
<div class="meta-line">Comments: Please visit our project page at https://cmlab-korea.github.io/CHIMERA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07155v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.07155v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cmlab-korea.github.io/CHIMERA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHIMERA：自适应缓存注入与语义锚点提示的零样本图像形态变换及其形态导向度量</div>
<div class="mono" style="margin-top:8px">扩散模型展示了卓越的生成能力，但在实现平滑且语义一致的图像形态变换方面仍面临挑战。现有方法往往由于缺乏自适应结构和语义对齐而产生突兀的过渡或过度饱和的外观。我们提出CHIMERA，一种基于扩散的零样本框架，将形态变换形式化为缓存反演引导的去噪过程。为处理大规模的语义和外观差异，我们提出了自适应缓存注入和语义锚点提示。自适应缓存注入（ACI）在DDIM反演过程中缓存来自两个输入的低、中、高层特征，并在去噪过程中自适应地重新注入，从而在深度和时间自适应的方式下实现空间和语义对齐，并实现自然特征融合和平滑过渡。语义锚点提示（SAP）利用视觉-语言模型生成共享的锚点提示，作为语义锚点，连接不相似的输入，并引导去噪过程向一致的结果发展。最后，我们引入全局-局部一致性评分（GLCS），这是一种形态导向度量，同时评估两个输入的全局和谐性和局部形态变换的平滑度。广泛的实验和用户研究显示，CHIMERA实现了比现有方法更平滑且更语义对齐的过渡，建立了图像形态变换的新基准。代码和项目页面将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CHIMERA is a zero-shot diffusion-based framework for image morphing that addresses the challenges of abrupt transitions and over-saturated appearances by introducing Adaptive Cache Injection and Semantic Anchor Prompting. ACI caches and re-injects features from both inputs during the denoising process, enabling spatial and semantic alignment. SAP uses a vision-language model to generate a shared anchor prompt, guiding the denoising process towards coherent results. The GLCS metric evaluates global harmonization and local smoothness. Experiments show CHIMERA outperforms existing methods in achieving smoother and more semantically aligned transitions, setting a new state of the art in image morphing.</div>
<div class="mono" style="margin-top:8px">CHIMERA 是一种零样本扩散基础框架，通过引入自适应缓存注入（ACI）和语义锚点提示（SAP）来解决图像变形中的突变过渡和过度饱和问题。ACI 在反转过程中缓存两个输入的特征并在去噪过程中适配性注入，而 SAP 利用视觉语言模型生成一个语义锚点提示，以引导去噪过程产生一致的结果。GLCS 指标同时评估全局和谐度和局部变形的平滑度。实验表明，CHIMERA 在实现更平滑和更语义对齐的图像变形结果方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models</div>
<div class="meta-line">Authors: Zizhi Chen, Yizhen Gao, Minghao Han, Yizhou Liu, Zhaoyu Chen, Dingkang Yang, Lihua Zhang</div>
<div class="meta-line">First: 2025-12-15T08:09:40+00:00 · Latest: 2025-12-15T08:09:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13072v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13072v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model&#x27;s capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>锻造动态记忆：基于检索的持续学习框架促进通用医疗基础模型</div>
<div class="mono" style="margin-top:8px">多模态生物医学视觉-语言模型（VLMs）在持续学习（CL）领域展现出巨大的潜力。然而，它们面临一个核心难题：如何在不同模态之间巨大的领域差距中保留精细的跨模态特征。为了解决这一挑战，我们提出了一种全面的框架。利用我们从PubMed科学论文中提取的1800万规模的多模态综合医学检索数据库，我们首次将检索增强生成（RAG）集成到持续学习中。具体而言，我们采用多模态、多层RAG系统，通过动态、按需的知识检索为模型微调提供实时指导。在此基础上，我们引入了一种动态知识蒸馏框架。该框架通过动态调节参数空间的重要性、蒸馏知识的粒度以及参考数据集的数据分布，以适应所需的详细程度，精确解决了上述核心难题。为了彻底验证我们策略的临床价值，我们设计了一个更严格的医学通用任务增量学习（MGTIL）基准。该基准旨在同时评估模型在面对显著领域变化时的适应能力、保留细微跨域特征的能力以及实时学习新型复杂医学任务的能力。广泛的实验结果表明，我们提出的方法在所有指标上均达到了最先进的（SOTA）性能。代码已提供在附录材料中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of preserving fine-grained intra-modality features while bridging the domain gap in multimodal biomedical Vision-Language Models for Continual Learning. The method involves a Retrieval-Augmented Generation system that dynamically retrieves knowledge for model fine-tuning and a dynamic knowledge distillation framework that modulates the importance of the parameter space and distilled knowledge. The proposed method achieves state-of-the-art performance across all metrics in the MGTIL benchmark, which evaluates adaptation to domain shifts, retention of intra-domain features, and real-time learning of new medical tasks.</div>
<div class="mono" style="margin-top:8px">论文针对多模态生物医学视觉-语言模型（VLMs）在连续学习（CL）中的挑战，即如何在保持细粒度的跨模态特征的同时跨越不同模态之间的领域差距。它提出了一种结合检索增强生成（RAG）的框架，用于动态知识检索和蒸馏，并动态调整参数空间的重要性以及蒸馏知识的粒度。该方法通过严格的医学通用任务增量学习（MGTIL）基准进行验证，展示了在所有指标上的最佳性能。</div>
</details>
</div>
<div class="card">
<div class="title">GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</div>
<div class="meta-line">Authors: Tong Wei, Yijun Yang, Changhao Zhang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye</div>
<div class="meta-line">First: 2025-12-15T07:11:56+00:00 · Latest: 2025-12-15T07:11:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13043v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a &quot;free&quot; teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the &quot;entropy collapse&quot; observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GTR-Turbo：合并检查点秘密地成为自主VLM训练的免费教师</div>
<div class="mono" style="margin-top:8px">基于视觉语言模型（VLMs）构建的多模态代理的多轮强化学习（RL）受到稀疏奖励和长期信用分配的阻碍。最近的方法通过查询提供逐步反馈的教师来增加奖励密度，例如引导思想强化学习（GTR）和在线策略蒸馏，但依赖于昂贵且通常是有特权的模型作为教师，限制了其实用性和可再现性。我们引入了GTR-Turbo，这是一种GTR的高效升级版，无需训练或查询昂贵的教师模型即可达到相同性能。具体而言，GTR-Turbo将正在进行的RL训练过程中生成的检查点权重合并，并使用此合并模型作为“免费”的教师，通过监督微调或软logit蒸馏来指导后续的RL。此设计消除了对特权VLM（例如GPT或Gemini）的依赖，缓解了先前工作中观察到的“熵崩溃”现象，并保持了训练的稳定性。在多种视觉代理任务中，GTR-Turbo将基线模型的准确性提高了10-30%，同时将墙钟训练时间减少了50%，计算成本减少了60%相对于GTR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GTR-Turbo addresses the challenges of multi-turn reinforcement learning for multi-modal agents by merging checkpoints from ongoing RL training to create a &#x27;free&#x27; teacher model. This method eliminates the need for expensive, privileged models and improves the baseline model&#x27;s accuracy by 10-30% while reducing training time and compute cost by 50% and 60%, respectively, compared to GTR.</div>
<div class="mono" style="margin-top:8px">GTR-Turbo通过合并正在进行的RL训练产生的检查点来创建一个‘免费’教师模型，解决了多轮强化学习中多模态代理的挑战。该方法消除了对昂贵特权模型的依赖，并将基线模型的准确性提高了10-30%，同时将训练时间和计算成本分别减少了50%和60%，与GTR相比。</div>
</details>
</div>
<div class="card">
<div class="title">DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes</div>
<div class="meta-line">Authors: Jiajun Jiang, Yiming Zhu, Zirui Wu, Jie Song</div>
<div class="meta-line">Venue: IEEE Robotics and Automation Letters, Vol. 10, No. 12, pp. 12612-12619, 2025</div>
<div class="meta-line">First: 2025-06-02T17:59:10+00:00 · Latest: 2025-12-15T06:43:59+00:00</div>
<div class="meta-line">Comments: 14 pages, 14 figures. Published in IEEE Robotics and Automation Letters (RA-L), 2025. Code: https://github.com/Eku127/DualMap Project page: https://eku127.github.io/DualMap/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.01950v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.01950v4">PDF</a> · <a href="https://github.com/Eku127/DualMap">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://eku127.github.io/DualMap/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce DualMap, an online open-vocabulary mapping system that enables robots to understand and navigate dynamically changing environments through natural language queries. Designed for efficient semantic mapping and adaptability to changing environments, DualMap meets the essential requirements for real-world robot navigation applications. Our proposed hybrid segmentation frontend and object-level status check eliminate the costly 3D object merging required by prior methods, enabling efficient online scene mapping. The dual-map representation combines a global abstract map for high-level candidate selection with a local concrete map for precise goal-reaching, effectively managing and updating dynamic changes in the environment. Through extensive experiments in both simulation and real-world scenarios, we demonstrate state-of-the-art performance in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation. Project page: https://eku127.github.io/DualMap/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DualMap：动态变化场景中基于自然语言导航的在线开放词汇语义映射</div>
<div class="mono" style="margin-top:8px">我们介绍了DualMap，一种在线开放词汇映射系统，使机器人能够通过自然语言查询理解并导航动态变化的环境。DualMap 旨在实现高效的语义映射和对变化环境的适应性，满足现实世界机器人导航应用的基本要求。我们提出的混合分段前端和对象级状态检查消除了先前方法所需的昂贵的3D对象合并，从而实现高效的在线场景映射。双映射表示结合了一个全局抽象地图用于高层候选选择和一个局部具体地图用于精确目标到达，有效管理并更新环境中的动态变化。通过在仿真和真实场景中的广泛实验，我们展示了在3D开放词汇分割、高效场景映射和在线语言引导导航方面的最新性能。项目页面：https://eku127.github.io/DualMap/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DualMap is an online open-vocabulary mapping system that allows robots to navigate dynamically changing environments through natural language queries. It uses a hybrid segmentation frontend and object-level status check to efficiently map scenes without the need for 3D object merging, and employs a dual-map representation combining a global abstract map and a local concrete map to handle dynamic changes. Extensive experiments show that DualMap outperforms previous methods in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation.</div>
<div class="mono" style="margin-top:8px">DualMap 是一种在线开放词汇映射系统，使机器人能够通过自然语言查询理解并导航动态变化的环境。它使用混合分割前端和对象级状态检查来高效地映射场景，无需进行 3D 对象合并，并采用双地图表示来处理动态变化。广泛的实验表明，DualMap 在 3D 开放词汇分割、高效场景映射和在线语言引导导航方面优于先前的方法。</div>
</details>
</div>
<div class="card">
<div class="title">TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</div>
<div class="meta-line">Authors: Xi Luo, Shixin Xu, Ying Xie, JianZhong Hu, Yuwei He, Yuhui Deng, Huaxiong Huang</div>
<div class="meta-line">First: 2025-12-15T06:08:16+00:00 · Latest: 2025-12-15T06:08:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13008v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13008v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TWLR：基于文本引导的弱监督病变定位与严重程度回归方法及其在可解释性糖尿病视网膜病变分级中的应用</div>
<div class="mono" style="margin-top:8px">准确的医学图像分析可以大大辅助临床诊断，但其效果依赖于高质量的专家注释。获取医学图像的像素级标签，尤其是眼底图像的标签，仍然成本高昂且耗时。同时，尽管深度学习在医学成像领域取得了成功，但缺乏可解释性限制了其在临床中的应用。为了解决这些挑战，我们提出了一种名为TWLR的两阶段框架，用于可解释的糖尿病视网膜病变（DR）评估。在第一阶段，视觉-语言模型将领域特定的眼科知识整合到文本嵌入中，联合执行DR分级和病变分类，有效地将语义医学概念与视觉特征联系起来。第二阶段引入了一种基于弱监督语义分割的迭代严重程度回归框架。通过迭代细化生成的病变显著图引导一种渐进的修复机制，系统地消除病理特征，有效降低疾病严重程度，使其向更健康的眼底外观转变。关键的是，这种严重程度回归方法实现了双重好处：在无需像素级监督的情况下实现准确的病变定位，并提供疾病到健康转变的可解释可视化。在FGADR、DDR和一个私人数据集上的实验结果表明，TWLR在DR分类和病变分割方面均取得了竞争力的表现，提供了一种更可解释和注释高效的自动视网膜图像分析解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">TWLR is a two-stage framework designed to improve diabetic retinopathy (DR) assessment by integrating ophthalmological knowledge with deep learning. In the first stage, a vision-language model combines text and visual features to grade DR and classify lesions. The second stage uses an iterative severity regression approach to refine lesion saliency maps, which helps in systematically reducing disease severity. This method achieves accurate lesion localization and provides an interpretable visualization of disease-to-healthy transformations, demonstrating competitive performance on multiple datasets.</div>
<div class="mono" style="margin-top:8px">TWLR 是一个两阶段框架，通过将眼科知识整合到文本嵌入中进行糖尿病视网膜病变分级和病变分类，并使用基于弱监督语义分割的迭代严重性回归方法来细化病变显著图并降低疾病严重程度。该系统实现了准确的病变定位，并提供了疾病到健康转变的可解释可视化，多项数据集上表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</div>
<div class="meta-line">Authors: Xianzhe Fan, Xuhui Zhou, Chuanyang Jin, Kolby Nottingham, Hao Zhu, Maarten Sap</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-29T00:54:13+00:00 · Latest: 2025-12-15T04:45:27+00:00</div>
<div class="meta-line">Comments: 24 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.23046v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.23046v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model&#x27;s ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SoMi-ToM：评估具身社会互动中的多视角心智理论</div>
<div class="mono" style="margin-top:8px">人类在动态的现实世界社会互动中不断推断他人的状态、目标和行为。然而，大多数心智理论（ToM）基准仅评估静态的文本场景，与实际互动存在显著差距。我们提出了SoMi-ToM基准，旨在评估具身多智能体复杂社会互动中的多视角ToM。该基准基于由互动环境SoMi生成的丰富多模态互动数据，涵盖了多样的制作目标和社会关系。我们的框架支持多层次评估：（1）第一人称评估提供任务期间从第一人称视角的多模态（视觉、对话、动作等）输入，用于实时状态推断；（2）第三人称评估提供任务结束后完整的第三人称视角视频和文本记录，用于目标和行为推断。这种评估方法允许从主观即时体验和客观全局观察两个方面对模型的ToM能力进行更全面的考察。我们构建了一个具有挑战性的数据集，包含35个第三人称视角视频、363个第一人称视角图像和1225个专家标注的多项选择题（三个选项）。在该数据集上，我们系统地评估了人类受试者和几种最先进的大型视觉-语言模型（LVLMs）的表现。结果显示，LVLMs在SoMi-ToM上的表现显著低于人类：第一人称评估中的平均准确率差距为40.1%，第三人称评估中的差距为26.4%。这表明未来LVLMs需要进一步提高其在具身复杂社会互动中的ToM能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to evaluate multi-perspective Theory of Mind (ToM) in embodied social interactions by proposing the SoMi-ToM benchmark. This benchmark uses rich multimodal interaction data from the SoMi interaction environment, covering various crafting goals and social relationships. The evaluation method includes first-person and third-person perspectives, allowing for a comprehensive examination of ToM capabilities. Experiments show that state-of-the-art large vision-language models perform significantly worse than humans, with an average accuracy gap of 40.1% in first-person evaluation and 26.4% in third-person evaluation, highlighting the need for improved ToM capabilities in embodied social interactions.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出SoMi-ToM基准来评估多视角的理论思维（ToM）在实体社会互动中的表现，该基准使用来自SoMi互动环境的丰富多模态交互数据。基准通过第一人称和第三人称视角进行评估，结果显示最先进的大型视觉-语言模型的表现明显不如人类，第一人称评估的平均准确率差距为40.1%，第三人称评估为26.4%。这表明未来模型需要在复杂的社会互动中进一步提高其ToM能力。</div>
</details>
</div>
<div class="card">
<div class="title">Content Adaptive based Motion Alignment Framework for Learned Video Compression</div>
<div class="meta-line">Authors: Tiange Zhang, Xiandong Meng, Siwei Ma</div>
<div class="meta-line">First: 2025-12-15T02:51:47+00:00 · Latest: 2025-12-15T02:51:47+00:00</div>
<div class="meta-line">Comments: Accepted to Data Compression Conference (DCC) 2026 as a poster paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12936v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.12936v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于内容自适应的运动对齐框架以实现学习视频压缩</div>
<div class="mono" style="margin-top:8px">端到端视频压缩的最新进展因其统一的端到端学习优化而显示出有希望的结果。然而，这样的通用框架往往缺乏内容特定的适应性，导致压缩性能不佳。为了解决这个问题，本文提出了一种基于内容自适应的运动对齐框架，通过根据不同的内容特性调整编码策略来提高性能。具体而言，我们首先引入了一种两阶段的流导向可变形扭曲机制，通过粗到细的偏移预测和掩码调制来细化运动补偿，从而实现精确的特征对齐。其次，我们提出了一种多参考质量感知策略，根据参考质量调整失真权重，并将其应用于分层训练以减少误差传播。第三，我们整合了一个无需训练的模块，通过运动幅度和分辨率对帧进行下采样，以获得平滑的运动估计。在标准测试数据集上的实验结果表明，我们的框架CAMA在所有最先进的神经视频压缩模型中取得了显著的改进，相对于我们的基线模型DCVC-TCM，实现了24.95%的BD-rate（PSNR）节省，同时在重新实现的DCVC-DC和传统编解码器HM-16.25上也表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes a content adaptive based motion alignment framework (CAMA) to improve end-to-end video compression performance. It introduces a two-stage flow-guided deformable warping mechanism for precise feature alignment, a multi-reference quality-aware strategy to adjust distortion weights, and a training-free module for smooth motion estimation. Experiments show that CAMA outperforms state-of-the-art models, achieving a 24.95% BD-rate (PSNR) reduction compared to the baseline model DCVC-TCM and traditional codec HM-16.25.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于内容自适应的运动对齐框架（CAMA），以提高端到端视频压缩性能。该框架引入了两阶段流动引导可变形变形机制以实现精确特征对齐，多参考质量感知策略以调整失真权重，并引入了无训练模块以获得平滑的运动估计。实验结果表明，CAMA 模型优于最先进的模型，与基线模型 DCVC-TCM 和传统编解码器 HM-16.25 相比，BD-rate（PSNR）减少了 24.95%。</div>
</details>
</div>
<div class="card">
<div class="title">ID-Crafter: VLM-Grounded Online RL for Compositional Multi-Subject Video Generation</div>
<div class="meta-line">Authors: Panwang Pan, Jingjing Zhao, Yuchen Lin, Chenguo Lin, Chenxin Li, Hengyu Liu, Tingting Shen, Yadong MU</div>
<div class="meta-line">First: 2025-11-01T11:29:14+00:00 · Latest: 2025-12-15T02:43:16+00:00</div>
<div class="meta-line">Comments: Project page: https://angericky.github.io/ID-Crafter, Code: https://github.com/paulpanwang/IDCrafter</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00511v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.00511v4">PDF</a> · <a href="https://github.com/paulpanwang/IDCrafter">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://angericky.github.io/ID-Crafter">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Significant progress has been achieved in high-fidelity video synthesis, yet current paradigms often fall short in effectively integrating identity information from multiple subjects. This leads to semantic conflicts and suboptimal performance in preserving identities and interactions, limiting controllability and applicability. To tackle this issue, we introduce ID-Crafter, a framework for multi-subject video generation that achieves superior identity preservation and semantic coherence. ID-Crafter integrates three key components: (i) a hierarchical identity-preserving attention mechanism that progressively aggregates features at intra-subject, inter-subject, and cross-modal levels; (ii) a semantic understanding module powered by a pretrained Vision-Language Model (VLM) to provide fine-grained guidance and capture complex inter-subject relationships; and (iii) an online reinforcement learning phase to further refine the model for critical concepts. Furthermore, we construct a new dataset to facilitate robust training and evaluation. Extensive experiments demonstrate that ID-Crafter establishes new state-of-the-art performance on multi-subject video generation benchmarks, excelling in identity preservation, temporal consistency, and overall video quality. Project page: https://angericky.github.io/ID-Crafter</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ID-Crafter：基于VLM的在线强化学习多主体视频生成</div>
<div class="mono" style="margin-top:8px">在高保真视频合成方面取得了显著进展，但当前范式往往难以有效整合多主体的身份信息，导致语义冲突和身份及互动的次优表现，限制了可控性和应用范围。为解决这一问题，我们提出了ID-Crafter，一种实现多主体视频生成的框架，能够实现卓越的身份保留和语义一致性。ID-Crafter 结合了三个关键组件：(i) 一种分层的身份保留注意力机制，逐步在主体内、主体间和跨模态层面聚合特征；(ii) 由预训练的视觉-语言模型（VLM）驱动的语义理解模块，提供精细的指导并捕捉复杂的主体间关系；(iii) 一个在线强化学习阶段，进一步细化模型以优化关键概念。此外，我们构建了一个新的数据集以促进稳健的训练和评估。广泛的实验表明，ID-Crafter 在多主体视频生成基准测试中建立了新的最佳性能，特别是在身份保留、时间一致性和整体视频质量方面表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ID-Crafter is a framework for multi-subject video generation that addresses the challenge of integrating identity information from multiple subjects. It uses a hierarchical identity-preserving attention mechanism, a semantic understanding module powered by a pretrained Vision-Language Model, and an online reinforcement learning phase. Experiments show that ID-Crafter outperforms existing methods in identity preservation, temporal consistency, and overall video quality on multi-subject video generation benchmarks.</div>
<div class="mono" style="margin-top:8px">ID-Crafter 是一个多主体视频生成框架，旨在解决身份保持和语义一致性的问题。它采用了层次化的身份保留注意力机制、基于预训练视觉-语言模型的语义理解模块以及在线强化学习阶段。实验表明，ID-Crafter 在多主体视频生成基准测试中，在身份保持、时间一致性以及整体视频质量方面均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</div>
<div class="meta-line">Authors: Zhenkai Wu, Xiaowen Ma, Zhenliang Ni, Dengming Zhang, Han Shu, Xin Jiang, Xinghao Chen</div>
<div class="meta-line">First: 2025-12-02T12:30:05+00:00 · Latest: 2025-12-15T02:13:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.02700v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.02700v2">PDF</a> · <a href="https://github.com/Casey-bit/VLMPruner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup. The code is available at https://github.com/Casey-bit/VLMPruner.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLM-Pruner：高效VLM中基于离心式标记剪枝的空间稀疏性缓冲</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在图像理解任务中表现出色，但大量的视觉标记导致了显著的计算成本，阻碍了其在移动设备上的部署。许多剪枝方法仅依赖于标记的重要性，从而忽视了标记间的冗余性，保留了大量重复的标记，浪费了容量。尽管已经提出了一些具有冗余意识的方法，但它们往往忽略了视觉标记之间的空间关系。这可能导致保留标记的选择过于稀疏，无法充分覆盖目标对象的区域。为了解决这些限制，我们提出了VLM-Pruner，这是一种无需训练的标记剪枝算法，明确平衡冗余性和空间稀疏性。我们引入了一种离心式标记剪枝范式，能够在优先保留细粒度对象细节的同时，实现从近到远的选择。此外，我们设计了一种空间稀疏性缓冲（BSS）准则，推迟选择空间上距离较远的标记。我们还采用了一种并行贪婪策略来高效地进行标记选择。为了减轻剪枝带来的信息损失，我们有选择地将被丢弃标记中的重要信息融合到保留的标记中。全面的比较表明，VLM-Pruner在五个VLM中以88.9%的剪枝率持续优于强大的基线模型，同时实现了端到端的推理加速。代码可在https://github.com/Casey-bit/VLMPruner获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VLM-Pruner is a training-free token pruning algorithm designed to address the computational challenges of vision-language models (VLMs) by balancing redundancy and spatial sparsity. It introduces a centrifugal token pruning paradigm and a Buffering for Spatial Sparsity (BSS) criterion to efficiently select tokens while preserving fine-grained object details. Experimental results show that VLM-Pruner outperforms strong baselines with an 88.9% pruning rate and provides an end-to-end inference speedup without significant loss of performance.</div>
<div class="mono" style="margin-top:8px">VLM-Pruner 是一种无需训练的 token 剪枝算法，旨在通过平衡冗余和空间稀疏性来解决视觉语言模型（VLMs）的计算挑战。它引入了离心 token 剪枝范式和空间稀疏性缓冲（BSS）标准，以保留细粒度的物体细节。实验结果表明，VLM-Pruner 在 88.9% 的剪枝率下优于强基线，并在五个 VLM 上提供端到端的推理加速同时保持性能。</div>
</details>
</div>
<div class="card">
<div class="title">SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</div>
<div class="meta-line">Authors: Minghao Zhu, Zhihao Zhang, Anmol Sidhu, Keith Redmill</div>
<div class="meta-line">First: 2025-12-14T23:56:34+00:00 · Latest: 2025-12-14T23:56:34+00:00</div>
<div class="meta-line">Comments: Submitted to IV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12885v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.12885v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework&#x27;s effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SignRAG：一种用于可扩展零样本道路标志识别的检索增强系统</div>
<div class="mono" style="margin-top:8px">自动道路标志识别是智能交通系统中的关键任务，但传统的深度学习方法难以应对标志类别的庞大数量以及创建详尽标注数据集的不切实际性。本文提出了一种新颖的零样本识别框架，将检索增强生成（RAG）范式适应于解决这一挑战。我们的方法首先使用视觉语言模型（VLM）从输入图像生成标志的文本描述。该描述用于从参考设计向量数据库中检索最相关的标志候选集。随后，大型语言模型（LLM）对检索到的候选集进行推理以做出最终的细粒度识别。我们在来自俄亥俄州 MUTCD 的全面的 303 个监管标志数据集上验证了该方法。实验结果表明该框架的有效性，在理想参考图像上准确率为 95.58%，在具有挑战性的实际道路数据上准确率为 82.45%。这项工作证明了基于 RAG 的架构在无需特定任务训练的情况下创建可扩展且准确的道路标志识别系统的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper presents SignRAG, a zero-shot road sign recognition system that uses a Retrieval-Augmented Generation (RAG) approach. It leverages a Vision Language Model (VLM) to generate a textual description of a sign from an image, which is then used to retrieve relevant sign candidates from a vector database. A Large Language Model (LLM) then reasons over these candidates to make a final recognition. The system was tested on 303 regulatory signs and achieved 95.58% accuracy on ideal images and 82.45% on real-world data, showcasing its effectiveness in scalable and accurate road sign recognition without task-specific training.</div>
<div class="mono" style="margin-top:8px">该论文提出了SignRAG，一种使用检索增强生成（RAG）方法的零样本道路标志识别系统。该系统利用视觉语言模型（VLM）从图像中生成标志的文本描述，然后从矢量数据库中检索相关标志候选者。大型语言模型（LLM）随后对这些候选者进行推理以做出最终识别。该系统在303个监管标志上进行了测试，理想图像上的准确率为95.58%，真实世界数据上的准确率为82.45%，展示了其在无需特定任务训练的情况下实现高效和准确的道路标志识别的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners</div>
<div class="meta-line">Authors: N. K. B. M. P. K. B. Narasinghe, Uthayasanker Thayasivam</div>
<div class="meta-line">First: 2025-12-14T20:13:21+00:00 · Latest: 2025-12-14T20:13:21+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures. Accepted to VISAPP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12824v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.12824v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa&#x27;s distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an &quot;augmentation divergence&quot;: while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>适应少样本学习的多模态基础模型：对比式图释器的全面研究</div>
<div class="mono" style="margin-top:8px">大规模多模态基础模型，尤其是对比式图释器（CoCa），通过统一对比对齐与生成图释，实现了最先进的结果。虽然零样本迁移能力已被广泛记录，但这些生成-对比混合模型如何适应下游任务（少样本学习）在极端数据稀缺的情况下仍处于探索阶段。现有文献主要集中在双编码器架构如CLIP上，留下了对CoCa独特潜空间如何响应参数高效微调（PEFT）的理解空白。本文对适应CoCa视觉主干进行少样本图像分类的全面经验研究进行了介绍。我们系统地评估了一系列策略，从无需训练的混合原型到通过低秩适应（LoRA）进行深度参数适应。首先，我们发现了一种“增强偏差”：虽然强大的数据增强在少样本设置中会降低线性探针的性能，但它是稳定LoRA微调的关键。我们还证明，结合监督对比损失（SupCon）的混合目标在不同样本数量下比标准交叉熵提供了更一致的性能提升。至关重要的是，我们描述了训练配置对数据稀缺性的敏感性，提供了正则化、秩和采样策略的实证参考设置，以促进生成-对比基础模型的有效适应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the adaptation of Contrastive Captioners (CoCa) for few-shot learning, focusing on parameter-efficient fine-tuning methods. The study evaluates various strategies, from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). Key findings include the importance of strong data augmentation for stabilizing LoRA fine-tuning and the consistent performance gains from hybrid objectives incorporating Supervised Contrastive (SupCon) loss. The research also characterizes the sensitivity of training configurations to data scarcity, offering empirical guidelines for efficient model adaptation in few-shot settings.</div>
<div class="mono" style="margin-top:8px">本文探讨了如何通过参数高效微调方法将对比生成器（CoCa）适应于少量样本学习。研究评估了从无训练混合原型到通过低秩适应（LoRA）进行深度参数微调的各种策略，并发现强数据增强对于稳定LoRA微调至关重要。研究还表明，结合监督对比（SupCon）损失的混合目标在不同样本数量下能持续提升性能。关键发现包括训练配置对数据稀缺性的敏感性，这有助于调整正则化、秩和采样策略以高效适应生成-对比基础模型。</div>
</details>
</div>
<div class="card">
<div class="title">SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling</div>
<div class="meta-line">Authors: Fanjiang Ye, Zepeng Zhao, Yi Mu, Jucheng Shen, Renjie Li, Kaijian Wang, Saurabh Agarwal, Myungjin Lee, Triston Cao, Aditya Akella, Arvind Krishnamurthy, T. S. Eugene Ng, Zhengzhong Tu, Yuke Wang</div>
<div class="meta-line">First: 2025-08-25T07:49:17+00:00 · Latest: 2025-12-14T17:45:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.17756v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.17756v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SUPERGEN, an efficient tile-based framework for ultra-high-resolution video generation. SUPERGEN features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SUPERGEN incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SUPERGEN also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations show that SUPERGEN maximizes performance gains while achieving high output quality across various benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SuperGen：一种高效的超高清视频生成系统，结合素描和拼块技术</div>
<div class="mono" style="margin-top:8px">扩散模型在生成任务（例如图像和视频生成）中最近取得了显著的成功，对高质量内容（例如2K/4K视频）的需求在各个领域迅速增加。然而，由于过度的重新训练要求和高昂的计算和内存成本，现有标准分辨率（例如720p）平台生成超高清视频仍然具有挑战性。为此，我们引入了SUPERGEN，这是一种高效的基于拼块的超高清视频生成框架。SUPERGEN 特设了一种新颖的无需训练的算法创新，通过拼块技术成功支持广泛的分辨率范围，无需额外的训练努力，同时显著减少了内存占用和计算复杂度。此外，SUPERGEN 结合了一种拼块定制的、自适应的、区域感知的缓存策略，通过利用去噪步骤和空间区域之间的冗余性来加速视频生成。SUPERGEN 还集成了基于缓存、通信最小化的拼块并行性，以提高吞吐量并最小化延迟。评估表明，SUPERGEN 在各种基准测试中实现了高性能增益，同时保持了高质量的输出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SUPERGEN is an efficient tile-based framework for ultra-high-resolution video generation that addresses the challenges of excessive re-training and high computational costs. It uses a training-free tiling algorithm to support various resolutions without additional training and reduces memory footprint and computational complexity. SUPERGEN also employs an adaptive caching strategy and cache-guided parallelism to accelerate video generation and minimize latency, achieving high output quality across benchmarks.</div>
<div class="mono" style="margin-top:8px">SUPERGEN 是一种高效的基于瓷砖的超高清视频生成框架，解决了过度重新训练和高计算成本的问题。它采用无训练的分块方法支持多种分辨率，同时减少内存和计算复杂度。SUPERGEN 还采用自适应缓存策略和缓存引导的并行计算来加速视频生成。实验表明，SUPERGEN 在不同基准测试中实现了高质量的输出，同时最大化了性能提升。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251216_0326.html">20251216_0326</a>
<a href="archive/20251215_0321.html">20251215_0321</a>
<a href="archive/20251214_0316.html">20251214_0316</a>
<a href="archive/20251213_0319.html">20251213_0319</a>
<a href="archive/20251212_0322.html">20251212_0322</a>
<a href="archive/20251211_0319.html">20251211_0319</a>
<a href="archive/20251210_0317.html">20251210_0317</a>
<a href="archive/20251209_0321.html">20251209_0321</a>
<a href="archive/20251208_0317.html">20251208_0317</a>
<a href="archive/20251207_0317.html">20251207_0317</a>
<a href="archive/20251206_0318.html">20251206_0318</a>
<a href="archive/20251205_0320.html">20251205_0320</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
