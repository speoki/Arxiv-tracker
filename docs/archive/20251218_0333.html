<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-18 03:33</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251218_0333</div>
    <div class="row"><div class="card">
<div class="title">VIBE: Can a VLM Read the Room?</div>
<div class="meta-line">Authors: Tania Chakraborty, Eylon Caplan, Dan Goldwasser</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2025-06-11T19:07:35+00:00 · Latest: 2025-12-16T18:42:51+00:00</div>
<div class="meta-line">Comments: Findings of EMNLP, 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.11162v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.11162v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding human social behavior such as recognizing emotions and the social dynamics causing them is an important and challenging problem. While LLMs have made remarkable advances, they are limited to the textual domain and cannot account for the major role that non-verbal cues play in understanding social situations. Vision Language Models (VLMs) can potentially account for this gap, however their ability to make correct inferences over such social cues has received little attention. In this paper, we explore the capabilities of VLMs at social reasoning. We identify a previously overlooked limitation in VLMs: the Visual Social-Pragmatic Inference gap. To target this gap, we propose a new task for VLMs: Visual Social-Pragmatic Inference. We construct a high quality dataset to test the abilities of a VLM for this task and benchmark the performance of several VLMs on it.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIBE：VLM能否读懂房间里的社交信号？</div>
<div class="mono" style="margin-top:8px">理解人类社会行为，如识别情绪及其背后的社会动态，是一个重要且具有挑战性的问题。尽管语言模型（LLMs）取得了显著进展，但它们仅限于文本领域，无法解释非言语线索在理解社交情境中的重要作用。视觉语言模型（VLMs）有可能弥补这一差距，但它们在推理此类社会线索方面的能力尚未受到广泛关注。在本文中，我们探讨了VLM在社会推理方面的能力。我们发现VLM的一个先前未被注意到的局限性：视觉社会-语用推理差距。为解决这一差距，我们为VLM提出了一项新任务：视觉社会-语用推理。我们构建了一个高质量的数据集来测试VLM在该任务上的能力，并在该数据集上对几种VLM进行了基准测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the social reasoning capabilities of Vision Language Models (VLMs) by introducing a new task called Visual Social-Pragmatic Inference. The authors identify a limitation in VLMs known as the Visual Social-Pragmatic Inference gap and propose a high-quality dataset to evaluate VLMs. The study benchmarks several VLMs on this task, highlighting their current limitations in understanding non-verbal social cues.</div>
<div class="mono" style="margin-top:8px">本文通过引入一个新的任务——视觉社会语用推理，研究视觉语言模型（VLM）的社会推理能力。作者指出了VLM中存在的视觉社会语用推理差距，并构建了一个高质量的数据集来评估VLM。研究对几个VLM进行了基准测试，揭示了它们在理解非言语社会线索方面的当前局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</div>
<div class="meta-line">Authors: Antonio Guillen-Perez</div>
<div class="meta-line">First: 2025-12-12T20:07:04+00:00 · Latest: 2025-12-16T17:15:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12012v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12012v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of &quot;Long-Tail&quot; training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a &quot;System 2&quot; inference-time alignment strategy, utilizing a multi-model &quot;Judge-Scout&quot; consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40% ccompared to the best single scout models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Semantic-Drive addresses the challenge of curating long-tail data for autonomous vehicles by introducing a local-first, neuro-symbolic framework. It uses a real-time open-vocabulary detector (YOLOE) for symbolic grounding and a reasoning VLM for cognitive analysis. This system achieves a recall of 0.966 in identifying rare safety-critical events compared to 0.475 for CLIP and reduces risk assessment error by 40% compared to the best single scout models. It runs on consumer hardware, offering a privacy-preserving alternative to cloud-based solutions.</div>
<div class="mono" style="margin-top:8px">Semantic-Drive通过引入一个本地优先、神经符号框架来解决自动驾驶汽车中长尾数据标注的挑战。该系统使用实时开放词汇检测器（YOLOE）进行符号接地，并使用推理VLM进行认知分析。该系统在识别罕见的安全关键事件方面的召回率为0.966，而CLIP为0.475，并且与最佳单个侦察模型相比，风险评估误差降低了40%。它运行在消费级硬件上，提供了一种比云解决方案更具隐私保护的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</div>
<div class="meta-line">Authors: Dabin Jeong, Amirhossein Vahidi, Ciro Ramírez-Suástegui, Marie Moullet, Kevin Ly, Mohammad Vali Sanian, Sebastian Birk, Yinshui Chang, Adam Boxall, Daniyal Jafree, Lloyd Steele, Vijaya Baskar MS, Muzlifah Haniffa, Mohammad Lotfollahi</div>
<div class="meta-line">First: 2025-11-19T14:22:23+00:00 · Latest: 2025-12-16T16:01:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.15464v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.15464v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIGMMA：基于图的多尺度多模态对比对齐框架在组织病理图像和空间转录组学之间的层次表示</div>
<div class="mono" style="margin-top:8px">计算病理学的最新进展利用视觉-语言模型学习苏木精和伊红（HE）图像与空间转录组学（ST）谱型的联合表示。然而，现有方法通常在单尺度上对HE切片与其对应的ST谱型进行对齐，忽视了细微的细胞结构及其空间组织。为了解决这个问题，我们提出Sigmma，一种多模态对比对齐框架，用于在多个尺度上学习HE图像和空间转录组学谱型的层次表示。Sigmma 引入了多尺度对比对齐，确保不同尺度下学习的表示在模态间保持一致。此外，通过将细胞相互作用表示为图，并整合跨子图和子图内部关系，我们的方法有效地捕捉了组织微环境中从精细到粗略的细胞-细胞相互作用。我们证明Sigmma 学习的表示更好地捕捉了跨模态对应关系，在基因表达预测任务中平均提高了9.78%，在跨模态检索任务中平均提高了26.93%。我们进一步表明，它在下游分析中学习了有意义的多组织组织。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the alignment of histopathology images and spatial transcriptomic profiles by addressing the limitations of existing single-scale approaches. Sigmma, a hierarchical graph-based multi-scale multi-modal contrastive alignment framework, is proposed to learn coherent representations across multiple scales. The method captures cell-cell interactions from fine to coarse levels, enhancing cross-modal correspondences. Experimental results show a 9.78% improvement in gene-expression prediction and a 26.93% improvement in cross-modal retrieval across datasets.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有单尺度方法的局限性，提高组织病理图像和空间转录组学资料的对齐。Sigmma 是一个多模态对比对齐框架，引入了多尺度对比对齐，并将细胞相互作用表示为图，以捕捉从精细到粗略的细胞-细胞相互作用。该方法显著提高了跨模态对应关系，分别在基因表达预测和跨模态检索任务中提高了9.78%和26.93%。此外，它还有助于下游分析中的有意义的多组织组织。</div>
</details>
</div>
<div class="card">
<div class="title">A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</div>
<div class="meta-line">Authors: Zixin Zhang, Kanghao Chen, Hanqing Wang, Hongfei Zhang, Harold Haodong Chen, Chenfei Liao, Litao Guo, Ying-Cong Chen</div>
<div class="meta-line">First: 2025-12-16T14:27:47+00:00 · Latest: 2025-12-16T14:27:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14442v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A4-Agent：一种用于零样本功能推理的代理框架</div>
<div class="mono" style="margin-top:8px">功能预测识别基于语言指令的对象上的交互区域，对于体态人工智能至关重要。现有的端到端模型将高层推理和低层语义结合在一个单一的管道中，并依赖于标注数据集的训练，这导致在新对象和未见过的环境中泛化能力较差。在本文中，我们提出了A4-Agent，一种无需训练的代理框架，将功能预测拆分为三个阶段的管道。我们的框架在测试时协调专门的基础模型：(1) 一个**Dreamer**，使用生成模型来可视化**如何**进行交互；(2) 一个**Thinker**，利用大型视觉语言模型来决定**与哪个**对象部分进行交互；(3) 一个**Spotter**，协调视觉基础模型来精确定位**哪里**是交互区域。通过利用预训练模型的互补优势，而无需任何特定任务的微调，我们的零样本框架在多个基准测试中显著优于最先进的监督方法，并在真实世界环境中展示了鲁棒的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces A4-Agent, a zero-shot framework for affordance prediction that decouples the process into three stages: Dreamer, Thinker, and Spotter. Dreamer visualizes how an interaction would look, Thinker decides which object part to interact with, and Spotter locates the precise interaction area. This framework, which leverages pre-trained models without fine-tuning, outperforms existing supervised methods and shows robust generalization to real-world settings.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有端到端模型的局限性，提高体态AI中的功能预测。提出的A4-Agent框架将过程分为三个阶段：Dreamer、Thinker和Spotter，分别用于可视化交互场景、决定交互的对象部分以及定位交互区域。该框架利用预训练模型而不进行微调，实现了在各种基准测试中的优越零样本性能，并在真实世界环境中表现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy</div>
<div class="meta-line">Authors: Zhuo Chen, Fanyue Wei, Runze Xu, Jingjing Li, Lixin Duan, Angela Yao, Wen Li</div>
<div class="meta-line">First: 2025-12-16T14:08:00+00:00 · Latest: 2025-12-16T14:08:00+00:00</div>
<div class="meta-line">Comments: Project page:https://synps26.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14423v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14423v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://synps26.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意力共享中的魔鬼在于注意力协同：通过注意力协同提高复杂非刚性图像编辑的忠实度</div>
<div class="mono" style="margin-top:8px">无需训练的大规模扩散模型使图像编辑变得实用，但执行复杂的非刚性编辑（例如姿态或形状变化）仍然极具挑战性。我们发现一个关键原因：现有注意力共享机制中的注意力崩溃，其中位置嵌入或语义特征之一主导视觉内容检索，导致过度编辑或不足编辑。为解决这一问题，我们引入了SynPS方法，该方法协同利用位置嵌入和语义信息进行忠实的非刚性图像编辑。我们首先提出了一种编辑测量方法，量化每个去噪步骤所需的编辑量。基于此测量，我们设计了一种注意力协同管道，动态调节位置嵌入的影响，使SynPS能够平衡语义修改和保真度保留。通过适配性地整合位置和语义线索，SynPS有效地避免了过度编辑和不足编辑。在公共和新收集的基准上的广泛实验表明，我们方法的优越性能和忠实度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of performing complex non-rigid image edits using large diffusion models, which often result in over- or under-editing due to attention collapse. To tackle this, the authors introduce SynPS, a method that synergistically combines positional embeddings and semantic information. SynPS uses an editing measurement to dynamically adjust the influence of positional and semantic cues, ensuring faithful non-rigid edits. Experiments show that SynPS outperforms existing methods in terms of both performance and faithfulness.</div>
<div class="mono" style="margin-top:8px">论文解决了使用大型扩散模型进行复杂非刚性图像编辑时经常出现过度编辑或不足编辑的问题，这是由于注意力坍塌造成的。为此，作者提出了SynPS方法，该方法结合了位置嵌入和语义信息。SynPS利用编辑测量动态调整位置和语义线索的影响，确保进行忠实的非刚性编辑。实验表明，SynPS在性能和忠实度方面均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning</div>
<div class="meta-line">Authors: Nakamasa Inoue, Kanoko Goto, Masanari Oi, Martyna Gruszka, Mahiro Ukai, Takumi Hirose, Yusuke Sekikawa</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-16T14:06:35+00:00 · Latest: 2025-12-16T14:06:35+00:00</div>
<div class="meta-line">Comments: Paper accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14420v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14420v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the robustness of automatic image caption evaluation, especially under domain-shift conditions. DISCODE, a finetuning-free method, generates evaluation scores aligned with human judgments by using an Adaptive Test-Time (ATT) loss with a Gaussian prior. Experiments show that DISCODE outperforms existing reference-free metrics on MCEval and four other benchmarks.</div>
<div class="mono" style="margin-top:8px">研究旨在提高自动图像字幕评估的鲁棒性，特别是在领域转换场景下。DISCODE是一种无需微调的方法，通过引入带有高斯先验的Adaptive Test-Time (ATT)损失来生成与人类判断一致的评估分数。实验表明，DISCODE在Multi-domain Caption Evaluation (MCEval)基准和四个其他基准上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Semantic Transformer for 3D Scene Understanding</div>
<div class="meta-line">Authors: Sebastian Koch, Johanna Wald, Hide Matsuki, Pedro Hermosilla, Timo Ropinski, Federico Tombari</div>
<div class="meta-line">First: 2025-12-16T12:49:35+00:00 · Latest: 2025-12-16T12:49:35+00:00</div>
<div class="meta-line">Comments: Project page: https://unite-page.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14364v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14364v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://unite-page.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一语义变换器用于3D场景理解</div>
<div class="mono" style="margin-top:8px">整体3D场景理解涉及捕获和解析未结构化的3D环境。由于现实世界的固有复杂性，现有的模型大多被开发出来并局限于特定任务。我们引入了UNITE，一种用于3D场景理解的统一语义变换器，这是一种新颖的前馈神经网络，能够在一个模型中统一多种3D语义任务。我们的模型以端到端的方式处理未见过的场景，并且只需几秒钟即可推断出完整的3D语义几何结构。我们的方法能够直接预测多个语义属性，包括3D场景分割、实例嵌入、开放词汇特征，以及操作性和关节，仅从RGB图像中。该方法通过结合2D蒸馏训练，高度依赖于自我监督，并利用了设计用于确保3D视图一致性的新型多视图损失。我们证明，UNITE在多种不同的语义任务上达到了最先进的性能，并且在许多情况下甚至超过了特定任务的模型，甚至超越了在真实3D几何上操作的方法。请参见项目网站：unite-page.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UNITE is a unified semantic transformer designed for holistic 3D scene understanding, capable of predicting multiple semantic attributes such as 3D scene segmentation and instance embeddings directly from RGB images. It uses a combination of 2D distillation and self-supervision, along with novel multi-view losses to ensure 3D view consistency. UNITE outperforms task-specific models and even surpasses methods using ground truth 3D geometry on several semantic tasks, achieving state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一个统一模型来处理各种语义任务。引入了UNITE，一种统一语义变换器，能够从RGB图像中预测多个语义属性。该模型通过2D蒸馏和自我监督进行训练，实现了多个任务的最先进性能，在许多情况下甚至优于特定任务模型，且能在未见过的场景中仅需几秒即可推断出完整的3D语义几何。项目网站见unite-page.github.io</div>
</details>
</div>
<div class="card">
<div class="title">Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure</div>
<div class="meta-line">Authors: Jooyeol Yun, Jaegul Choo</div>
<div class="meta-line">First: 2025-12-16T12:03:46+00:00 · Latest: 2025-12-16T12:03:46+00:00</div>
<div class="meta-line">Comments: yeolj00.github.io/personal-projects/vector-prism</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14336v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14336v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of automating the animation of Scalable Vector Graphics (SVG) using vision-language models (VLMs). The method involves recovering the semantic structure of SVGs by aggregating multiple weak part predictions, which helps VLMs to better understand and animate the graphics coherently. Experiments show significant improvements over existing approaches, indicating that semantic recovery is crucial for robust SVG animation and more interpretable interactions between VLMs and vector graphics.</div>
<div class="mono" style="margin-top:8px">研究旨在通过视觉-语言模型（VLMs）自动化SVG动画。方法是通过聚合多个弱部分预测来恢复SVG的语义结构，使VLMs能够生成更连贯的动画。关键发现表明，相较于现有方法，语义恢复对于实现稳健的SVG动画和VLMs与矢量图形之间更可解释的交互至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region</div>
<div class="meta-line">Authors: Akila Premarathna, Kanishka Hewageegana, Garcia Andarcia Mariangel</div>
<div class="meta-line">First: 2025-12-16T11:28:55+00:00 · Latest: 2025-12-16T11:28:55+00:00</div>
<div class="meta-line">Comments: 9 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14312v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14312v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8&#x27;s true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study compares vision-language models (VLMs) with YOLOv8 for identifying wastewater treatment plants (WWTPs) in satellite images from the Middle East and North Africa. VLMs were evaluated in zero-shot and few-shot settings, with a dataset of 83,566 high-resolution images from Egypt, Saudi Arabia, and the UAE. Zero-shot evaluations showed that several VLMs outperformed YOLOv8, with Gemma-3 achieving the highest true positive rate. The results suggest that VLMs can replace YOLOv8 for efficient, annotation-free WWTP classification, facilitating scalable remote sensing.</div>
<div class="mono" style="margin-top:8px">该研究旨在利用卫星图像提高中东和北非（MENA）地区污水处理厂（WWTPs）的识别。传统方法如YOLOv8需要大量人工标注，而视觉语言模型（VLMs）则提供了更高效的替代方案。研究在零样本和少量样本设置下比较了VLMs，使用来自埃及、沙特阿拉伯和阿联酋的83,566张高分辨率卫星图像。Gemma-3等VLMs在零样本评估中优于YOLOv8，表明VLMs可以替代YOLOv8进行无标注的WWTP分类，实现可扩展的遥感监测。</div>
</details>
</div>
<div class="card">
<div class="title">SAM3-I: Segment Anything with Instructions</div>
<div class="meta-line">Authors: Jingjing Li, Yue Feng, Yuchen Guo, Jincai Huang, Yongri Piao, Qi Bi, Miao Zhang, Xiaoqi Zhao, Qiang Chen, Shihao Zou, Wei Ji, Huchuan Lu, Li Cheng</div>
<div class="meta-line">First: 2025-12-04T09:00:25+00:00 · Latest: 2025-12-16T11:17:40+00:00</div>
<div class="meta-line">Comments: Preliminary results; work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04585v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04585v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3&#x27;s existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAM3-I: 按指令分割一切</div>
<div class="mono" style="margin-top:8px">SAM3模型通过可提示的概念分割实现了高级的开放词汇分割，允许用户分割给定概念的所有实例，通常用简短的名词短语（NP）提示指定。虽然这标志着SAM家族首次将语言级概念集成进来，但在实际应用中，通常需要更丰富的表达，包括属性、空间关系、功能、动作、状态，甚至实例间的隐式推理。目前，SAM3依赖外部多模态代理将复杂指令转换为NP，然后进行迭代掩码过滤。然而，这些NP级概念过于粗略，往往无法精确表示特定实例。在此项工作中，我们提出了SAM3-I，这是一种增强框架，将概念级理解和指令级推理统一在SAM家族中。SAM3-I引入了一种指令感知级联适应机制，逐步将表达性的指令语义与SAM3现有的视觉-语言表示对齐，从而实现直接的指令遵循分割，同时保留其原有的概念驱动能力。此外，我们设计了一种结构化的指令分类体系，涵盖概念、简单和复杂三个层级，并开发了一个可扩展的数据引擎来构建包含多样指令-掩码对的数据集。实验表明，SAM3-I表现出令人满意的效果，证明SAM3可以有效扩展以遵循自然语言指令，同时保持其强大的概念基础。我们开源了SAM3-I，并提供了实用的微调工作流程，使研究人员能够将其应用于特定领域。源代码可在此获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SAM3-I enhances open-vocabulary segmentation by integrating instruction-level reasoning with concept-level understanding, allowing direct segmentation based on natural-language instructions without losing concept-driven capabilities. Experiments show that SAM3-I performs well, enabling SAM3 to follow complex instructions effectively while maintaining its strong concept grounding. The source code is available for researchers to adapt it to specific domains.</div>
<div class="mono" style="margin-top:8px">SAM3-I通过将指令级推理与概念级理解相结合，增强了开放词汇分割，允许直接遵循指令进行分割。它引入了一种指令感知的级联适应机制，将表达性的指令语义与SAM3的视觉语言表示进行对齐。实验表明，SAM3-I在保持强大的概念基础的同时，能够有效地处理复杂的指令，展示了在自然语言指令跟随方面的良好性能。源代码可供研究人员将其应用于特定领域。</div>
</details>
</div>
<div class="card">
<div class="title">Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models</div>
<div class="meta-line">Authors: Ishaan Gangwani, Aayam Bansal</div>
<div class="meta-line">Venue: ICML</div>
<div class="meta-line">First: 2025-11-30T13:17:08+00:00 · Latest: 2025-12-16T10:51:18+00:00</div>
<div class="meta-line">Comments: ICML NewInML</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00888v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00888v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in &lt;= 0.40 s and &lt;= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>轻量级基准揭示零样本表格式基础模型的隐藏硬件成本</div>
<div class="mono" style="margin-top:8px">零样本基础模型（FMs）承诺在表格式数据上无需训练即可进行预测，但其硬件占用情况尚未得到充分描述。我们提供了一个完全可复现的基准测试，该测试在四个公开数据集（Adult-Income、Higgs-100k、Wine-Quality 和 California-Housing）上报告了测试准确率、墙钟延迟、峰值CPU RAM 和峰值GPU VRAM。在单个NVIDIA T4 GPU 上，两种开源FMs（TabPFN-1.0 和 TabICL-base）与调优后的XGBoost、LightGBM 和随机森林基线进行比较。树集合模型在三个数据集上的准确率不低于FM，同时完成全测试批次所需时间≤0.40秒且≤150MB RAM，无需使用任何VRAM。TabICL 在Higgs数据集上获得了0.8个百分点的提升，但需要大约40,000倍的延迟（960秒）和9GB VRAM。TabPFN 在Wine和Housing数据集上的准确率与树模型相当，但峰值VRAM达到4GB，无法处理完整的100,000行Higgs表格。这些结果量化了当前表格式FMs中的硬件与准确率之间的重大权衡，并为未来效率导向的研究提供了开放基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study benchmarks zero-shot tabular foundation models (FMs) on four datasets to reveal their hardware costs. Two open FMs, TabPFN-1.0 and TabICL-base, are compared with tuned tree ensembles on a single NVIDIA T4 GPU. The tree ensembles match FM accuracy while using minimal resources, highlighting significant hardware-versus-accuracy trade-offs in current FMs.</div>
<div class="mono" style="margin-top:8px">研究通过将零样本表结构基础模型（FMs）与传统树集合模型在四个数据集上的性能进行对比，评估了其硬件需求。基准测试衡量准确率、延迟和内存使用情况，结果显示树集合模型可以达到与FM相同的准确率，同时使用更少的VRAM和CPU RAM。然而，如TabICL这样的FM需要更高的VRAM和延迟，突显了硬件效率与模型性能之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes</div>
<div class="meta-line">Authors: Joseph Hoche, Andrei Bursuc, David Brellmann, Gilles Louppe, Pavel Izmailov, Angela Yao, Gianni Franchi</div>
<div class="meta-line">First: 2025-12-16T08:15:24+00:00 · Latest: 2025-12-16T08:15:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14177v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14177v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语义高斯过程提高LVLM的语义不确定性量化</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）经常生成看似合理但实际上不可靠的输出，因此稳健的不确定性估计至关重要。最近关于语义不确定性估计的工作依赖于外部模型对多个采样响应进行聚类并测量它们的语义一致性。然而，这些聚类方法往往很脆弱，对细微的措辞变化非常敏感，并且可能会错误地将语义相似的答案分组或分开，导致不可靠的不确定性估计。我们提出了语义高斯过程不确定性（SGPU），这是一种贝叶斯框架，通过分析答案嵌入的空间几何结构来量化语义不确定性，从而避免了脆弱的聚类。SGPU 将生成的答案映射到密集的语义空间，计算它们嵌入的格拉姆矩阵，并通过特征谱总结它们的语义配置。然后将这种谱表示输入到高斯过程分类器中，该分类器学习将语义一致性的模式映射到预测不确定性，并且可以在黑盒和白盒设置中应用。在六个LLM和LVLM的八个数据集上，包括VQA、图像分类和文本问答，SGPU始终实现了最先进的校准（ECE）和区分（AUROC、AUARC）性能。我们进一步表明，SGPU可以在模型和模态之间进行迁移，表明其谱表示捕捉到了语义不确定性的普遍模式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the reliability of semantic uncertainty quantification in Large Vision-Language Models (LVLMs) by proposing Semantic Gaussian Process Uncertainty (SGPU), which avoids the fragility of clustering methods. SGPU analyzes the geometric structure of answer embeddings, computes the Gram matrix, and uses the eigenspectrum to represent semantic configurations, which are then used by a Gaussian Process Classifier to estimate uncertainty. Experiments on six LLMs and LVLMs across eight datasets show that SGPU achieves state-of-the-art performance in calibration and discriminative measures, and its spectral representation is transferable across different models and modalities.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出基于语义高斯过程不确定性（SGPU）的贝叶斯框架来提高大型视觉-语言模型（LVLM）的语义不确定性估计的可靠性，该框架通过分析答案嵌入的几何结构来工作，避免了聚类方法的脆弱性。SGPU在六种LLM和LVLM的八个数据集上的一致性达到了最先进的校准和区分性能。进一步研究表明，SGPU在不同模型和模态之间具有可转移性，表明其谱表示捕捉到了语义不确定性的一般模式。</div>
</details>
</div>
<div class="card">
<div class="title">From My View to Yours: Ego-to-Exo Transfer in VLMs for Understanding Activities of Daily Living</div>
<div class="meta-line">Authors: Dominick Reilly, Manish Kumar Govind, Le Xue, Srijan Das</div>
<div class="meta-line">First: 2025-01-10T05:01:58+00:00 · Latest: 2025-12-16T07:48:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.05711v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.05711v3">PDF</a> · <a href="https://github.com/dominickrei/EgoExo4ADL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) have achieved strong performance across diverse video understanding tasks. However, their viewpoint invariant training limits their ability to understand egocentric properties (e.g., human object interactions) from exocentric video observations. This limitation is critical for many applications, such as Activities of Daily Living (ADL) monitoring, where the understanding of egocentric properties is essential, and egocentric cameras are impractical to deploy. To address this limitation, we propose Ego2ExoVLM, a VLM that learns to infer egocentric properties from exocentric videos by leveraging time-synchronized ego-exo videos during training. Ego2ExoVLM accomplishes this through the use of two components: Ego2Exo Sequence Distillation, which transfers knowledge from an egocentric teacher to an exocentric student, and Ego Adaptive Visual Tokens, designed to enhance the effectiveness of this knowledge transfer. To measure this capability, we introduce Ego-in-Exo Perception, a benchmark of 3.9K questions curated to explicitly measure the understanding of egocentric properties from exocentric videos. Ego2ExoVLM is evaluated on 10 tasks across Ego-in-Exo Perception and existing ADL benchmarks, achieving state-of-the-art results on the ADL-X benchmark suite and outperforming strong baselines on our proposed benchmark. All code, models, and data will be released at https://github.com/dominickrei/EgoExo4ADL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从我的视角到你的视角：在理解日常生活活动中的自我中心属性到环境中心属性的迁移</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在多种视频理解任务中取得了强大的性能。然而，它们的观点不变性训练限制了它们从环境中心视频观察中理解自我中心属性（例如，人类物体交互）的能力。这一限制对于许多应用至关重要，如日常生活活动（ADL）监测，其中理解自我中心属性是必不可少的，而部署自我中心摄像头是不切实际的。为了解决这一限制，我们提出了Ego2ExoVLM，这是一种通过利用同步的自我中心和环境中心视频在训练期间进行知识迁移的VLM，以推断环境中心视频中的自我中心属性。Ego2ExoVLM 通过两个组件实现这一点：Ego2Exo 序列蒸馏，将自我中心教师的知识转移到环境中心学生，以及Ego自适应视觉标记，旨在增强这种知识迁移的有效性。为了衡量这种能力，我们引入了Ego-in-Exo知觉基准，这是一个包含3900个问题的基准，专门用于明确衡量从环境中心视频理解自我中心属性的能力。Ego2ExoVLM 在Ego-in-Exo知觉基准和现有ADL基准上的10个任务中进行了评估，其在ADL-X基准套件上达到了最先进的结果，并在我们提出的基准上优于强基线。所有代码、模型和数据将在https://github.com/dominickrei/EgoExo4ADL上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Vision Language Models (VLMs) in understanding egocentric properties from exocentric views, crucial for Activities of Daily Living (ADL) monitoring. Ego2ExoVLM is proposed, which uses Ego2Exo Sequence Distillation to transfer knowledge from egocentric to exocentric videos and Ego Adaptive Visual Tokens to improve this transfer. Evaluated on 10 tasks including ADL benchmarks and a new Ego-in-Exo Perception benchmark, Ego2ExoVLM achieves state-of-the-art results on ADL-X and outperforms strong baselines on the new benchmark.</div>
<div class="mono" style="margin-top:8px">研究解决了视觉语言模型（VLMs）在从外部视角理解主观属性方面的局限性，这对于日常生活活动（ADL）监测至关重要。提出了Ego2ExoVLM，通过Ego2Exo序列蒸馏将知识从主观视角转移到外部视角，并使用Ego自适应视觉标记来提高这种知识转移的效果。在包括ADL基准和新提出的Ego-in-Exo感知基准的10个任务上进行评估，Ego2ExoVLM在ADL-X基准上取得了最先进的结果，并在新基准上优于强大的基线。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs</div>
<div class="meta-line">Authors: Anran Qi, Changjian Li, Adrien Bousseau, Niloy J. Mitra</div>
<div class="meta-line">First: 2025-12-15T14:45:05+00:00 · Latest: 2025-12-16T07:08:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13392v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.13392v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://anranqi.github.io/beyond-visible.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address image-to-video generation with explicit user control over the final frame&#x27;s disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyond-visible.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越可见：基于代理动态图的消隐感知编辑</div>
<div class="mono" style="margin-top:8px">我们通过显式用户控制最终帧的消隐区域来解决图像到视频的生成问题。当前的图像到视频流水线能够生成可信的运动，但在生成可预测、有组织的运动并确保用户指定内容在新揭示区域的同时遇到困难。我们的核心思想是将运动规范与外观合成分离：我们引入了一个轻量级、用户可编辑的代理动态图（PDG），它以确定性但近似的方式驱动部分运动，而冻结的扩散先验用于合成遵循该运动的可信外观。在我们的无需训练的流水线中，用户对PDG进行粗略标注和重新定位，从中我们计算密集的运动流以利用扩散作为运动导向的着色器。然后，用户可以在图像的消隐区域编辑外观，并利用PDG编码的可见性信息在这些区域执行潜在空间合成，以协调运动与用户意图。此设计实现了可控的组织和对消隐的用户控制，无需微调。我们展示了与最先进的替代方案相比，在将图像转化为包含有组织对象、家具、车辆和变形体的短视频方面的明显优势。我们的方法结合了生成控制（松散的姿态和结构）与最终帧中消隐区域的外观规范的可预测控制，解锁了一种新的图像到视频工作流程。代码将在接受后发布。项目页面：https://anranqi.github.io/beyond-visible.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating plausible motion in image-to-video conversion while allowing user control over disoccluded regions. It introduces a Proxy Dynamic Graph (PDG) to drive part motion deterministically, while a frozen diffusion model synthesizes plausible appearance. Users can edit the appearance in the newly revealed areas, and the method leverages visibility information to reconcile motion with user intent. This approach provides controllable articulation and user control over disocclusions without requiring fine-tuning, outperforming existing methods for converting images into videos of articulated objects, furniture, vehicles, and deformables.</div>
<div class="mono" style="margin-top:8px">研究解决了图像到视频生成问题，用户可以控制未遮挡区域的内容。引入了代理动态图（PDG）来驱动部分运动，并使用冻结的扩散模型进行外观合成。方法允许用户编辑未遮挡区域的外观，并通过潜空间合成来实现运动与用户意图的统一，无需微调。实验表明，该方法在将图像转换为具有动画效果的对象、家具、车辆和可变形物体的视频方面具有明显优势。</div>
</details>
</div>
<div class="card">
<div class="title">UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis</div>
<div class="meta-line">Authors: Amirmohammad Pasdar, Toby Murray, Van-Thuan Pham</div>
<div class="meta-line">First: 2025-12-16T06:26:29+00:00 · Latest: 2025-12-16T06:26:29+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14130v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UIXPOSE：通过意图-行为差异分析检测移动恶意软件</div>
<div class="mono" style="margin-top:8px">我们介绍了UIXPOSE，一种源代码无关的框架，适用于编译后的和开源应用。该框架使用意图行为对齐（IBA）进行移动恶意软件分析，将UI推断出的意图与运行时语义对齐。以往的工作要么静态推断意图，例如基于权限，或者在组件级别进行监控，要么监控粗粒度的动态信号（端点、部分资源使用），这些信号会遗漏内容和上下文。UIXPOSE 使用视觉语言模型和知识结构从每个屏幕推断意图向量，并将解码的网络负载、堆/内存信号和资源利用率轨迹组合成行为向量。它们在运行时的对齐可以检测异常行为并突出显示行为丰富的路径。在三个实际案例研究中，UIXPOSE 揭示了逃避元数据基线的隐蔽数据泄露和隐藏的后台活动，证明了IBA如何提高动态检测效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UIXPOSE is a source-code-agnostic framework that analyzes both compiled and open-source apps using Intention Behaviour Alignment (IBA) to detect mobile malware. It infers intent from each screen using vision-language models and combines this with runtime behavior signals to align intent and behavior. This approach detects misbehavior and highlights rich behavioral paths, outperforming metadata-only baselines in three case studies by revealing covert exfiltration and hidden background activities.</div>
<div class="mono" style="margin-top:8px">UIXPOSE 是一个无需源代码的框架，可以分析编译后的和开源的应用程序，并使用意图行为对齐（IBA）来检测移动恶意软件。它使用视觉语言模型从每个屏幕推断意图，并结合运行时的行为信号来对齐意图和行为。这种方法在三个案例研究中通过揭示隐蔽的数据泄露和隐藏的后台活动，优于仅基于元数据的基线，展示了如何通过 IBA 提高动态检测能力。</div>
</details>
</div>
<div class="card">
<div class="title">Consistent Instance Field for Dynamic Scene Understanding</div>
<div class="meta-line">Authors: Junyi Wu, Van Nguyen Nguyen, Benjamin Planche, Jiachen Tao, Changchang Sun, Zhongpai Gao, Zhenghao Zhao, Anwesa Choudhuri, Gengyu Zhang, Meng Zheng, Feiran Wang, Terrence Chen, Yan Yan, Ziyan Wu</div>
<div class="meta-line">First: 2025-12-16T06:12:11+00:00 · Latest: 2025-12-16T06:12:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14126v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一致实例场：动态场景理解的连续概率时空表示</div>
<div class="mono" style="margin-top:8px">我们提出了一个连续且概率性的时空表示方法——一致实例场，用于动态场景理解。与依赖离散跟踪或视点相关特征的先前方法不同，我们的方法通过将每个空间-时间点建模为占用概率和条件实例分布来分离可见性与持久对象身份。为了实现这一点，我们引入了一种基于可变形3D高斯分布的新颖实例嵌入表示，该表示联合编码辐射和语义信息，并通过可微放像素化直接从输入RGB图像和实例掩码中学习。此外，我们引入了新的机制来校准每个高斯分布的身份，并重新采样高斯分布以朝向语义活跃区域，从而确保空间和时间上的一致实例表示。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法在新颖视角全景分割和开放词汇4D查询任务上显著优于现有最佳方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research introduces Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike previous methods that depend on discrete tracking or view-dependent features, this approach models each space-time point with an occupancy probability and a conditional instance distribution, disentangling visibility from persistent object identity. The method uses a novel instance-embedded representation based on deformable 3D Gaussians to jointly encode radiance and semantic information, which are learned from input RGB images and instance masks through differentiable rasterization. Experiments show that this method outperforms existing techniques in novel-view panoptic segmentation and open-vocabulary 4D querying tasks on the HyperNeRF and Neu3D datasets.</div>
<div class="mono" style="margin-top:8px">研究引入了连续且概率性的空间-时间表示方法——一致实例场，用于动态场景理解。不同于依赖离散跟踪或视点依赖特征的先前方法，该方法通过每个空间-时间点的占用概率和条件实例分布来区分可见性和持久对象身份。该方法使用基于可变形3D高斯的新型实例嵌入表示，联合编码辐射和语义信息，并通过可微放像素化直接从输入的RGB图像和实例掩码中学习。实验表明，该方法在HyperNeRF和Neu3D数据集上的新颖视角全景分割和开放词汇4D查询任务中优于现有技术。</div>
</details>
</div>
<div class="card">
<div class="title">MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</div>
<div class="meta-line">Authors: Minghui Hou, Wei-Hsing Huang, Shaofeng Liang, Daizong Liu, Tai-Hao Wen, Gang Wang, Runwei Guan, Weiping Ding</div>
<div class="meta-line">First: 2025-12-15T10:37:59+00:00 · Latest: 2025-12-16T05:50:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13177v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.13177v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMDrive：超越视觉的多表示融合交互场景理解</div>
<div class="mono" style="margin-top:8px">视觉语言模型通过多源信息融合，使复杂交通场景的理解和推理成为可能，成为自动驾驶的核心技术。然而，现有的视觉语言模型受限于二维图像理解范式，限制了其对三维空间信息的感知能力和深层次语义融合的能力，导致在复杂自动驾驶环境中表现不佳。本研究提出MMDrive，这是一种多模态视觉语言模型框架，将传统的图像理解扩展到通用的三维场景理解框架。MMDrive结合了三种互补模态，包括占用地图、LiDAR点云和文本场景描述。为此，它引入了两种新的组件，用于自适应跨模态融合和关键信息提取。具体来说，文本导向的多模态调制器根据问题中的语义线索动态加权每个模态的贡献，引导上下文感知特征整合。跨模态抽象器使用可学习的抽象标记生成紧凑的跨模态总结，突出关键区域和重要语义。在DriveLM和NuScenes-QA基准上的全面评估表明，MMDrive在自动驾驶中的性能显著优于现有视觉语言模型，在DriveLM上的BLEU-4得分为54.56，METEOR得分为41.78，在NuScenes-QA上的准确率为62.7%。MMDrive有效地突破了传统的仅图像理解障碍，能够在复杂驾驶环境中实现稳健的多模态推理，并为可解释的自动驾驶场景理解提供新的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MMDrive is a multimodal vision-language model framework that extends traditional image understanding to a 3D scene understanding framework by incorporating occupancy maps, LiDAR point clouds, and textual scene descriptions. It introduces two novel components for adaptive cross-modal fusion and key information extraction, achieving significant performance gains over existing models in autonomous driving benchmarks. On DriveLM, MMDrive scores 54.56 in BLEU-4 and 41.78 in METEOR, and on NuScenes-QA, it achieves 62.7% accuracy.</div>
<div class="mono" style="margin-top:8px">MMDrive 是一种多模态视觉-语言模型框架，将传统的二维图像理解扩展到三维场景理解，结合了占用图、LiDAR 点云和文本场景描述。它引入了两种新型组件：面向文本的多模态调制器进行自适应跨模态融合和跨模态抽象器生成紧凑的跨模态摘要。MMDrive 在 DriveLM 和 NuScenes-QA 基准测试中取得了显著的性能提升，BLEU-4 得分为 54.56，METEOR 得分为 41.78，NuScenes-QA 的准确率为 62.7%。这项研究打破了传统的图像单一理解障碍，增强了在复杂驾驶环境中的稳健多模态推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes</div>
<div class="meta-line">Authors: Robinson Umeike, Neil Getty, Yin Xiangyu, Yi Jiang</div>
<div class="meta-line">First: 2025-11-04T11:43:05+00:00 · Latest: 2025-12-16T05:43:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02503v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.02503v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automation of workflows in advanced microscopy is a key goal where foundation models like Language Models (LLMs) and Vision-Language Models (VLMs) show great potential. However, adapting these general-purpose models for specialized scientific tasks is critical, and the optimal domain adaptation strategy is often unclear. To address this, we introduce PtychoBench, a new multi-modal, multi-task benchmark for ptychographic analysis. Using this benchmark, we systematically compare two specialization strategies: Supervised Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies on a visual artifact detection task with VLMs and a textual parameter recommendation task with LLMs in a data-scarce regime. Our findings reveal that the optimal specialization pathway is task-dependent. For the visual task, SFT and ICL are highly complementary, with a fine-tuned model guided by context-aware examples achieving the highest mean performance (Micro-F1 of 0.728). Conversely, for the textual task, ICL on a large base model is the superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a powerful &quot;super-expert&quot; SFT model (0-shot Micro-F1 of 0.839). We also confirm the superiority of context-aware prompting and identify a consistent contextual interference phenomenon in fine-tuned models. These results, benchmarked against strong baselines including GPT-4o and a DINOv3-based classifier, offer key observations for AI in science: the optimal specialization path in our benchmark is dependent on the task modality, offering a clear framework for developing more effective science-based agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将通用基础模型适应于低数据量条件下的X射线透视成像</div>
<div class="mono" style="margin-top:8px">在先进显微镜工作流自动化方面，像语言模型（LLMs）和视觉-语言模型（VLMs）这样的基础模型显示出巨大的潜力。然而，将这些通用模型适应于特定的科学任务至关重要，而最优的领域适应策略往往不明确。为解决这一问题，我们引入了PtychoBench，这是一种新的多模态、多任务基准，用于透视分析。利用这一基准，我们系统地比较了两种专业化策略：监督微调（SFT）和上下文学习（ICL）。我们在数据稀缺的环境中，使用VLMs进行视觉伪影检测任务，使用LLMs进行文本参数推荐任务，评估这些策略。我们的研究结果表明，最优的专业化路径取决于任务。对于视觉任务，SFT和ICL高度互补，带有上下文感知示例指导的微调模型达到了最高的平均性能（Micro-F1为0.728）。相反，对于文本任务，大型基础模型上的ICL是更优策略，达到了峰值Micro-F1为0.847，并且优于强大的“超级专家”SFT模型（零样本Micro-F1为0.839）。我们还确认了上下文感知提示的优越性，并在微调模型中识别出了一致的上下文干扰现象。这些结果，与包括GPT-4o和基于DINOv3的分类器在内的强基线进行基准测试，为科学中的AI提供了关键观察：在我们的基准中，最优的专业化路径取决于任务模态，为开发更有效的基于科学的代理系统提供了清晰的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to explore effective strategies for adapting general-purpose models like VLMs and LLMs for specialized scientific tasks, specifically ptychographic analysis. Two strategies, Supervised Fine-Tuning (SFT) and In-Context Learning (ICL), were evaluated on visual and textual tasks in a data-scarce regime. For the visual task, SFT and ICL were complementary, with a context-aware fine-tuned model achieving the highest performance. For the textual task, ICL on a large base model outperformed SFT, reaching a peak performance. The research highlights the task-dependent nature of the optimal specialization pathway and provides insights for developing more effective AI systems in science.</div>
<div class="mono" style="margin-top:8px">论文介绍了PtychoBench，这是一个用于ptychographic分析的基准，用于评估Supervised Fine-Tuning (SFT)和In-Context Learning (ICL)在将通用模型适应特定科学任务方面的有效性。在数据稀缺的情况下，研究发现SFT和ICL对于视觉特征检测任务是互补的，通过上下文感知示例进行微调的模型达到了最高性能。对于文本参数推荐任务，ICL在大型基础模型上的表现优于SFT，达到了0.847的峰值性能。研究强调了最优专业化路径依赖于任务模态，并突出了上下文感知提示的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries</div>
<div class="meta-line">Authors: Emanuele Mezzi, Gertjan Burghouts, Maarten Kruithof</div>
<div class="meta-line">First: 2025-12-16T05:33:44+00:00 · Latest: 2025-12-16T05:33:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14102v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14102v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM&#x27;s effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE&#x27;s potential for real-world RS applications through a use case on post-flood satellite image retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经符号推理在基础模型中的应用：针对遥感文本到图像检索的复杂查询</div>
<div class="mono" style="margin-top:8px">遥感（RS）中的文本到图像检索随着大型视觉语言模型（LVLMs）的发展而迅速进步，这些模型专门针对航空和卫星图像，最终形成了遥感大型视觉语言模型（RS-LVLMS）。然而，有限的可解释性和对复杂空间关系的处理不足仍然是实际应用中的关键挑战。为了解决这些问题，我们引入了RUNE（基于神经符号实体的推理），这是一种将大型语言模型（LLMs）与神经符号人工智能结合的方法，通过推理检测到的实体与从文本查询中推导出的一阶逻辑（FOL）表达式的兼容性来检索图像。与依赖于隐式联合嵌入的RS-LVLMs不同，RUNE执行显式推理，从而提高性能和可解释性。为了提高可扩展性，我们提出了一种逻辑分解策略，该策略在检测到的实体的条件子集上操作，确保执行时间比神经方法更短。我们仅利用基础模型生成一阶逻辑表达式，将推理任务委托给神经符号推理模块。为了评估，我们重新利用了DOTA数据集，该数据集最初用于对象检测，并通过增加比现有基准更复杂的查询来增强它。我们展示了大型语言模型在文本到逻辑转换中的有效性，并将RUNE与最先进的RS-LVLMs进行比较，显示出更好的性能。我们引入了两个指标，检索对查询复杂性的鲁棒性（RRQC）和检索对图像不确定性的鲁棒性（RRIU），以评估性能相对于查询复杂性和图像不确定性。RUNE在复杂的RS检索任务中优于联合嵌入模型，提供了性能、鲁棒性和可解释性的提升。我们通过一个关于洪水后卫星图像检索的应用案例展示了RUNE在实际遥感应用中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of explainability and handling complex spatial relations in text-to-image retrieval for remote sensing (RS) using large vision-language models (LVLMs). It introduces RUNE, which combines Large Language Models (LLMs) with neurosymbolic AI to perform explicit reasoning over First-Order Logic (FOL) expressions derived from text queries. RUNE outperforms joint-embedding models in complex RS retrieval tasks, showing superior performance, robustness, and explainability. The authors introduce two new metrics, RRQC and RRIU, to evaluate performance under varying query complexity and image uncertainty, demonstrating RUNE&#x27;s effectiveness in real-world RS applications.</div>
<div class="mono" style="margin-top:8px">本文旨在解决遥感（RS）中的文本到图像检索中大型视觉语言模型（LVLMs）在解释性和处理复杂空间关系方面的挑战。文中提出了一种名为RUNE的方法，将大型语言模型（LLMs）与神经符号AI结合，通过推理First-Order Logic（FOL）表达式来处理文本查询。RUNE在复杂RS检索任务中优于联合嵌入模型，表现出更好的性能、鲁棒性和解释性。作者引入了两个新的评估指标RRQC和RRIU，以评估在不同查询复杂性和图像不确定性下的性能，证明了RUNE在实际RS应用中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings</div>
<div class="meta-line">Authors: Shengkai Xu, Hsiang Lun Kao, Tianxiang Xu, Honghui Zhang, Junqiao Wang, Runmeng Ding, Guanyu Liu, Tianyu Shi, Zhenyu Yu, Guofeng Pan, Ziqian Bi, Yuqi Ouyang</div>
<div class="meta-line">First: 2025-12-13T23:33:05+00:00 · Latest: 2025-12-16T04:40:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12492v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12492v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>适应性检测-验证框架在开放世界环境下的零样本结肠息肉检测</div>
<div class="mono" style="margin-top:8px">在干净数据集上训练的结肠息肉检测器在实际临床内镜检查中往往表现不佳，因为光照变化、运动模糊和遮挡会降低图像质量。现有方法难以弥合受控实验室条件与临床实践中普遍存在的不良成像条件之间的差距。在本工作中，我们提出了一种新颖的两阶段检测-验证框架——AdaptiveDetector，该框架包括一个YOLOv11检测器和一个视觉语言模型（VLM）验证器。检测器在VLM的指导下自适应地调整每帧的置信度阈值，而验证器则使用组相对策略优化（GRPO）进行微调，采用一种不对称的成本敏感奖励函数，专门设计用于减少漏检——这是临床中的关键需求。为了在具有挑战性的条件下进行现实评估，我们通过系统地将临床实践中常见的不良条件应用于干净数据集，构建了一个全面的合成测试床，为零样本评估提供了一个严格的基准。在合成降级的CVC-ClinicDB和Kvasir-SEG图像上的零样本评估表明，与YOLO单独使用相比，我们的方法在召回率上提高了14到22个百分点，而精度仅比基线低0.7到1.7个百分点。这种自适应阈值调整和成本敏感强化学习的结合实现了临床对齐的开放世界结肠息肉检测，显著减少了假阴性，从而降低了漏检的癌前息肉的风险，改善了患者预后。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the underperformance of polyp detectors in real-world endoscopy due to adverse imaging conditions. The proposed AdaptiveDetector framework uses a YOLOv11 detector and a vision-language model verifier to adaptively adjust confidence thresholds and fine-tune the verifier with Group Relative Policy Optimization. The approach significantly improves recall by 14 to 22 percentage points over YOLO alone, while maintaining precision close to the baseline. This method achieves clinically aligned, open-world polyp detection with fewer false negatives, enhancing patient outcomes.</div>
<div class="mono" style="margin-top:8px">本文提出了一种两阶段检测-验证框架AdaptiveDetector，以解决现实世界内窥镜检查中polyp检测器的性能差距问题。检测器在视觉-语言模型的指导下自适应调整置信阈值，而验证器则使用成本敏感的奖励函数通过组相对策略优化进行微调。实验结果表明，AdaptiveDetector在合成降级图像上的召回率比YOLO提高了14到22个百分点，同时保持精度接近基线，从而实现了临床对齐的开放世界polyp检测，减少了假阴性，提高了患者预后。</div>
</details>
</div>
<div class="card">
<div class="title">Learning neuroimaging models from health system-scale data</div>
<div class="meta-line">Authors: Yiwei Lyu, Samir Harake, Asadur Chowdury, Soumyanil Banerjee, Rachel Gologorsky, Shixuan Liu, Anna-Katharina Meissner, Akshay Rao, Chenhui Zhao, Akhil Kondepudi, Cheng Jiang, Xinhai Hou, Rushikesh S. Joshi, Volker Neuschmelting, Ashok Srinivasan, Dawn Kleindorfer, Brian Athey, Vikas Gulani, Aditya Pandey, Honglak Lee, Todd Hollon</div>
<div class="meta-line">First: 2025-09-23T04:49:59+00:00 · Latest: 2025-12-16T04:26:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18638v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.18638v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neuroimaging is a ubiquitous tool for evaluating patients with neurological diseases. The global demand for magnetic resonance imaging (MRI) studies has risen steadily, placing significant strain on health systems, prolonging turnaround times, and intensifying physician burnout. These challenges disproportionately impact patients in low-resource and rural settings. Here, we utilized a large academic health system as a data engine to develop Prima, the first vision language model (VLM) serving as an AI foundation for neuroimaging that supports real-world, clinical MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a hierarchical vision architecture that provides general and transferable MRI features. Prima was tested in a 1-year health system-wide study that included 30K MRI studies. Across 52 radiologic diagnoses from the major neurologic disorders, including neoplastic, inflammatory, infectious, and developmental lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0, outperforming other state-of-the-art general and medical AI models. Prima offers explainable differential diagnoses, worklist priority for radiologists, and clinical referral recommendations across diverse patient demographics and MRI systems. Prima demonstrates algorithmic fairness across sensitive groups and can help mitigate health system biases, such as prolonged turnaround times for low-resource populations. These findings highlight the transformative potential of health system-scale VLMs and Prima&#x27;s role in advancing AI-driven healthcare.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从健康系统规模数据中学习神经影像学模型</div>
<div class="mono" style="margin-top:8px">神经影像学是评估神经疾病患者的一种普遍工具。全球磁共振成像（MRI）研究的需求持续上升，给健康系统带来了巨大压力，延长了周转时间，并加剧了医生的职业倦怠。这些挑战在低资源和农村地区患者中尤为突出。在这里，我们利用一个大型学术健康系统作为数据引擎，开发了Prima，这是第一个作为神经影像学AI基础的视觉语言模型（VLM），支持实际临床MRI研究作为输入。Prima基于超过220,000份MRI研究进行训练，使用分层视觉架构提供通用和可转移的MRI特征。Prima在为期一年的健康系统范围内研究中进行了测试，包括30,000份MRI研究。在52种主要神经疾病放射学诊断中，包括肿瘤、炎症、感染和发育性病变，Prima实现了92.0的平均诊断ROC曲线下面积，优于其他最先进的通用和医疗AI模型。Prima提供了可解释的鉴别诊断、放射科医生的工作列表优先级和跨不同患者群体和MRI系统的临床转诊建议。Prima展示了对敏感群体的算法公平性，并能帮助缓解健康系统偏见，如低资源人群的长时间周转时间。这些发现突显了健康系统规模VLMs的变革潜力以及Prima在推动AI驱动医疗保健方面的作用。</div>
</details>
</div>
<div class="card">
<div class="title">SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding</div>
<div class="meta-line">Authors: Shuang Cheng, Yuhua Jiang, Zineng Zhou, Dawei Liu, Wang Tao, Linfeng Zhang, Biqing Qi, Bowen Zhou</div>
<div class="meta-line">First: 2025-12-16T04:12:52+00:00 · Latest: 2025-12-16T04:12:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14068v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14068v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SDAR-VL：视觉语言理解中的稳定高效块级扩散</div>
<div class="mono" style="margin-top:8px">块级离散扩散在并行生成和因果依赖建模之间提供了有吸引力的平衡，使其成为视觉语言建模的有前途的骨干。然而，其实际应用受到高训练成本、缓慢收敛和不稳定性的限制，使其至今仍落后于强大的自回归（AR）基线。我们提出了**SDAR-VL**，这是块级离散扩散首次系统应用于大规模视觉语言理解（VLU），并提供了一种**高效的稳定训练综合框架**。该框架统一了三个组件：（1）**异步块级噪声调度**，以在每个批次中多样化监督；（2）**有效的掩码比例缩放**，以在随机掩码下实现无偏损失归一化；以及（3）**渐进贝塔噪声课程**，该课程增加了有效掩码覆盖率，同时保持了破坏多样性。在21个单图像、多图像和视频基准上的实验表明，SDAR-VL 在训练效率、收敛稳定性和任务性能方面均优于传统的块扩散。在这一评估套件中，SDAR-VL 在基于扩散的视觉语言模型中设立了新的标准，并在匹配设置下与强大的 AR 基线（如 LLaVA-OneVision）以及全球扩散基线 LLaDA-V 相匹配或超越，确立了块级扩散作为 VLU 实用骨干的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SDAR-VL, a novel approach to vision-language understanding using block-wise discrete diffusion, addressing the limitations of high training cost and instability. It proposes an integrated framework with asynchronous block-wise noise scheduling, effective mask ratio scaling, and a progressive beta noise curriculum to enhance training efficiency and stability. Experiments demonstrate that SDAR-VL outperforms conventional block diffusion methods and matches or surpasses strong autoregressive models on 21 benchmarks, establishing block-wise diffusion as a practical backbone for vision-language understanding tasks.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种名为SDAR-VL的新方法，利用块离散扩散进行视觉-语言理解，解决了高训练成本和不稳定性的问题。它提出了一种集成框架，包括异步块噪声调度、有效的掩码比例缩放和渐进的贝塔噪声课程，以提高训练效率和稳定性。实验表明，SDAR-VL在21个基准测试中优于传统块扩散方法，并且在匹配设置下与强大的自回归模型（如LLaVA-OneVision）相当或超越，确立了块离散扩散作为视觉-语言理解任务的实用基础框架。</div>
</details>
</div>
<div class="card">
<div class="title">VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management</div>
<div class="meta-line">Authors: Hongbo Jin, Qingyuan Wang, Wenhao Zhang, Yang Liu, Sijie Cheng</div>
<div class="meta-line">First: 2025-12-04T07:42:13+00:00 · Latest: 2025-12-16T03:47:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04540v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04540v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoMem：通过自适应内存管理增强超长视频理解</div>
<div class="mono" style="margin-top:8px">超长视频的理解仍然是一个开放的挑战，现有的视觉语言模型（VLMs）在处理此类内容时由于上下文长度有限和长期记忆保留效率低下而表现不佳。为了解决这个问题，最近的研究尝试构建外部知识库和相应的检索增强生成（RAG）系统，但这些方法会带来巨大的存储和计算开销。在本文中，我们提出了VideoMem，这是一种新颖的框架，通过自适应内存管理将超长视频理解视为一个顺序生成任务。具体而言，VideoMem动态更新全局内存缓冲区，该缓冲区会自适应地保留关键信息并丢弃视频时间线上的冗余内容。为了高效地训练VLMs以处理长期任务，VideoMem集成了渐进分组相对策略优化（PRPO）算法，该算法配备了两个核心模块：渐进状态传播（PSP）自适应地保留有效的当前状态，将其传播到下一个展开步骤，并逐步缩小模型的探索空间。时间级联奖励（TCR）进一步缓解了奖励稀疏性，提高了样本利用效率并加速了收敛。广泛的实验表明，VideoMem在各种超长视频理解任务基准测试中显著优于现有开源模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of ultra-long video understanding by proposing VideoMem, a framework that uses adaptive memory management to enhance the context length and memory retention of vision language models (VLMs). VideoMem dynamically updates a global memory buffer to retain critical information and discard redundant content, integrating PRPO for efficient training. The PSP module retains valid states and narrows the exploration space, while TCR alleviates reward sparsity. Experiments show that VideoMem outperforms existing models on various benchmarks for ultra-long video understanding tasks.</div>
<div class="mono" style="margin-top:8px">论文提出了VideoMem框架，通过使用自适应内存管理来解决超长视频理解的挑战。该框架动态更新全局内存缓冲区，保留关键信息并丢弃冗余内容，并结合PRPO算法中的PSP和TCR模块高效训练VLMs进行长期任务。实验结果表明，VideoMem在各种超长视频理解任务基准上优于现有模型。</div>
</details>
</div>
<div class="card">
<div class="title">OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</div>
<div class="meta-line">Authors: Zhenguo Zhang, Haohan Zhen, Yishen Wang, Le Xu, Tianchen Deng, Xuefeng Chen, Qu Chen, Bo Zhang, Wuxiong Huang</div>
<div class="meta-line">First: 2025-12-16T03:19:28+00:00 · Latest: 2025-12-16T03:19:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14044v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and &quot;zoom in&quot; on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model&#x27;s significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniDrive-R1：强化驱动的交织多模态链式思考框架以实现可信赖的视觉语言自动驾驶</div>
<div class="mono" style="margin-top:8px">在自动驾驶（AD）等安全关键领域部署视觉语言模型（VLMs）受到可靠性故障的严重阻碍，尤其是对象幻觉。这种故障源于它们依赖于基于文本的链式思考（CoT）推理。虽然现有的多模态CoT方法试图缓解这一问题，但它们存在两个根本缺陷：（1）感知和推理阶段的分离，这妨碍了端到端的联合优化，（2）依赖昂贵的密集定位标签。因此，我们提出了OmniDrive-R1，这是一种为自动驾驶设计的端到端VLM框架，通过交织多模态链式思考（iMCoT）机制统一了感知和推理。我们的核心创新是一种强化驱动的视觉定位能力，使模型能够自主地引导其注意力并“聚焦”在关键区域进行精细分析。这种能力得益于我们纯两阶段强化学习训练管道和Clip-GRPO算法。关键的是，Clip-GRPO引入了一种无需标注的过程导向定位奖励。这种奖励不仅消除了对密集标签的需求，还通过强制实时跨模态一致性来避免外部工具调用的不稳定性。在DriveLMM-o1上的大量实验表明，我们的模型取得了显著改进。与基线Qwen2.5VL-7B相比，OmniDrive-R1的整体推理得分从51.77%提高到80.35%，最终答案准确性从37.81%提高到73.62%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">OmniDrive-R1 is an end-to-end VLM framework for autonomous driving that integrates perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. It introduces a reinforcement-driven visual grounding capability, allowing the model to focus on critical regions for fine-grained analysis. The model uses a two-stage reinforcement learning training pipeline and Clip-GRPO algorithm, which provides annotation-free, process-based grounding rewards, enhancing real-time cross-modal consistency. Experiments show that OmniDrive-R1 significantly improves reasoning and answer accuracy compared to the baseline Qwen2.5VL-7B, with a reasoning score increase from 51.77% to 80.35% and an answer accuracy increase from 37.81% to 73.62%.</div>
<div class="mono" style="margin-top:8px">OmniDrive-R1 是一个端到端的 VLM 框架，用于自动驾驶，通过交织的多模态链式思维（iMCoT）机制将感知和推理结合起来。它引入了一种基于强化学习的视觉定位能力，使模型能够聚焦于关键区域进行精细分析。实验表明，与基线 Qwen2.5VL-7B 相比，OmniDrive-R1 在整体推理得分上从 51.77% 提高到 80.35%，最终答案准确率从 37.81% 提高到 73.62%。</div>
</details>
</div>
<div class="card">
<div class="title">CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images</div>
<div class="meta-line">Authors: Bo Liu, Qiao Qin, Qinghui He</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-15T12:48:27+00:00 · Latest: 2025-12-16T02:47:19+00:00</div>
<div class="meta-line">Comments: 9 pages,Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13285v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.13285v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CausalCLIP：因果驱动的特征解缠与筛选以实现生成图像检测的泛化能力</div>
<div class="mono" style="margin-top:8px">生成模型的快速发展增加了对能够跨多样且不断演变的生成技术进行泛化的生成图像检测器的需求。然而，现有的方法，包括利用预训练的视觉-语言模型的方法，往往会产生高度纠缠的表示，将与任务相关的法医线索（因果特征）与虚假或无关的模式（非因果特征）混合在一起，从而限制了泛化能力。为了解决这一问题，我们提出了CausalCLIP框架，该框架明确地解缠因果特征和非因果特征，并通过因果推理原则进行目标筛选，仅保留最具转移性和区分性的法医线索。通过使用结构因果模型建模生成过程，并通过Gumbel-Softmax基特征掩码和Hilbert-Schmidt独立性判别准则（HSIC）约束来强制统计独立性，CausalCLIP隔离了对分布偏移具有鲁棒性的稳定因果特征。当在不同系列的未见过的生成模型上进行测试时，CausalCLIP展示了强大的泛化能力，相对于最先进的方法，在准确性和平均精度上分别提高了6.83%和4.06%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CausalCLIP is designed to improve the generalization of generated image detectors by disentangling causal and non-causal features and applying causal inference-based filtering. It uses a structural causal model and Gumbel-Softmax-based feature masking with HSIC constraints to isolate stable causal features. Experiments show that CausalCLIP outperforms existing methods, achieving 6.83% higher accuracy and 4.06% higher average precision on unseen generative models.</div>
<div class="mono" style="margin-top:8px">论文提出了CausalCLIP框架，该框架通过因果推理原则和结构因果模型来分离因果特征和非因果特征，以提高检测器的一般化能力。该方法过滤掉非因果特征，保留最可迁移和区分的因果特征，实现在未见过的生成模型上的显著准确性和平均精度提升，展示了强大的一般化能力。</div>
</details>
</div>
<div class="card">
<div class="title">MobileWorldBench: Towards Semantic World Modeling For Mobile Agents</div>
<div class="meta-line">Authors: Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Aditya Grover</div>
<div class="meta-line">First: 2025-12-16T02:16:42+00:00 · Latest: 2025-12-16T02:16:42+00:00</div>
<div class="meta-line">Comments: 21 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14014v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14014v1">PDF</a> · <a href="https://github.com/jacklishufan/MobileWorld">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MobileWorldBench：面向移动代理的语义世界建模</div>
<div class="mono" style="margin-top:8px">世界模型在提高具身代理任务性能方面显示出巨大的实用性。尽管先前的工作主要集中在像素空间的世界模型上，但在GUI设置中，这些方法在预测未来状态中的复杂视觉元素时往往面临实际限制。在本工作中，我们探索了GUI代理的一种替代世界建模形式，其中状态转换用自然语言描述，而不是预测原始像素。首先，我们介绍了MobileWorldBench，这是一个基准测试，评估视觉语言模型（VLMs）作为移动GUI代理世界模型的能力。其次，我们发布了MobileWorld，这是一个包含140万样本的大规模数据集，显著提高了VLMs的世界建模能力。最后，我们提出了一种新的框架，将VLM世界模型整合到移动代理的规划框架中，证明了语义世界模型可以直接通过提高任务成功率来造福移动代理。代码和数据集可在https://github.com/jacklishufan/MobileWorld获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitations of pixel-space world models in GUI settings by proposing a new approach where state transitions are described in natural language. The authors introduce MobileWorldBench, a benchmark to evaluate vision-language models as world models for mobile GUI agents, and release MobileWorld, a large dataset of 1.4M samples. They also propose a framework integrating these models into mobile agent planning, showing improved task success rates. The code and dataset are available at https://github.com/jacklishufan/MobileWorld.</div>
<div class="mono" style="margin-top:8px">该研究针对GUI环境中像素空间世界模型的局限性，提出了一种新的方法，即通过自然语言描述状态转换。作者引入了MobileWorldBench，用于评估视觉语言模型作为移动GUI代理的世界模型能力，并发布了包含140万样本的大规模数据集MobileWorld。他们还提出了一种框架，将VLM世界模型集成到移动代理的规划框架中，展示了任务成功率的提升。代码和数据集可在https://github.com/jacklishufan/MobileWorld获取。</div>
</details>
</div>
<div class="card">
<div class="title">Guideline-Consistent Segmentation via Multi-Agent Refinement</div>
<div class="meta-line">Authors: Vanshika Vats, Ashwani Rathee, James Davis</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-09-04T22:32:57+00:00 · Latest: 2025-12-16T01:40:54+00:00</div>
<div class="meta-line">Comments: To be published in The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04687v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04687v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic segmentation in real-world applications often requires not only accurate masks but also strict adherence to textual labeling guidelines. These guidelines are typically complex and long, and both human and automated labeling often fail to follow them faithfully. Traditional approaches depend on expensive task-specific retraining that must be repeated as the guidelines evolve. Although recent open-vocabulary segmentation methods excel with simple prompts, they often fail when confronted with sets of paragraph-length guidelines that specify intricate segmentation rules. To address this, we introduce a multi-agent, training-free framework that coordinates general-purpose vision-language models within an iterative Worker-Supervisor refinement architecture. The Worker performs the segmentation, the Supervisor critiques it against the retrieved guidelines, and a lightweight reinforcement learning stop policy decides when to terminate the loop, ensuring guideline-consistent masks while balancing resource use. Evaluated on the Waymo and ReasonSeg datasets, our method notably outperforms state-of-the-art baselines, demonstrating strong generalization and instruction adherence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于指南一致的多智能体细化分割</div>
<div class="mono" style="margin-top:8px">在实际应用中的语义分割不仅需要准确的掩码，还需要严格遵守文本标注指南。这些指南通常复杂且冗长，无论是人工还是自动标注往往难以忠实遵守。传统方法依赖于昂贵的任务特定重训练，且随着指南的演变需要重复进行。尽管最近的开放式词汇分割方法在简单提示下表现出色，但在面对包含段落长度指南的复杂分割规则时往往失效。为解决这一问题，我们提出了一种无需训练的多智能体框架，该框架在迭代的工人-监督者细化架构中协调通用视觉-语言模型。工人执行分割，监督者根据检索到的指南对其进行评价，轻量级的强化学习停止策略决定何时终止循环，以确保指南一致的掩码并平衡资源使用。在Waymo和ReasonSeg数据集上评估，我们的方法显著优于最先进的基线，展示了强大的泛化能力和指令遵守能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of semantic segmentation in real-world applications, where strict adherence to complex textual labeling guidelines is crucial. The method introduces a multi-agent, training-free framework that uses an iterative Worker-Supervisor refinement architecture to ensure guideline-consistent masks. The Worker performs segmentation, the Supervisor critiques it against retrieved guidelines, and a reinforcement learning policy decides when to terminate the loop. The method outperforms state-of-the-art baselines on the Waymo and ReasonSeg datasets, showing strong generalization and instruction adherence.</div>
<div class="mono" style="margin-top:8px">研究针对现实世界应用中语义分割需要严格遵守复杂文本标注指南的挑战。提出了一种多代理、无需训练的框架，采用迭代的工人-监督者架构确保符合指南的分割结果。工人执行分割任务，监督者根据检索到的指南进行评价，轻量级的强化学习停止策略决定何时终止循环。在Waymo和ReasonSeg数据集上的实验表明，该方法优于现有方法，展示了强大的泛化能力和指令遵循能力。</div>
</details>
</div>
<div class="card">
<div class="title">From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation</div>
<div class="meta-line">Authors: Dawid Malarz, Artur Kasymov, Filip Manjak, Maciej Zięba, Przemysław Spurek</div>
<div class="meta-line">First: 2025-12-15T23:15:36+00:00 · Latest: 2025-12-15T23:15:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13953v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://gmum.github.io/UNBRANDING/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid progress of text-to-image diffusion models raises significant concerns regarding the unauthorized reproduction of trademarked content. While prior work targets general concepts (e.g., styles, celebrities), it fails to address specific brand identifiers. Crucially, we note that brand recognition is multi-dimensional, extending beyond explicit logos to encompass distinctive structural features (e.g., a car&#x27;s front grille). To tackle this, we introduce unbranding, a novel task for the fine-grained removal of both trademarks and subtle structural brand features, while preserving semantic coherence. To facilitate research, we construct a comprehensive benchmark dataset. Recognizing that existing brand detectors are limited to logos and fail to capture abstract trade dress (e.g., the shape of a Coca-Cola bottle), we introduce a novel evaluation metric based on Vision Language Models (VLMs). This VLM-based metric uses a question-answering framework to probe images for both explicit logos and implicit, holistic brand characteristics. Furthermore, we observe that as model fidelity increases, with newer systems (SDXL, FLUX) synthesizing brand identifiers more readily than older models (Stable Diffusion), the urgency of the unbranding challenge is starkly highlighted. Our results, validated by our VLM metric, confirm unbranding is a distinct, practically relevant problem requiring specialized techniques. Project Page: https://gmum.github.io/UNBRANDING/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从重新学习到UNBRANDING：商标安全文本到图像生成的基准</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型的迅速发展引发了对未经授权复制商标内容的重大关切。尽管先前的工作针对一般概念（例如，风格、名人），但未能解决特定品牌标识的问题。我们注意到，品牌识别是多维度的，不仅限于显性的标志，还包括独特的结构特征（例如，汽车的前格栅）。为应对这一挑战，我们引入了UNBRANDING这一新任务，旨在精细去除商标和微妙的品牌结构特征，同时保持语义连贯性。为促进研究，我们构建了一个全面的基准数据集。鉴于现有品牌检测器仅限于标志，无法捕捉抽象的贸易外观（例如，可口可乐瓶子的形状），我们引入了一种基于视觉语言模型（VLM）的新评估指标。该VLM指标使用问答框架来探测图像中的显性标志和隐含的整体品牌特征。此外，我们观察到，随着模型保真度的提高，新系统（如SDXL、FLUX）比旧系统（如Stable Diffusion）更容易生成品牌标识，这突显了UNBRANDING挑战的紧迫性。我们的结果，经由VLM指标验证，证实了UNBRANDING是一个独特且实际相关的问题，需要专门的技术来解决。项目页面：https://gmum.github.io/UNBRANDING/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of unauthorized reproduction of trademarked content by text-to-image models. It introduces the concept of unbranding, which involves removing both explicit trademarks and subtle structural brand features while maintaining semantic coherence. The authors develop a comprehensive benchmark dataset and a novel evaluation metric based on Vision Language Models (VLMs) to assess the effectiveness of unbranding techniques. The results show that newer models like SDXL and FLUX are more prone to generating brand identifiers, emphasizing the need for specialized unbranding methods. The VLM-based metric confirms that unbranding is a distinct and practically relevant challenge in text-to-image generation.</div>
<div class="mono" style="margin-top:8px">该论文旨在解决文本到图像模型中未经授权复制商标内容的问题。它引入了去品牌化任务，该任务涉及去除显性和隐性品牌特征，同时保持语义连贯性。作者开发了一个基准数据集和基于视觉语言模型的新颖评估指标来评估去品牌化技术的有效性。结果显示，如SDXL和FLUX等较新模型更容易生成品牌标识，突显了需要专门的去品牌化方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness</div>
<div class="meta-line">Authors: Xiaoyun Xu, Shujian Yu, Zhuoran Liu, Stjepan Picek</div>
<div class="meta-line">First: 2023-12-08T10:50:02+00:00 · Latest: 2025-12-15T22:24:07+00:00</div>
<div class="meta-line">Comments: Accepted by NDSS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.04960v5">Abs</a> · <a href="https://arxiv.org/pdf/2312.04960v5">PDF</a> · <a href="https://github.com/xiaoyunxxy/MIMIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers (ViTs) have emerged as a fundamental architecture and serve as the backbone of modern vision-language models. Despite their impressive performance, ViTs exhibit notable vulnerability to evasion attacks, necessitating the development of specialized Adversarial Training (AT) strategies tailored to their unique architecture. While a direct solution might involve applying existing AT methods to ViTs, our analysis reveals significant incompatibilities, particularly with state-of-the-art (SOTA) approaches such as Generalist (CVPR 2023) and DBAT (USENIX Security 2024). This paper presents a systematic investigation of adversarial robustness in ViTs and provides a novel theoretical Mutual Information (MI) analysis in its autoencoder-based self-supervised pre-training. Specifically, we show that MI between the adversarial example and its latent representation in ViT-based autoencoders should be constrained via derived MI bounds. Building on this insight, we propose a self-supervised AT method, MIMIR, that employs an MI penalty to facilitate adversarial pre-training by masked image modeling with autoencoders. Extensive experiments on CIFAR-10, Tiny-ImageNet, and ImageNet-1K show that MIMIR can consistently provide improved natural and robust accuracy, where MIMIR outperforms SOTA AT results on ImageNet-1K. Notably, MIMIR demonstrates superior robustness against unforeseen attacks and common corruption data and can also withstand adaptive attacks where the adversary possesses full knowledge of the defense mechanism. Our code and trained models are publicly available at: https://github.com/xiaoyunxxy/MIMIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MIMIR：基于互信息的对抗鲁棒性掩蔽图像建模</div>
<div class="mono" style="margin-top:8px">视觉变换器（ViTs）已成为一种基本架构，并作为现代视觉-语言模型的骨干。尽管它们表现出色，但ViTs在欺骗攻击面前显示出明显的脆弱性，这需要开发专门针对其独特架构的对抗训练（AT）策略。虽然直接解决方案可能涉及将现有的AT方法应用于ViTs，但我们的分析揭示了与最先进的（SOTA）方法如Generalist（CVPR 2023）和DBAT（USENIX Security 2024）之间存在显著的不兼容性。本文系统地研究了ViTs的对抗鲁棒性，并提供了其基于自编码器的半监督预训练中的互信息（MI）分析的新型理论。具体而言，我们展示了在基于ViT的自编码器中，对抗样本与其潜在表示之间的互信息应通过导出的互信息界进行约束。基于这一洞察，我们提出了一种半监督AT方法MIMIR，该方法利用互信息惩罚通过自编码器进行掩蔽图像建模以促进对抗预训练。在CIFAR-10、Tiny-ImageNet和ImageNet-1K上的广泛实验表明，MIMIR可以一致地提供改进的自然和鲁棒准确性，其中MIMIR在ImageNet-1K上的结果优于SOTA AT结果。值得注意的是，MIMIR在未预见的攻击和常见损坏数据面前表现出更强的鲁棒性，并且还可以抵御适应性攻击，其中对手完全了解防御机制。我们的代码和训练模型已公开发布于：https://github.com/xiaoyunxxy/MIMIR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of Vision Transformers (ViTs) to evasion attacks by proposing a novel Adversarial Training (AT) method called MIMIR. The authors conduct a systematic analysis of adversarial robustness in ViTs and introduce a Mutual Information (MI) analysis in autoencoder-based self-supervised pre-training. MIMIR employs an MI penalty to constrain the mutual information between adversarial examples and their latent representations, leading to improved natural and robust accuracy on CIFAR-10, Tiny-ImageNet, and ImageNet-1K. MIMIR demonstrates superior robustness against various attacks and corrupted data, and the code and models are publicly available.</div>
<div class="mono" style="margin-top:8px">本文针对Vision Transformers (ViTs) 对于欺骗攻击的脆弱性，提出了一种名为MIMIR的新型对抗训练方法。该方法利用自监督预训练中的互信息（MI）分析来约束对抗样本与其潜在表示之间的互信息。在CIFAR-10、Tiny-ImageNet和ImageNet-1K上的实验表明，MIMIR能够提高自然准确性和鲁棒性，并在ImageNet-1K上优于当前最佳的对抗训练方法，同时对各种攻击具有更强的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis</div>
<div class="meta-line">Authors: Yu Xin, Gorkem Can Ates, Kuang Gong, Wei Shao</div>
<div class="meta-line">First: 2025-03-25T20:09:30+00:00 · Latest: 2025-12-15T20:51:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.20047v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.20047v3">PDF</a> · <a href="https://github.com/mirthAI/Med3DVLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have shown promise in 2D medical image analysis, but extending them to 3D remains challenging due to the high computational demands of volumetric data and the difficulty of aligning 3D spatial features with clinical text. We present Med3DVLM, a 3D VLM designed to address these challenges through three key innovations: (1) DCFormer, an efficient encoder that uses decomposed 3D convolutions to capture fine-grained spatial features at scale; (2) SigLIP, a contrastive learning strategy with pairwise sigmoid loss that improves image-text alignment without relying on large negative batches; and (3) a dual-stream MLP-Mixer projector that fuses low- and high-level image features with text embeddings for richer multi-modal representations. We evaluate our model on the M3D dataset, which includes radiology reports and VQA data for 120,084 3D medical images. Results show that Med3DVLM achieves superior performance across multiple benchmarks. For image-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly outperforming the current state-of-the-art M3D model (19.10%). For report generation, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended visual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in closed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results highlight Med3DVLM&#x27;s ability to bridge the gap between 3D imaging and language, enabling scalable, multi-task reasoning across clinical applications. Our code is publicly available at https://github.com/mirthAI/Med3DVLM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Med3DVLM is designed to enhance 3D medical image analysis by addressing the challenges of volumetric data and image-text alignment. It introduces DCFormer for efficient 3D feature extraction, SigLIP for improved image-text alignment, and a dual-stream MLP-Mixer for multi-modal representation fusion. On the M3D dataset, Med3DVLM outperforms the state-of-the-art in image-text retrieval, report generation, and VQA tasks, demonstrating its effectiveness in bridging 3D imaging and language for clinical applications.</div>
<div class="mono" style="margin-top:8px">Med3DVLM旨在通过解决体积数据和图像-文本对齐的挑战来增强3D医学图像分析。它引入了DCFormer进行高效的3D特征提取，SigLIP以提高图像-文本对齐，并使用双流MLP-Mixer进行多模态表示融合。在M3D数据集上，Med3DVLM在图像-文本检索、报告生成和VQA任务中均优于当前最先进的模型，展示了其在临床应用中将3D成像与语言相结合的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251217_0321.html">20251217_0321</a>
<a href="archive/20251216_0326.html">20251216_0326</a>
<a href="archive/20251215_0321.html">20251215_0321</a>
<a href="archive/20251214_0316.html">20251214_0316</a>
<a href="archive/20251213_0319.html">20251213_0319</a>
<a href="archive/20251212_0322.html">20251212_0322</a>
<a href="archive/20251211_0319.html">20251211_0319</a>
<a href="archive/20251210_0317.html">20251210_0317</a>
<a href="archive/20251209_0321.html">20251209_0321</a>
<a href="archive/20251208_0317.html">20251208_0317</a>
<a href="archive/20251207_0317.html">20251207_0317</a>
<a href="archive/20251206_0318.html">20251206_0318</a>
<a href="archive/20251205_0320.html">20251205_0320</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
