<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-24 03:20</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251224_0320</div>
    <div class="row"><div class="card">
<div class="title">Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</div>
<div class="meta-line">Authors: Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra</div>
<div class="meta-line">First: 2025-12-22T18:41:45+00:00 · Latest: 2025-12-22T18:41:45+00:00</div>
<div class="meta-line">Comments: 14 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19663v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP&#x27;s 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越CLIP：知识增强的多模态变换器在糖尿病视网膜病变诊断中的跨模态对齐</div>
<div class="mono" style="margin-top:8px">糖尿病视网膜病变（DR）是全球可预防失明的主要原因，需要准确的自动化诊断系统。虽然通用领域的视觉-语言模型如对比语言-图像预训练（CLIP）在自然图像任务上表现良好，但在医学领域的应用中却遇到困难，特别是在眼科图像的跨模态检索方面。我们提出了一种新颖的知识增强联合嵌入框架，通过多模态变换器架构整合视网膜底片图像、临床文本和结构化患者数据，以解决医学图像-文本对齐的关键差距。我们的方法为每种模态使用单独的编码器：视网膜图像使用视觉变换器（ViT-B/16），临床叙述使用Bio-ClinicalBERT，结构化的人口统计和临床特征使用多层感知机。这些模态通过具有模态特定嵌入的联合变换器融合，使用包括模态对之间的对比损失、图像和文本的重构损失以及根据ICDR和SDRG方案的DR严重程度分类损失的多种目标进行训练。在巴西多标签眼科数据集（BRSET）上的实验结果表明，与基线模型相比有显著改进。我们的框架在文本到图像检索性能上达到99.94%的召回率@1，而微调后的CLIP仅为1.29%，同时保持SDRG分类准确率为97.05%，ICDR分类准确率为97.97%。此外，对未见过的DeepEyeNet数据集的零样本评估验证了强大的泛化能力，召回率@1为93.95%，而微调后的CLIP仅为0.22%。这些结果表明，我们的多模态训练方法有效地捕捉了医学领域的跨模态关系，建立了卓越的检索能力和稳健的诊断性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve automated diagnostic systems for diabetic retinopathy (DR) by developing a knowledge-enhanced joint embedding framework using a multimodal transformer architecture. This framework integrates retinal images, clinical text, and structured patient data, employing separate encoders for each modality and a joint transformer for fusion. The model is trained with multiple objectives, including contrastive losses, reconstruction losses, and classification losses. Experiments on the BRSET dataset show significant improvements over baseline models, achieving 99.94% Recall@1 for text-to-image retrieval and maintaining high classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Zero-shot evaluation on the DeepEyeNet dataset further validates its generalizability.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决通用视觉-语言模型如CLIP在医疗应用中的局限性，提高糖尿病视网膜病变（DR）的自动化诊断系统。研究提出了一种知识增强的联合嵌入框架，使用多模态变压器架构整合视网膜图像、临床文本和结构化患者数据。该框架在文本到图像检索性能（Recall@1为99.94%）和分类准确性（SDRG为97.05%，ICDR为97.97%）方面取得了显著进步。</div>
</details>
</div>
<div class="card">
<div class="title">AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models</div>
<div class="meta-line">Authors: Heng Zhang, Haichuan Hu, Yaomin Shen, Weihao Yu, Yilei Yuan, Haochen You, Guo Cheng, Zijian Zhang, Lubin Gan, Huihui Wei, Hao Zhang, Jin Huang</div>
<div class="meta-line">First: 2025-09-16T06:16:05+00:00 · Latest: 2025-12-22T18:22:20+00:00</div>
<div class="meta-line">Comments: This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12715v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.12715v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AsyMoE：利用模态不对称性增强大型视觉-语言模型专家专业化</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）通过扩展架构和大量训练，在多模态任务中表现出色。然而，现有的混合专家（MoE）方法由于视觉和语言处理之间的不对称性而面临挑战。视觉信息是空间上完整的，而语言需要保持顺序上下文。因此，MoE模型难以平衡模态特定特征和跨模态交互。通过系统分析，我们观察到深层的语言专家逐渐失去上下文定位，更多依赖参数知识，而不是利用提供的视觉和语言信息。为了解决这个问题，我们提出了一种新的AsyMoE架构，该架构使用三个专门的专家组来建模这种不对称性。我们设计了跨模态专家进行模态特定处理，超曲面跨模态专家进行分层跨模态交互，并设计了证据优先的语言专家来抑制参数偏差并保持上下文定位。广泛的实验表明，与vanilla MoE和模态特定MoE相比，AsyMoE分别实现了26.58%和15.45%的准确率提升，且参数激活量比密集模型少25.45%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of asymmetry between visual and linguistic processing in existing Mixture of Experts (MoE) models for large Vision-Language Models (LVLMs). It proposes AsyMoE, which introduces three specialized expert groups to handle this asymmetry: intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to maintain contextual grounding. Experiments show that AsyMoE outperforms vanilla MoE and modality-specific MoE by 26.58% and 15.45% in accuracy, respectively, while using fewer parameters.</div>
<div class="mono" style="margin-top:8px">论文旨在解决现有混合专家（MoE）方法在大型视觉-语言模型（LVLM）中由于视觉和语言处理之间的不对称性所面临的挑战。它提出了AsyMoE，引入了三种专门的专家组来建模这种不对称性：模态内专家进行模态特定处理，超球面跨模态专家进行分层跨模态交互，以及证据优先语言专家以保持上下文接地。实验表明，AsyMoE在准确率上分别比vanilla MoE和模态特定MoE高出26.58%和15.45%，并且比密集模型激活的参数少25.45%。然而，由于方法中的根本错误影响了结果的有效性，提交被作者撤回了。</div>
</details>
</div>
<div class="card">
<div class="title">GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</div>
<div class="meta-line">Authors: Heng Zheng, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Hao Zhang, Wenjun Huang, Jin Huang</div>
<div class="meta-line">First: 2025-11-02T11:58:55+00:00 · Latest: 2025-12-22T18:21:18+00:00</div>
<div class="meta-line">Comments: This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00908v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00908v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphGeo：基于异构图神经网络的多智能体辩论框架用于视觉地理定位</div>
<div class="mono" style="margin-top:8px">视觉地理定位需要广泛的空间知识和复杂的推理来确定图像位置，而不依赖GPS元数据。传统检索方法受限于数据库的覆盖范围和质量。最近的大规模视觉-语言模型（LVLMs）能够直接从图像内容进行位置推理，但单个模型在处理多样化的地理区域和复杂的场景时存在困难。现有的多智能体系统通过模型协作来提高性能，但所有智能体交互均被统一处理。它们缺乏有效处理相互矛盾预测的机制。我们提出 **GraphGeo**，一种使用异构图神经网络的多智能体辩论框架，用于视觉地理定位。我们的方法通过类型化的边来建模多样的辩论关系，区分支持性的合作、竞争性的论辩以及知识转移。我们引入了一种结合节点级细化和边级论辩建模的双层辩论机制。跨层拓扑细化策略使图结构和智能体表示能够共同进化。在多个基准上的实验表明，GraphGeo 显著优于现有最佳方法。我们的框架通过结构化的辩论将智能体之间的认知冲突转化为增强的地理定位准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GraphGeo is a multi-agent debate framework for visual geo-localization using heterogeneous graph neural networks. It models diverse debate relationships and introduces a dual-level debate mechanism to enhance reasoning. Experiments show that GraphGeo significantly outperforms existing methods. However, the submission was withdrawn due to a fundamental error in the methodology that affects the validity of the results.</div>
<div class="mono" style="margin-top:8px">研究旨在通过克服传统方法和LVLM的局限性，提高视觉地理定位的准确性。GraphGeo 使用多代理辩论框架和异质图神经网络来建模多样化的辩论关系并增强推理能力。实验表明，GraphGeo 的性能优于现有方法。然而，由于方法中的根本错误，该提交已被撤回，这影响了主要结果的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</div>
<div class="meta-line">Authors: Moritz Böhle, Amélie Royer, Juliette Marrie, Edouard Grave, Patrick Pérez</div>
<div class="meta-line">First: 2025-12-22T16:21:39+00:00 · Latest: 2025-12-22T16:21:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19535v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CASA：通过自注意力实现的跨注意力高效视觉-语言融合</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通常通过将预训练视觉编码器中的图像标记插入语言模型的文字流中来进行训练。这使得文本和图像信息能够在模型内部完全相互关注，但对高分辨率图像、长对话或流式视频来说，这在内存和计算上都非常昂贵。利用跨注意力的VLMs是标记插入的高效替代方案，但在涉及精细视觉细节的任务上表现出明显的性能差距。我们发现，提高此类模型的关键在于在专门的跨注意力层中也启用局部文本到文本的交互。基于此，我们提出了CASA（Cross-Attention via Self-Attention），一种简单而高效的范式，它在常见的图像理解基准测试中显著缩小了与完整标记插入的差距，同时在长上下文多模态任务如流式视频字幕生成中保持与跨注意力模型相同的可扩展性。如需查看示例和代码，请参见我们的项目页面https://kyutai.org/casa 。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the high computational cost of vision-language models (VLMs) when dealing with high-resolution images or long conversations. It proposes CASA, a method that combines cross-attention with self-attention to enable local text-to-text interaction within cross-attention layers, thereby reducing the performance gap with full token insertion. CASA achieves comparable performance to VLMs with full token insertion on common image understanding benchmarks while maintaining scalability for long-context tasks like streaming video captioning.</div>
<div class="mono" style="margin-top:8px">论文针对视觉-语言模型（VLMs）在处理高分辨率图像或长对话时的高计算成本问题，提出了一种CASA方法，该方法结合了交叉注意和自我注意，以在交叉注意层中实现局部文本到文本的交互，从而减少与全token插入模型之间的性能差距。CASA在常见的图像理解基准测试上达到了与VLMs全token插入模型相当的性能，同时保持了对长上下文任务（如流式视频字幕生成）的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</div>
<div class="meta-line">Authors: Li Puyin, Tiange Xiang, Ella Mao, Shirley Wei, Xinye Chen, Adnan Masood, Li Fei-fei, Ehsan Adeli</div>
<div class="meta-line">First: 2025-12-22T16:18:00+00:00 · Latest: 2025-12-22T16:18:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19526v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM&#x27;s physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM&#x27;s performance on estimating an object&#x27;s size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QuantiPhy：评估视觉语言模型物理推理能力的定量基准</div>
<div class="mono" style="margin-top:8px">理解物理世界对于通用人工智能代理至关重要。然而，尚不清楚最先进的视觉感知模型（例如大型VLM）是否能够进行定量的物理属性推理。现有的评估主要基于VQA且为定性的，提供的关于这些模型能否从视频观察中推断出移动物体的动力学量的见解有限。为解决这一问题，我们提出了QuantiPhy，这是第一个旨在定量测量VLM物理推理能力的基准。QuantiPhy包含超过3300个视频-文本实例，具有数值真实值，评估VLM在给定时间戳时估计物体大小、速度和加速度的表现，其中一个属性作为输入先验。基准标准化了提示和评分，以评估数值准确性，从而实现模型之间的公平比较。我们在最先进的VLM上的实验揭示了它们的定性合理性与实际数值正确性之间的一致差距。我们进一步深入分析了背景噪声、反事实先验和策略性提示等关键因素，发现最先进的VLM在进行定量动力学属性推理时，严重依赖预训练的世界知识，而不是忠实使用提供的视觉和文本输入作为参考。QuantiPhy提供了第一个严格的、可扩展的测试平台，推动VLM超越单纯的口头合理性，迈向基于数字的物理理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">QuantiPhy is a benchmark designed to evaluate the quantitative physical reasoning abilities of vision-language models. It consists of over 3,300 video-text instances with numerical ground truth, assessing models&#x27; ability to estimate an object&#x27;s size, velocity, and acceleration. Experiments show a gap between models&#x27; qualitative plausibility and numerical accuracy, indicating reliance on pre-trained knowledge rather than visual and textual inputs for quantitative reasoning.</div>
<div class="mono" style="margin-top:8px">QuantiPhy 是一个基准，用于评估视觉语言模型的定量物理推理能力。它包含超过 3,300 个视频-文本实例，具有数值 ground truth，评估模型估计物体大小、速度和加速度的能力。实验表明，模型的定性合理性与数值准确性之间存在差距，表明模型在推理动力学属性时更多依赖预训练知识而非视觉和文本输入。该基准提供了一种标准化的方法来比较模型，并推动它们向数值化的物理理解迈进。</div>
</details>
</div>
<div class="card">
<div class="title">VERDI: VLM-Embedded Reasoning for Autonomous Driving</div>
<div class="meta-line">Authors: Bowen Feng, Zhiting Mei, Baiang Li, Julian Ost, Filippo Ghilotti, Roger Girgis, Anirudha Majumdar, Felix Heide</div>
<div class="meta-line">First: 2025-05-21T18:24:36+00:00 · Latest: 2025-12-22T15:37:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.15925v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.15925v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VERDI: VLM嵌入式自主驾驶推理</div>
<div class="mono" style="margin-top:8px">在面对部分可观测性和现实复杂性带来的决策难题时，自主驾驶（AD）堆栈难以做出最优决策，而人类驾驶员则能够利用常识推理在信息有限的情况下做出近乎最优的决策。近期的研究尝试利用微调后的视觉-语言模型（VLMs）在推理时进行轨迹规划，以模拟人类行为。尽管这些方法在基准测试中表现出色，但它们在部署时往往不切实际（一个700亿参数的VLM推理需要每秒8个词，内存超过160G），并且其单一网络结构限制了安全性分解。为解决这一问题，我们提出了VLM嵌入式自主驾驶推理（VERDI），这是一种训练时框架，将VLM的推理过程和常识知识提炼到AD堆栈中。VERDI通过将模块化可微端到端（e2e）AD模型与VLM生成的解释驾驶推理过程的文本特征在感知、预测和规划阶段对齐，从而在潜在空间中促进对齐。通过这种方式，VERDI使模块化AD堆栈能够内化结构化推理，而不必承担大型VLM的推理时间成本。我们分别在开环（NuScenes和Bench2Drive基准）和闭环（HugSim模拟器）环境中验证了VERDI。结果显示，与不嵌入推理的现有端到端方法相比，VERDI在欧氏距离上提高了11%，在驾驶性能上提高了11%，同时保持了实时推理速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VERDI proposes a training-time framework that embeds commonsense reasoning from Vision-Language Models (VLMs) into autonomous driving systems to enhance decision-making under partial observability. By aligning intermediate outputs with text features from VLMs, VERDI enables modular AD models to internalize structured reasoning without the high inference-time costs of large VLMs. Experimental results show that VERDI outperforms existing end-to-end methods by up to 11% in $l_2$ distance and driving performance across open-loop and closed-loop settings, while maintaining real-time inference speed.</div>
<div class="mono" style="margin-top:8px">VERDI 是一个训练时框架，将 Vision-Language 模型（VLM）中的常识推理嵌入到自动驾驶系统中，以改善在部分可观测性下的决策能力。它通过将感知、预测和规划模块的中间输出与 VLM 生成的文本解释对齐，使 AD 堆栈能够内化结构化的推理，而不增加大型 VLM 的高推理时间成本。VERDI 在各种基准测试中比现有端到端方法在 $\ell_{2}$ 距离和驾驶性能上分别提高了最多 11%，同时保持实时推理速度。</div>
</details>
</div>
<div class="card">
<div class="title">SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning</div>
<div class="meta-line">Authors: Xiaojun Guo, Runyu Zhou, Yifei Wang, Qi Zhang, Chenheng Zhang, Stefanie Jegelka, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin, Yisen Wang</div>
<div class="meta-line">First: 2025-10-18T09:22:40+00:00 · Latest: 2025-12-22T15:14:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16416v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.16416v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework&#x27;s generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSL4RL：重新审视自监督学习作为视觉-语言推理内在奖励的方法</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通过将大型语言模型与视觉输入结合，展示了显著的能力。然而，它们往往未能充分利用视觉证据，要么依赖于视觉中心任务中的语言先验，要么在推理过程中求助于文本捷径。尽管强化学习（RL）可以将模型与期望的行为对齐，但将其应用于VLMs受到了缺乏可扩展且可靠的奖励机制的阻碍。为克服这一挑战，我们提出了一种名为SSL4RL的新框架，该框架利用自监督学习（SSL）任务作为RL基础微调的验证性奖励来源。我们的方法将SSL目标，如预测图像旋转或重建遮罩片段，重新表述为密集的自动奖励信号，从而消除了对人类偏好数据或不可靠的人工智能评估者的需要。实验表明，SSL4RL在视觉中心任务和视觉-语言推理基准测试中显著提高了性能。此外，通过系统性的消融实验，我们确定了影响SSL4RL任务有效性的关键因素，如任务难度、模型规模和与目标领域的语义对齐，为未来工作提供了新的设计原则。我们还通过将其应用于图学习，展示了该框架的通用性，其中取得了显著的改进。SSL4RL建立了一种灵活且有效的范式，用于使用可验证的自监督目标对多模态模型进行对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes SSL4RL, a framework that uses self-supervised learning tasks as intrinsic rewards for reinforcement learning in vision-language models. This approach improves performance on both vision-centric and vision-language reasoning benchmarks by providing dense, automatic reward signals without the need for human preference data. The study identifies key factors affecting the effectiveness of SSL4RL tasks and demonstrates its generality by applying it to graph learning, yielding significant gains.</div>
<div class="mono" style="margin-top:8px">论文提出了SSL4RL框架，该框架利用自我监督学习(SSL)任务作为强化学习(RL)微调视觉语言模型(VLMs)的内在奖励。这种方法通过提供密集的自动奖励信号，提高了视觉中心和视觉语言推理基准上的性能。研究还确定了影响SSL4RL任务有效性的关键因素，并通过将其应用于图学习来展示其普适性，显示出显著的改进。</div>
</details>
</div>
<div class="card">
<div class="title">EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration</div>
<div class="meta-line">Authors: Runze Li, Yuwen Zhai, Bo Xu, LiWu Xu, Nian Shi, Wei Zhang, Ran Lin, Liang Wang</div>
<div class="meta-line">First: 2025-12-22T13:42:18+00:00 · Latest: 2025-12-22T13:42:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19396v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19396v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital &#x27;&#x27;amnesia&#x27;&#x27; results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable &#x27;&#x27;memories&#x27;&#x27;. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent&#x27;s reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EchoTrail-GUI：通过评论引导自我探索构建GUI代理的操作性记忆</div>
<div class="mono" style="margin-top:8px">当代GUI代理由于大型视觉-语言模型（VLMs）的进步而变得越来越强大，但它们通常在每个任务上孤立操作，缺乏系统学习过去成功经验的机制。这种“数字健忘症”导致了次优性能、重复错误和对新挑战的不良泛化。为解决这一问题，我们提出了EchoTrail-GUI，这是一种新型框架，旨在通过为代理提供动态且易于访问的记忆来模拟人类经验学习。我们的框架分为三个阶段。首先，在经验探索阶段，代理自主与GUI环境交互，构建由奖励模型验证的成功任务轨迹数据库，整个知识库构建过程完全自动化，无需人类监督。其次，在记忆注入阶段，当收到新任务时，系统高效检索最相关的过去轨迹，作为可操作的“记忆”。最后，在GUI任务推理阶段，这些记忆作为上下文指导注入，以指导代理的推理和决策过程。我们在Android World和AndroidLab等基准测试上展示了我们方法的有效性。结果表明，EchoTrail-GUI 显著提高了基线代理的任务成功率和操作效率，验证了结构化记忆在创建更强大和智能的GUI自动化方面的强大功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EchoTrail-GUI is a framework that addresses the issue of digital amnesia in GUI agents by providing them with a dynamic memory system. The framework consists of three stages: Experience Exploration, where agents autonomously learn from successful task trajectories; Memory Injection, where relevant past trajectories are retrieved for new tasks; and GUI Task Inference, where these memories guide the agent&#x27;s decision-making. Experiments on Android World and AndroidLab show that EchoTrail-GUI enhances task success rates and operational efficiency compared to baseline agents.</div>
<div class="mono" style="margin-top:8px">EchoTrail-GUI 是一个框架，旨在通过为 GUI 代理提供动态记忆系统来解决数字健忘的问题。该框架包括三个阶段：经验探索，其中代理自主学习成功的任务轨迹；记忆注入，其中为新任务检索相关的历史轨迹；以及 GUI 任务推理，其中这些记忆指导代理的决策过程。实验结果表明，EchoTrail-GUI 在 Android World 和 AndroidLab 上提高了任务成功率和操作效率，验证了结构化记忆在创建更强大和智能的 GUI 自动化方面的力量。</div>
</details>
</div>
<div class="card">
<div class="title">Xiaomi MiMo-VL-Miloco Technical Report</div>
<div class="meta-line">Authors: Jiaze Li, Jingyang Chen, Yuxun Qu, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, Wenhui Tan, Pei Fu, Jianzhong Ju, Zhenbo Luo, Jian Luan</div>
<div class="meta-line">First: 2025-12-19T10:43:37+00:00 · Latest: 2025-12-22T13:27:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17436v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17436v2">PDF</a> · <a href="https://github.com/XiaoMi/xiaomi-mimo-vl-miloco">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We open-source MiMo-VL-Miloco-7B and its quantized variant MiMo-VL-Miloco-7B-GGUF, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at https://github.com/XiaoMi/xiaomi-mimo-vl-miloco to support research and deployment in real-world smart-home applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小米MiMo-VL-Miloco技术报告</div>
<div class="mono" style="margin-top:8px">我们开源了MiMo-VL-Miloco-7B及其量化变体MiMo-VL-Miloco-7B-GGUF，这是一个面向家庭场景的视觉-语言模型对，同时在家庭场景理解和通用多模态推理方面表现出色。基于MiMo-VL-7B骨干网络，MiMo-VL-Miloco-7B专门针对智能家居环境，实现了手势识别和常见家庭场景理解的领先F1分数，并在视频基准测试（如Video-MME、Video-MMMU和Charades-STA）以及语言理解基准测试（如MMMU-Pro和MMLU-Pro）中也取得了持续的改进。在我们的实验中，MiMo-VL-Miloco-7B在家庭场景理解和多个多模态推理基准测试中均优于强大的闭源和开源基线。为了平衡专业化和通用性，我们设计了一种两阶段训练管道，结合了监督微调和基于组相对策略优化的强化学习，利用高效的多域数据。我们进一步引入了思维链监督和令牌预算感知推理，使模型能够在数据高效学习的同时，也能高效推理。我们的分析表明，针对家庭场景的训练不仅增强了活动和手势理解，还仅以适度的文档中心任务权衡提高了文本推理能力。模型检查点、量化GGUF权重以及我们的家庭场景评估工具包可在https://github.com/XiaoMi/xiaomi-mimo-vl-miloco 公开获取，以支持在实际智能家居应用中的研究和部署。</div>
</details>
</div>
<div class="card">
<div class="title">SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: A. A. Gde Yogi Pramana, Jason Ray, Anthony Jaya, Michael Wijaya</div>
<div class="meta-line">First: 2025-12-22T12:07:33+00:00 · Latest: 2025-12-22T12:07:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19317v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19317v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\% accuracy on clean inputs, collapse to approximately 25\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeMed-R1：针对通用和鲁棒医学推理的对抗强化学习</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在医学视觉问答（VQA）中显示出显著的潜力，但在临床环境中的部署受到严重对抗攻击的威胁。标准的对抗训练虽然对简单任务有效，但往往会降低泛化性能和生成的临床推理质量。我们提出了SafeMed-R1，这是一种混合防御框架，确保在保持高质量、可解释的医学推理的同时实现鲁棒性能。SafeMed-R1采用两阶段方法：在训练阶段，我们结合对抗训练与组相对策略优化（AT-GRPO）以明确地使推理过程对抗最坏情况的扰动；在推理阶段，我们通过随机平滑增强模型，提供$L_2$范数鲁棒性保证。我们在OmniMedVQA基准上对SafeMed-R1进行了评估，涵盖了88,000多个样本，涉及八种医学成像模态。我们的实验表明，尽管标准微调的VLMs在干净输入上达到95%的准确率，但在PGD攻击下准确率降至约25%。相比之下，SafeMed-R1在相同对抗条件下保持84.45%的准确率，显示出59个百分点的鲁棒性提升。此外，我们证明了带有明确推理链的模型相比仅指令版本具有更好的对抗鲁棒性，这表明在医学AI系统中可解释性和安全性之间存在协同作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SafeMed-R1 is a hybrid defense framework designed to enhance the robustness of Vision-Language Models (VLMs) in medical applications. It uses a two-stage approach: adversarial training with Group Relative Policy Optimization (AT-GRPO) during training to robustify the reasoning process, and Randomized Smoothing at inference time for certified $L_2$-norm robustness. SafeMed-R1 significantly improves robustness under adversarial attacks, maintaining 84.45% accuracy compared to 25% for standard models. Models with explicit chain-of-thought reasoning show better adversarial robustness, indicating a positive correlation between interpretability and security.</div>
<div class="mono" style="margin-top:8px">论文提出了SafeMed-R1，这是一种针对医学视觉问答中视觉-语言模型的混合防御框架，以应对对抗性攻击。该框架采用两阶段方法：训练时使用组相对策略优化的对抗训练来增强推理过程的鲁棒性，在推理时使用随机化平滑提供$L_2$范数的认证鲁棒性保证。SafeMed-R1显著提高了鲁棒性，在PGD攻击下保持84.45%的准确率，而标准模型则降至25%，并且展示了具有显式推理链的模型比仅指令模型更具鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing</div>
<div class="meta-line">Authors: Xu Zhang, Junyao Ge, Yang Zheng, Kaitai Guo, Jimin Liang</div>
<div class="meta-line">First: 2025-12-22T11:46:42+00:00 · Latest: 2025-12-22T11:46:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19302v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19302v1">PDF</a> · <a href="https://github.com/Ricardo-XZ/Think2Seg-RS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义与几何融合：一种解耦的LVLM-SAM框架用于遥感推理分割</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）在推进遥感（RS）分析方面具有巨大潜力，但现有的推理分割框架通过端到端的监督微调将语言推理和像素预测耦合在一起，导致几何定位薄弱且跨任务泛化能力有限。为了解决这一问题，我们开发了Think2Seg-RS，这是一种解耦框架，通过结构化的几何提示训练LVLM提示器控制冻结的分割任何模型（SAM）。通过掩码强化学习目标，LVLM学习将抽象的语义推理转化为空间上定位的动作，实现了在EarthReason数据集上的最佳性能。令人惊讶的是，学习到的提示策略在多个引用分割基准上实现了零样本泛化，揭示了语义级和实例级定位之间的明显差异。我们还发现，在语义级监督下，紧凑的分割器优于较大的分割器，并且在异质航空背景中负提示无效。这些发现共同确立了语义级推理分割作为地理空间理解的新范式，为统一、可解释的LVLM驱动地球观测铺平了道路。我们的代码和模型可在https://github.com/Ricardo-XZ/Think2Seg-RS获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the geometric grounding and generalization of remote sensing analysis using large vision-language models (LVLMs). It introduces Think2Seg-RS, a decoupled framework that separates linguistic reasoning from pixel prediction by training an LVLM prompter to control a frozen Segment Anything Model (SAM) with structured geometric prompts. The framework achieves state-of-the-art performance on the EarthReason dataset and demonstrates zero-shot generalization to multiple benchmarks, highlighting the divide between semantic-level and instance-level grounding. The study also finds that compact segmenters outperform larger ones under semantic-level supervision and that negative prompts are ineffective in heterogeneous aerial backgrounds.</div>
<div class="mono" style="margin-top:8px">研究旨在通过大型视觉语言模型（LVLM）提高遥感分析的几何定位和泛化能力。作者开发了Think2Seg-RS，这是一种分离语言推理和像素预测的框架。通过训练LVLM提示器控制冻结的SAM模型并使用结构化的几何提示，该框架在EarthReason数据集上达到了最先进的性能。学习到的提示策略在多个基准上表现出良好的泛化能力，突显了语义级推理的重要性而非实例级定位。研究还表明，在异质航空背景中，紧凑的分割器优于较大的模型，并且负向提示在语义级监督下无效。</div>
</details>
</div>
<div class="card">
<div class="title">VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis</div>
<div class="meta-line">Authors: Meng Chu, Senqiao Yang, Haoxuan Che, Suiyun Zhang, Xichen Zhang, Shaozuo Yu, Haokun Gui, Zhefan Rao, Dandan Tu, Rui Liu, Jiaya Jia</div>
<div class="meta-line">First: 2025-12-22T10:25:38+00:00 · Latest: 2025-12-22T10:25:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19243v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models can now produce photorealistic imagery, yet they still struggle with the long, multi-goal prompts that professional designers issue. To expose this gap and better evaluate models&#x27; performance in real-world settings, we introduce Long Goal Bench (LGBench), a 2,000-task suite (1,000 T2I and 1,000 I2I) whose average instruction contains 18 to 22 tightly coupled goals spanning global layout, local object placement, typography, and logo fidelity. We find that even state-of-the-art models satisfy fewer than 72 percent of the goals and routinely miss localized edits, confirming the brittleness of current pipelines. To address this, we present VisionDirector, a training-free vision-language supervisor that (i) extracts structured goals from long instructions, (ii) dynamically decides between one-shot generation and staged edits, (iii) runs micro-grid sampling with semantic verification and rollback after every edit, and (iv) logs goal-level rewards. We further fine-tune the planner with Group Relative Policy Optimization, yielding shorter edit trajectories (3.1 versus 4.2 steps) and stronger alignment. VisionDirector achieves new state of the art on GenEval (plus 7 percent overall) and ImgEdit (plus 0.07 absolute) while producing consistent qualitative improvements on typography, multi-object scenes, and pose editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisionDirector：视觉-语言引导的闭环精炼方法在生成图像合成中的应用</div>
<div class="mono" style="margin-top:8px">生成模型现在可以生成逼真的图像，但它们仍然难以处理专业设计师发布的长篇多目标指令。为了揭示这一差距并更好地评估模型在实际环境中的表现，我们引入了长目标基准（LGBench），这是一个包含2000个任务的套件（1000个T2I和1000个I2I），其平均指令包含18到22个紧密耦合的目标，涵盖全局布局、局部对象放置、字体设计和标志保真度。我们发现，即使是最先进的模型也仅能实现不到72%的目标，并且经常遗漏局部编辑，证实了当前管道的脆弱性。为了解决这一问题，我们提出了VisionDirector，这是一种无需训练的视觉-语言监督器，（i）从长指令中提取结构化目标，（ii）动态决定是一次生成还是分阶段编辑，（iii）进行微网格采样，并在每次编辑后进行语义验证和回滚，（iv）记录目标级奖励。我们进一步使用组相对策略优化对规划器进行微调，从而获得更短的编辑轨迹（3.1步 vs 4.2步）和更强的对齐。VisionDirector在GenEval（提高7%）和ImgEdit（提高0.07绝对值）上达到了新的最先进的水平，同时在字体设计、多对象场景和姿态编辑方面产生了持续的定性改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve generative models&#x27; ability to handle long, multi-goal instructions as used by professional designers. VisionDirector, a vision-language guided system, extracts structured goals from complex instructions, dynamically decides between one-shot generation and staged edits, and uses micro-grid sampling with semantic verification. This approach leads to shorter edit trajectories and better alignment, achieving new state-of-the-art results on GenEval and ImgEdit benchmarks while improving typography, multi-object scenes, and pose editing consistency.</div>
<div class="mono" style="margin-top:8px">研究旨在解决生成模型在处理长多目标指令方面的局限性，这些指令在专业设计中很常见。VisionDirector，一种基于视觉语言的指导系统，被引入以改进生成图像合成。它从指令中提取结构化目标，决定是进行一次生成还是分阶段编辑，并使用带有语义验证的微网格采样。VisionDirector 在 GenEval 和 ImgEdit 上达到了新的最佳结果，并在字体排版、多对象场景和姿态编辑的一致性方面取得了改进。</div>
</details>
</div>
<div class="card">
<div class="title">ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models</div>
<div class="meta-line">Authors: Mingxu Zhang, Dazhong Shen, Qi Zhang, Ying Sun</div>
<div class="meta-line">First: 2025-12-22T10:21:40+00:00 · Latest: 2025-12-22T10:21:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19240v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model&#x27;s general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM&#x27;s intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ChemATP：一种无需训练的化学推理框架用于大型语言模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在一般推理方面表现出色，但在分子科学领域却因缺乏标准字符串表示中的显式化学先验而遇到困难。当前解决方案面临一个根本性的困境。基于训练的方法将先验注入参数，但这种静态耦合阻碍了知识的快速更新，并且往往损害了模型的一般推理能力。相反，现有的无需训练的方法避免了这些问题，但依赖于表面级提示，无法提供化学推理所需的精细原子级先验。为了解决这一问题，我们引入了ChemATP，这是一种将化学知识与推理引擎解耦的框架。通过构建首个原子级文本知识库，ChemATP使冻结的LLM能够动态检索和推理这些信息。该架构确保了可解释性和适应性，同时保留了LLM固有的通用智能。实验表明，ChemATP显著优于现有的无需训练基线，并且与最先进的基于训练的模型相当，证明了显式先验注入是隐式参数更新的有竞争力的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of integrating chemical knowledge into large language models (LLMs) without compromising their general reasoning abilities. ChemATP is introduced as a framework that decouples chemical knowledge from the reasoning process, allowing frozen LLMs to dynamically retrieve and use atom-level chemical information. Experiments show that ChemATP outperforms existing training-free methods and matches the performance of training-based approaches, indicating that explicit prior injection can be as effective as implicit parameter updates for chemical reasoning tasks.</div>
<div class="mono" style="margin-top:8px">ChemATP 是一个训练-free 框架，旨在通过将化学知识与推理过程解耦来增强大型语言模型（LLM）的化学推理能力。它构建了一个原子级的文本知识库，使冻结的 LLM 能够动态检索和推理这些信息。实验结果表明，ChemATP 在性能上超过了现有的训练-free 方法，并且与最先进的训练-based 模型相当，突显了显式先验注入的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Minimal Fine-Tuning of VLMs</div>
<div class="meta-line">Authors: Tiange Luo, Lajanugen Logeswaran, Jaekyeom Kim, Justin Johnson, Honglak Lee</div>
<div class="meta-line">First: 2025-12-22T10:02:10+00:00 · Latest: 2025-12-22T10:02:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19219v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>朝向VLMs的极简微调</div>
<div class="mono" style="margin-top:8px">我们引入了Image-LoRA，这是一种轻量级参数高效微调（PEFT）方法，用于基于变换器的视觉语言模型（VLMs）。Image-LoRA 仅对视觉标记跨度内的注意力层的价值路径应用低秩适应，从而将适配器仅训练的FLOPs大致减少为视觉标记比例的倍数。我们进一步只适应了一部分注意力头，这些头是使用秩1 Image-LoRA 估计的头影响得分来选择的，并通过层更新大小归一化来稳定层更新。在跨越文本密集到图像密集领域的屏幕中心定位和引用基准测试中，Image-LoRA 在使用更少可训练参数和更低适配器仅训练FLOPs的情况下，达到了或接近标准LoRA的准确度。该方法还保持了VLMs在微调前后纯文本推理性能，如在GSM8K上进一步所示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research introduces Image-LoRA, a lightweight parameter-efficient fine-tuning method for vision-language models. It applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing training FLOPs. The method also selectively adapts a subset of attention heads and normalizes per-layer updates. Across various benchmarks, Image-LoRA achieves comparable or near-standard LoRA accuracy with fewer parameters and lower FLOPs, while maintaining pure-text reasoning performance before and after fine-tuning.</div>
<div class="mono" style="margin-top:8px">研究引入了Image-LoRA，一种轻量级参数高效微调方法，用于视觉语言模型。该方法仅对视觉标记跨度内的注意力层的价值路径进行低秩适应，减少训练FLOPs。此外，该方法还选择性地适应了一部分注意力头，并对每层更新进行归一化。在各种基准测试中，Image-LoRA在较少参数和更低FLOPs的情况下达到了与标准LoRA相当或相近的准确性，同时保持了微调前后纯文本推理性能。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Language-Policy Model for Dynamic Robot Task Planning</div>
<div class="meta-line">Authors: Jin Wang, Kim Tien Ly, Jacques Cloete, Nikos Tsagarakis, Ioannis Havoutis</div>
<div class="meta-line">First: 2025-12-22T09:12:48+00:00 · Latest: 2025-12-22T09:12:48+00:00</div>
<div class="meta-line">Comments: Manuscript under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19178v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19178v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robovlp.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向动态机器人任务规划的视觉-语言-政策模型</div>
<div class="mono" style="margin-top:8px">在非结构化环境中，将自然语言指令与自主执行之间的差距仍然是机器人技术中的一个开放挑战。这要求机器人通过多种模态感知和推理当前任务场景，并计划其行为以实现其目标。传统的机器人任务规划方法往往难以将低级执行与高级任务推理相结合，并且在执行过程中指令发生变化时无法动态更新任务策略，这最终限制了它们对新任务的多样性和适应性。在本文中，我们提出了一种基于语言模型的动态机器人任务规划新框架。我们的视觉-语言-政策（VLP）模型基于在真实数据上微调的视觉-语言模型，可以解释语义指令并结合当前任务场景的推理来生成控制机器人完成任务的行为策略。此外，它可以根据任务变化动态调整任务策略，实现对不断变化的任务要求的灵活适应。在不同机器人和各种真实世界任务上的实验表明，训练后的模型可以高效地适应新场景并动态更新其策略，展示了强大的规划自主性和跨体征的一般性。视频：https://robovlp.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of bridging natural language commands with autonomous execution in unstructured environments. It introduces a Vision-Language-Policy (VLP) model, which uses a fine-tuned vision-language model to interpret semantic instructions and reason about the current task scene, generating behavior policies for robot task execution. The model can dynamically adjust its strategy during execution, adapting to changes in the task. Experiments show that the VLP model can efficiently adapt to new scenarios and update its policy, demonstrating strong planning autonomy and generalization across different robots and tasks.</div>
<div class="mono" style="margin-top:8px">本文解决了自然语言指令与在非结构化环境中自主执行之间的鸿沟。提出了一种Vision-Language-Policy (VLP) 模型，该模型利用预训练的视觉语言模型来解释语义指令并推理当前任务场景，生成控制机器人完成任务的行为策略。该模型在执行过程中可以动态调整策略，适应任务变化。实验表明，VLP模型能够高效地适应新场景并更新其策略，展示了强大的规划自主性和跨体征的一般化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation</div>
<div class="meta-line">Authors: Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Mian</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-22T06:57:42+00:00 · Latest: 2025-12-22T06:57:42+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026 Workshop on New Frontiers in Information Retrieval</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19088v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19088v1">PDF</a> · <a href="https://github.com/ndkhanh360/BoxOVIS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector&#x27;s ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于盒子引导的开放词汇实例分割从3D场景检索对象</div>
<div class="mono" style="margin-top:8px">从场景级点云中定位和检索对象是一个具有广泛机器人技术和增强现实应用挑战性的问题。该任务通常被表述为开放词汇3D实例分割。尽管最近的方法表现出色，但它们严重依赖SAM和CLIP从伴随点云的图像中生成和分类3D实例掩码，导致巨大的计算开销和缓慢的处理速度，限制了它们在实际环境中的部署。Open-YOLO 3D通过使用实时2D检测器来分类由预训练3D分割器直接从点云生成的类无差别掩码，从而缓解了这一问题，消除了对SAM和CLIP的需求，并显著减少了推理时间。然而，Open-YOLO 3D往往难以泛化到在3D训练数据中出现频率较低的对象类别。在本文中，我们提出了一种方法，该方法通过2D开放词汇检测器从RGB图像中生成新对象的3D实例掩码。我们的方法继承了2D检测器识别新对象的能力，同时保持高效的分类，从而能够从开放文本查询中快速准确地检索稀有实例。我们的代码将在https://github.com/ndkhanh360/BoxOVIS上公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of locating and retrieving objects from 3D scenes, formulated as open-vocabulary 3D instance segmentation. It proposes Open-YOLO 3D, which uses a real-time 2D detector to classify class-agnostic masks from point clouds, reducing computational overhead. However, it struggles with rare object categories. The authors then introduce a method that generates 3D instance masks for novel objects from RGB images, guided by a 2D open-vocabulary detector, enhancing generalization and efficiency for rare instances.</div>
<div class="mono" style="margin-top:8px">论文解决了从3D场景中定位和检索物体的问题，将其形式化为开放词汇的3D实例分割。提出了一种名为Open-YOLO 3D的方法，使用实时2D检测器从点云中分类无类别的掩码，减少了计算开销。然而，它在处理罕见物体类别时表现不佳。作者随后提出了一种方法，通过2D开放词汇检测器从RGB图像中生成新型物体的3D实例掩码，增强了泛化能力和对开放文本查询的高效检索。</div>
</details>
</div>
<div class="card">
<div class="title">Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning</div>
<div class="meta-line">Authors: Yanzhi Zhang, Yitong Duan, Zhaoxi Zhang, Jiyan He, Shuxin Zheng</div>
<div class="meta-line">First: 2025-12-22T06:42:46+00:00 · Latest: 2025-12-22T06:42:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19081v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19081v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Population-Evolve：一种用于LLM数学推理的并行采样与进化方法</div>
<div class="mono" style="margin-top:8px">近年来，测试时缩放已成为提高大型语言模型推理能力的一个有前途的方向。在本文中，我们提出了一种名为Population-Evolve的无需训练的方法，该方法受到遗传算法的启发，用于优化LLM推理。我们的方法通过并行推理为每个问题维护一个动态候选解群体。通过引入进化提示，LLM在所有迭代中自我进化其群体。在收敛后，最终答案通过多数投票得出。此外，我们建立了一个统一框架，通过遗传算法的视角解释现有的测试时缩放策略。实验证明，Population-Evolve在保持低性能波动和计算效率的同时实现了更高的准确性。我们的研究结果突显了进化策略在推理过程中解锁LLM推理能力的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Population-Evolve is a training-free method inspired by Genetic Algorithms designed to enhance the reasoning capabilities of Large Language Models (LLMs). It maintains a dynamic population of candidate solutions for each problem through parallel reasoning and self-evolves the population via an evolve prompt in each iteration. Upon convergence, the final answer is determined by majority voting. Experiments show that Population-Evolve achieves high accuracy with low variance and computational efficiency, indicating the potential of evolutionary strategies in LLM inference.</div>
<div class="mono" style="margin-top:8px">Population-Evolve 是一种受遗传算法启发的无训练方法，旨在增强大型语言模型（LLM）的推理能力。它通过并行推理维护每个问题的动态候选解决方案群体，并使用进化提示自我进化该群体。收敛后，最终答案通过多数投票确定。实验表明，Population-Evolve 达到了高精度，具有低性能波动和计算效率，表明进化策略在推理期间解锁 LLM 的推理能力的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding</div>
<div class="meta-line">Authors: Ruiqi Ma, Yu Yan, Chunhong Zhang, Minghao Yin, XinChao Liu, Zhihong Jin, Zheng Hu</div>
<div class="meta-line">First: 2025-12-22T06:20:53+00:00 · Latest: 2025-12-22T06:20:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19070v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19070v1">PDF</a> · <a href="https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model&#x27;s dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>密切关注：通过解耦解码减轻大型视觉-语言模型中的对象幻觉</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）在视觉和语言模态之间架起桥梁，展现出在多种领域的强大潜力。然而，尽管取得了显著进展，LVLMs 在物体识别任务中仍然遭受严重的幻觉问题。这些模型往往无法准确识别某些物体，导致生成的文本看似流畅但与视觉内容不符，这在实际应用中可能会产生严重后果。最近，提出了一些方法来缓解LVLM的幻觉问题，但大多数方法仅专注于减少语言模态中的幻觉。为了同时减轻语言和视觉模态中的幻觉，我们引入了一种无需训练的幻觉解耦解码（HDD）方法。HDD 通过分割图像并选择增强原始图像的图像来增强原始图像，同时利用空白图像消除原始图像和分割图像中的语言先验幻觉。该设计不仅减少了模型对语言先验的依赖，还增强了其视觉性能。（代码: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding）</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of object hallucinations in large vision-language models (LVLMs) by introducing Hallucination Disentangled Decoding (HDD), a method that enhances the original image through segmentation and selection of augmenting images, while also using a blank image to reduce language prior hallucinations. This approach mitigates hallucinations in both language and visual modalities without requiring additional training, thereby improving the model&#x27;s visual performance and reducing its dependence on language priors.</div>
<div class="mono" style="margin-top:8px">论文通过引入Hallucination Disentangled Decoding (HDD)方法，该方法通过对原始图像进行分割和选择增强图像，并使用空白图像来减少语言先验幻觉，从而在不需额外训练的情况下减轻语言和视觉模态中的幻觉问题，提高模型的视觉性能并减少其对语言先验的依赖。</div>
</details>
</div>
<div class="card">
<div class="title">Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</div>
<div class="meta-line">Authors: Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang</div>
<div class="meta-line">First: 2025-12-22T03:45:04+00:00 · Latest: 2025-12-22T03:45:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19004v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19004v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.
  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于上下文的初始化以减少扩散语言模型生成路径长度</div>
<div class="mono" style="margin-top:8px">扩散大型语言模型（DLLMs）能够实现完全并行的标记解码，但在推理时由于需要许多去噪迭代来将信息空白的完全掩码初始化优化为连贯的文本，因此往往仍不实用。现有的大多数加速方法侧重于通过改进求解器或采样策略更有效地遍历这种生成轨迹。我们提出了一种互补的观点：通过基于上下文的初始化从目标分布更近的地方开始，从而缩短轨迹本身。我们提出了一种无需训练的接口，将轻量级辅助模型的条件提示注入到扩散初始化中，并通过离散标记注入和表示级嵌入插值实现它。由于注入的先验可能不完美，且仅解掩可能过早地做出承诺，我们还引入了一种基于置信度的简单重新掩码机制，作为一种先验怀疑的形式。初步证据表明，基于上下文的初始化可以显著减少去噪迭代（在我们的设置中约减少35%的功能评估次数），同时揭示了一个关键的开放挑战：简单的预热启动可能会相对于强大的扩散基线降低最终的准确性。我们利用这些发现来推动围绕校准、修订机制和表示对齐的研究议程，以实现可靠的预热启动扩散解码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper aims to reduce the computational cost of generating coherent text in diffusion language models by shortening the generative trajectory. It proposes context-aware initialization, which injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization. The study shows that this method can reduce the number of denoising iterations by about 35% and highlights the need for calibration and revision mechanisms to maintain accuracy. However, naive warm-starting can sometimes degrade final accuracy compared to strong diffusion baselines.</div>
<div class="mono" style="margin-top:8px">论文针对扩散大型语言模型（DLLMs）在推理时因需要多次去噪迭代而效率低下问题，提出了一种基于上下文的初始化方法，通过向扩散初始化注入条件先验来缩短生成路径长度。研究显示这种方法可以显著减少去噪迭代次数，大约减少35%，并指出需要校准和修订机制以确保可靠的暖启动扩散解码。</div>
</details>
</div>
<div class="card">
<div class="title">ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling</div>
<div class="meta-line">Authors: Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, Li Erran Li</div>
<div class="meta-line">Venue: ECCV 2024</div>
<div class="meta-line">First: 2024-02-09T01:00:14+00:00 · Latest: 2025-12-22T03:14:10+00:00</div>
<div class="meta-line">Comments: Accepted by ECCV 2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.06118v4">Abs</a> · <a href="https://arxiv.org/pdf/2402.06118v4">PDF</a> · <a href="https://github.com/amazon-science/vigor">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">By combining natural language understanding, generation capabilities, and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented visual reasoning capabilities. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucination of nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes of and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through a variety of evaluation methods and benchmarks. Additionally, we released our human annotation (https://github.com/amazon-science/vigor) comprising 15,440 images and generated text pairs with fine-grained evaluations to contribute to related research in the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViGoR：通过细粒度奖励建模提高大型视觉语言模型的视觉定位能力</div>
<div class="mono" style="margin-top:8px">通过将大型语言模型的自然语言理解、生成能力和广泛知识与图像感知相结合，最近的大型视觉语言模型（LVLMs）展示了前所未有的视觉推理能力。然而，生成的文本往往在视觉输入中缺乏准确的定位，导致诸如虚构不存在的场景元素、遗漏场景的重要部分以及错误推断对象属性和关系等错误。为了解决这些问题，我们提出了一种新的框架，ViGoR（通过细粒度奖励建模进行视觉定位），利用细粒度奖励建模显著提高了LVLMs的视觉定位能力，超越了预训练基线。这种改进是通过使用更便宜的人类评估而不是全面监督以及自动化方法来高效实现的。我们通过多种评估方法和基准展示了我们方法的有效性。此外，我们还发布了包含15,440张图像及其生成文本对的细粒度评估注释（https://github.com/amazon-science/vigor），以促进相关研究领域的贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ViGoR, a framework that uses fine-grained reward modeling to improve the visual grounding of large vision language models (LVLMs). It addresses issues like hallucination and missing parts in the generated text by enhancing the models&#x27; ability to accurately ground in visual inputs. The approach leverages cheaper human evaluations and automated methods, showing effectiveness through various benchmarks and evaluations.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决大型视觉语言模型生成文本中的不准确问题来提高其视觉定位能力。方法是采用一种名为ViGoR的新框架，利用细粒度奖励建模来增强视觉定位。该方法通过多种评估方法和基准展示了相对于预训练基线的显著改进，这主要通过更便宜的人类评估和自动化方法实现。研究还发布了包含15,440张图像和生成文本对的数据集，供进一步研究使用。</div>
</details>
</div>
<div class="card">
<div class="title">ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation</div>
<div class="meta-line">Authors: Gyeongrok Oh, Youngdong Jang, Jonghyun Choi, Suk-Ju Kang, Guang Lin, Sangpil Kim</div>
<div class="meta-line">First: 2025-12-22T03:13:08+00:00 · Latest: 2025-12-22T03:13:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18991v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.18991v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ICP-4D：连接迭代最近点与LiDAR全景分割</div>
<div class="mono" style="margin-top:8px">4D LiDAR全景分割的主要范式通常需要训练大型叠加点云的深度神经网络，或者设计专门的模块进行实例关联。然而，这些方法会进行冗余的点处理，从而变得计算成本高昂，同时仍然忽视了原始点云中固有的丰富几何先验。为此，我们引入了ICP-4D，这是一种简单而有效的无需训练框架，通过实例级点集之间的几何关系统一空间和时间推理。具体而言，我们应用迭代最近点（ICP）算法直接关联时序一致的实例，通过估计的变换对源点集和目标点集进行对齐。为了在噪声实例预测下稳定关联，我们引入了一种基于Sinkhorn的软匹配。这种方法利用潜在的实例分布来获得准确的点对对应关系，从而实现稳健的几何对齐。此外，我们精心设计的管道考虑了三种实例类型——静态、动态和缺失——提供了计算效率和遮挡感知的匹配。我们在SemanticKITTI和panoptic nuScenes上的广泛实验表明，我们的方法在不进行额外训练或额外点云输入的情况下，始终优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ICP-4D is a training-free framework that integrates spatial and temporal reasoning for 4D LiDAR panoptic segmentation using geometric relations among instance-level point sets. It employs the Iterative Closest Point (ICP) algorithm to align point sets and introduces a Sinkhorn-based soft matching to handle noisy instance predictions. The method effectively associates temporally consistent instances and considers three instance types: static, dynamic, and missing, leading to robust geometric alignment. Experiments on SemanticKITTI and panoptic nuScenes show that ICP-4D outperforms state-of-the-art approaches without additional training or extra point cloud inputs.</div>
<div class="mono" style="margin-top:8px">ICP-4D 是一个无需训练的框架，结合了空间和时间推理来实现 4D LiDAR 分段。它使用迭代最近点（ICP）算法对齐点集，并引入基于 Sinkhorn 的软匹配来处理噪声预测。该方法有效地关联了时间上一致的实例，并提供了计算效率和遮挡感知的匹配。在 SemanticKITTI 和 panoptic nuScenes 上的实验表明，ICP-4D 在无需额外训练或额外点云输入的情况下优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</div>
<div class="meta-line">Authors: Ryosuke Korekata, Quanting Xie, Yonatan Bisk, Komei Sugiura</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-12-22T02:55:25+00:00 · Latest: 2025-12-22T02:55:25+00:00</div>
<div class="meta-line">Comments: Accepted to IEEE RA-L, with presentation at ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18987v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.18987v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Affordance RAG：基于操作感知体态记忆的分层多模态检索</div>
<div class="mono" style="margin-top:8px">在本研究中，我们解决了开放词汇的移动操作问题，即机器人需要根据自由形式的自然语言指令携带各种物体到容器中。这一任务具有挑战性，因为它涉及理解视觉语义和操作动作的可行性。为了解决这些挑战，我们提出了Affordance RAG，这是一种零样本分层多模态检索框架，能够从预先探索的图像中构建操作感知体态记忆。该模型基于区域和视觉语义检索候选目标，并使用操作得分重新排序，使机器人能够识别在实际环境中可能执行的操作选项。我们的方法在大型室内环境中的移动操作指令检索性能上优于现有方法。此外，在基于自由形式指令在室内环境中进行移动操作的实地实验中，所提出的方法实现了85%的任务成功率，优于现有方法在检索性能和整体任务成功率上的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of open-vocabulary mobile manipulation by proposing Affordance RAG, a zero-shot hierarchical multimodal retrieval framework. It constructs Affordance-Aware Embodied Memory from pre-explored images to retrieve and rerank candidate targets based on regional and visual semantics, as well as affordance scores. The method outperformed existing approaches in retrieval performance and achieved an 85% task success rate in real-world experiments.</div>
<div class="mono" style="margin-top:8px">本研究通过提出零样本层次多模态检索框架Affordance RAG，解决了开放词汇的移动操作问题。该方法从预先探索的图像中构建感知操作的体记忆，基于区域和视觉语义以及操作分数检索和重新排名候选目标。该方法在检索性能上优于现有方法，并在真实世界实验中实现了85%的任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</div>
<div class="meta-line">Authors: Yue Li, Qi Ma, Runyi Yang, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Theo Gevers, Luc Van Gool, Danda Pani Paudel, Martin R. Oswald</div>
<div class="meta-line">First: 2025-12-19T17:22:35+00:00 · Latest: 2025-12-22T02:33:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17817v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17817v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.
  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians&#x27; centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>合唱：多教师预训练以实现全面的3D高斯场景编码</div>
<div class="mono" style="margin-top:8px">虽然3DGS已经作为一种高保真场景表示出现，但直接从其基本元素中编码丰富的通用特征仍然未被充分探索。我们通过引入合唱，一种多教师预训练框架，通过从2D基础模型中提取互补信号来学习一个全面的前馈3D高斯点积（3DGS）场景编码器，来解决这一问题。合唱使用共享的3D编码器和教师特定的投影器，从语言对齐、通用和对象感知的教师中学习，鼓励一个共享的嵌入空间，该空间捕捉从高层语义到细粒度结构的信号。
我们评估合唱在一系列任务上的表现：开放词汇语义和实例分割、线性和解码器探针，以及数据高效监督。除了3DGS，我们还测试了合唱在仅支持点云的几个基准上的表现，通过预训练一个仅使用高斯中心、颜色、估计法线作为输入的变体。有趣的是，这个编码器表现出强大的迁移性能，并在使用39.9倍少的训练场景时优于点云基线。最后，我们提出了一种渲染和提取适应方法，以促进域外微调。我们的代码和模型将在发表后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Chorus is a multi-teacher pretraining framework that addresses the under-explored area of encoding rich, general-purpose features directly from 3D Gaussian Splatting (3DGS) primitives. It uses a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, capturing signals from high-level semantics to fine-grained structure. Chorus is evaluated on various tasks including semantic and instance segmentation, linear and decoder probing, and data-efficient supervision, showing strong transfer and outperforming point clouds baselines with fewer training scenes.</div>
<div class="mono" style="margin-top:8px">Chorus 是一种多教师预训练框架，旨在解决从 3D 贝塞尔点积原语中直接编码丰富的一般特征这一未被充分探索的问题。它使用共享的 3D 编码器和教师特定的投影器，从语言对齐、通用和对象感知的教师中学习，以捕捉从高层语义到细粒度结构的各种信号。Chorus 在语义和实例分割、线性探针和解码器探针等多种任务上进行了评估，显示出强大的迁移性能，并在使用更少的训练场景时优于点云基线。</div>
</details>
</div>
<div class="card">
<div class="title">DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation</div>
<div class="meta-line">Authors: Guandong Li, Yijun Ding</div>
<div class="meta-line">First: 2025-12-22T02:25:05+00:00 · Latest: 2025-12-22T02:25:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18964v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.18964v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent tuning-free identity customization methods achieve high facial fidelity but often overlook visual context, such as lighting, skin texture, and environmental tone. This limitation leads to ``Semantic-Visual Dissonance,&#x27;&#x27; where accurate facial geometry clashes with the input&#x27;s unique atmosphere, causing an unnatural ``sticker-like&#x27;&#x27; effect. We propose **DVI (Disentangled Visual-Identity)**, a zero-shot framework that orthogonally disentangles identity into fine-grained semantic and coarse-grained visual streams. Unlike methods relying solely on semantic vectors, DVI exploits the inherent statistical properties of the VAE latent space, utilizing mean and variance as lightweight descriptors for global visual atmosphere. We introduce a **Parameter-Free Feature Modulation** mechanism that adaptively modulates semantic embeddings with these visual statistics, effectively injecting the reference&#x27;s ``visual soul&#x27;&#x27; without training. Furthermore, a **Dynamic Temporal Granularity Scheduler** aligns with the diffusion process, prioritizing visual atmosphere in early denoising stages while refining semantic details later. Extensive experiments demonstrate that DVI significantly enhances visual consistency and atmospheric fidelity without parameter fine-tuning, maintaining robust identity preservation and outperforming state-of-the-art methods in IBench evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DVI：分离语义和视觉身份以实现无需训练的个性化生成</div>
<div class="mono" style="margin-top:8px">近期无需调优的身份定制方法在面部保真度方面取得了高水准，但往往忽视了视觉上下文，如照明、皮肤纹理和环境色调。这一限制导致了“语义-视觉不和谐”的问题，即准确的面部几何结构与输入的独特氛围产生冲突，造成一种不自然的“贴纸效果”。我们提出**DVI（分离视觉-身份）**，这是一种零样本框架，正交地将身份分离为细粒度语义流和粗粒度视觉流。与仅依赖语义向量的方法不同，DVI 利用 VAE 潜空间的固有统计特性，使用均值和方差作为轻量级的全局视觉氛围描述符。我们引入了一种**无参数特征调制**机制，该机制自适应地用这些视觉统计信息调制语义嵌入，有效地注入参考的“视觉灵魂”而无需训练。此外，一种**动态时间粒度调度器**与扩散过程对齐，在早期去噪阶段优先考虑视觉氛围，而在后期细化语义细节。大量实验表明，DVI 显著提高了视觉一致性和氛围保真度，无需参数微调，同时保持了鲁棒的身份保留，并在 IBench 评估中优于现有最佳方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the issue of &#x27;Semantic-Visual Dissonance&#x27; in identity customization methods, which often result in unnatural facial renderings. DVI proposes a zero-shot framework that disentangles identity into semantic and visual components, using the VAE latent space to describe the visual atmosphere. It introduces a parameter-free feature modulation mechanism and a dynamic temporal granularity scheduler to enhance visual consistency and atmospheric fidelity. Experimental results show that DVI outperforms existing methods in maintaining robust identity preservation and visual realism.</div>
<div class="mono" style="margin-top:8px">研究旨在解决无调参身份定制方法中存在的‘语义-视觉不和谐’问题，这类方法往往忽视了光照、皮肤纹理等视觉上下文。DVI（解耦视觉-身份）提出了一种零样本框架，将身份解耦为语义和视觉流。它利用VAE潜在空间的均值和方差来描述视觉氛围，并引入无参数特征调制机制，以适应性地注入参考的视觉特征。此外，动态时间粒度调度器与扩散过程对齐，早期优先处理视觉氛围，后期细化语义细节。实验表明，DVI在增强视觉一致性和氛围保真度方面表现出色，无需参数微调，并优于现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning</div>
<div class="meta-line">Authors: Chuang Jiang, Mingyue Cheng, Xiaoyu Tao, Qingyang Mao, Jie Ouyang, Qi Liu</div>
<div class="meta-line">Venue: WSDM 2026</div>
<div class="meta-line">First: 2025-09-08T02:00:31+00:00 · Latest: 2025-12-22T01:55:37+00:00</div>
<div class="meta-line">Comments: Comments: 10 pages, 6 figures. Submitted to WSDM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.06278v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.06278v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Table reasoning requires models to jointly perform comprehensive semantic understanding and precise numerical operations. Although recent large language model (LLM)-based methods have achieved promising results, most of them still rely on a single-turn reasoning paradigm that processes flattened tables in a single forward pass. This paradigm suffers from inherent limitations, including context overflow on large tables, weak sensitivity to continuous numerical values, and the absence of explicit tool-use and reflection. In this paper, we propose TableMind, a tuning-based autonomous programmatic table agent that simulates the human-like cognitive schema of the multi-turn interaction within a lightweight LLM. Instead of adopting a training-free workflow design, TableMind learns to internalize planning, action, and reflection through a principled two-stage training strategy. To bootstrap structured table reasoning capabilities, we construct and filter high-quality reasoning data for the supervised fine-tuning (SFT) stage. To enable precise code generation, we introduce a designed multi-perspective reward scheme and a novel optimization objective in the reinforcement learning (RL) stage. Extensive experiments on diverse benchmarks demonstrate that TableMind consistently outperforms previous baselines, validating the effectiveness of training autonomous agents to improve overall performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TableMind：一种自主程序代理，用于工具增强的表格推理</div>
<div class="mono" style="margin-top:8px">表格推理需要模型同时进行全面的语义理解和精确的数值操作。尽管最近基于大型语言模型（LLM）的方法取得了令人鼓舞的结果，但大多数方法仍然依赖于单一回合的推理范式，即在单次前向传递中处理扁平化的表格。这种范式存在固有的局限性，包括在大表格上出现上下文溢出、对连续数值的敏感性较弱以及缺乏明确的工具使用和反思。在本文中，我们提出了一种基于调优的自主程序化表格代理TableMind，该代理模拟了轻量级LLM中多回合交互的人类认知模式。与无训练工作流设计不同，TableMind 通过一个原则性的两阶段训练策略学习内化规划、行动和反思。为了启动结构化表格推理能力，我们构建并筛选高质量的推理数据用于监督微调（SFT）阶段。为了实现精确的代码生成，我们在强化学习（RL）阶段引入了一种设计的多视角奖励方案和一种新的优化目标。在多种基准上的广泛实验表明，TableMind 一致地优于先前的基线，验证了训练自主代理以提高整体性能的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">TableMind is designed to enhance table reasoning by integrating semantic understanding and numerical operations. It uses a two-stage training strategy to learn planning, action, and reflection, and introduces a reward scheme for precise code generation. Experiments show that TableMind outperforms previous methods across various benchmarks, demonstrating its effectiveness in improving overall performance through autonomous agent training.</div>
<div class="mono" style="margin-top:8px">TableMind 是一个自主程序化代理，旨在解决表格推理中的单轮推理范式限制。它采用两阶段训练策略学习规划、行动和反思，并引入多视角奖励方案和新颖的优化目标以实现精确的代码生成。实验表明，TableMind 在各种基准测试中均优于先前的方法，验证了其在提高整体性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">The Ensemble Schr{ö}dinger Bridge filter for Nonlinear Data Assimilation</div>
<div class="meta-line">Authors: Feng Bao, Hui Sun</div>
<div class="meta-line">First: 2025-12-22T00:06:49+00:00 · Latest: 2025-12-22T00:06:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18928v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.18928v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work puts forward a novel nonlinear optimal filter namely the Ensemble Schr{ö}dinger Bridge nonlinear filter. The proposed filter finds marriage of the standard prediction procedure and the diffusion generative modeling for the analysis procedure to realize one filtering step. The designed approach finds no structural model error, and it is derivative free, training free and highly parallizable. Experimental results show that the designed algorithm performs well given highly nonlinear dynamics in (mildly) high dimension up to 40 or above under a chaotic environment. It also shows better performance than classical methods such as the ensemble Kalman filter and the Particle filter in numerous tests given different level of nonlinearity. Future work will focus on extending the proposed approach to practical meteorological applications and establishing a rigorous convergence analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Schr{ö}dinger 桥集成滤波器在非线性数据同化中的应用</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的非线性最优滤波器，即集成 Schr{ö}dinger 桥非线性滤波器。所提出的滤波器将标准预测过程与扩散生成建模相结合，以实现一次滤波步骤。设计的方法没有结构模型误差，且无需求导、无需训练，高度并行化。实验结果表明，在混沌环境中，该设计算法在（轻微）高维（40或以上）且高度非线性动力学条件下表现良好。此外，在不同非线性程度下，与经典方法（如集成卡尔曼滤波器和粒子滤波器）相比，该算法在多次测试中表现出更好的性能。未来的工作将致力于将所提出的方法扩展到实际气象应用，并建立严格的收敛分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work introduces the Ensemble Schrödinger Bridge nonlinear filter, which combines standard prediction with diffusion generative modeling for analysis. The filter is derivative-free, training-free, and highly parallelizable, showing superior performance in highly nonlinear dynamics up to 40 dimensions under chaotic conditions, outperforming traditional methods like the ensemble Kalman filter and Particle filter in various tests. Future work aims to apply this approach to meteorological applications and conduct rigorous convergence analysis.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新的非线性滤波器——Ensemble Schrödinger Bridge非线性滤波器，该滤波器将标准预测与扩散生成建模相结合用于分析。该滤波器无导数要求、无需训练且高度并行化，在高维（高达40维）非线性动力学条件下表现出色，并在不同非线性程度的多种测试中优于传统方法如集合卡尔曼滤波器和粒子滤波器。</div>
</details>
</div>
<div class="card">
<div class="title">Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Mohamad Zamini, Diksha Shukla</div>
<div class="meta-line">First: 2025-12-21T23:02:56+00:00 · Latest: 2025-12-21T23:02:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18910v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.18910v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) combine visual and textual representations to enable rich reasoning capabilities. However, the high computational cost of processing dense visual tokens remains a major bottleneck. A critical component in this pipeline is the visual projector, which bridges the vision encoder and the language model. Standard designs often employ a simple multi-layer perceptron for direct token mapping, but this approach scales poorly with high-resolution inputs, introducing significant redundancy. We present Delta-LLaVA, a token-efficient projector that employs a low-rank DeltaProjection to align multi-level vision features into a compact subspace before further interaction. On top of this base alignment, lightweight Transformer blocks act as specialization layers, capturing both global and local structure under constrained token budgets. Extensive experiments and ablations demonstrate that this base-then-specialize design yields consistent gains across multiple benchmarks with only 144 tokens, highlighting the importance of token formation prior to scaling interaction capacity. With Delta-LLaVA, inference throughput improves by up to 55%, while end-to-end training accelerates by nearly 4-5x in pretraining and over 1.5x in finetuning, highlighting the dual benefits of our design in both efficiency and scalability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Delta-LLaVA：基于先基化后专业化对齐的高效视觉语言模型</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）结合视觉和文本表示，以实现丰富的推理能力。然而，处理密集视觉标记的高计算成本仍然是一个主要瓶颈。此管道中的关键组件是视觉投影器，它连接视觉编码器和语言模型。标准设计通常采用简单的多层感知机进行直接标记映射，但这种方法在高分辨率输入下扩展性差，引入了大量冗余。我们提出了Delta-LLaVA，这是一种高效的投影器，采用低秩DeltaProjection将多级视觉特征对齐到一个紧凑的子空间中，然后再进一步交互。在此基础对齐之上，轻量级的Transformer块作为专业化层，能够在受限的标记预算下捕捉全局和局部结构。广泛的实验和消融研究表明，这种先基化后专业化的设计在多个基准测试中仅使用144个标记就能获得一致的收益，突显了在扩展交互能力之前标记形成的重要性。通过Delta-LLaVA，推理吞吐量提高了55%，而端到端训练在预训练中加速了近4-5倍，在微调中加速了超过1.5倍，突显了我们设计在效率和可扩展性方面的双重优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Delta-LLaVA is designed to address the computational challenges of processing dense visual tokens in multimodal large language models. It introduces a low-rank DeltaProjection to align multi-level vision features into a compact subspace, followed by lightweight Transformer blocks for specialization. This base-then-specialize approach achieves consistent gains across multiple benchmarks with only 144 tokens, improving inference throughput by up to 55% and accelerating end-to-end training by nearly 4-5x in pretraining and over 1.5x in finetuning.</div>
<div class="mono" style="margin-top:8px">Delta-LLaVA旨在解决处理密集视觉标记在多模态大型语言模型中的计算挑战。它引入了一种低秩DeltaProjection将多级视觉特征对齐到一个紧凑的子空间，随后是轻量级的Transformer块进行专业化。实验表明，这种基底然后专业化的方法可以将推理吞吐量提高高达55%，并加速端到端训练，预训练加速近4-5倍，微调加速超过1.5倍，同时仅使用144个标记即可保持在多个基准上的持续收益。</div>
</details>
</div>
<div class="card">
<div class="title">AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</div>
<div class="meta-line">Authors: Aashray Reddy, Andrew Zagula, Nicholas Saban</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-04T08:56:28+00:00 · Latest: 2025-12-21T22:30:20+00:00</div>
<div class="meta-line">Comments: Presented at NeurIPS 2025 Lock-LLM Workshop. Code is available at https://github.com/AAN-AutoAdv/AutoAdv</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02376v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.02376v3">PDF</a> · <a href="https://github.com/AAN-AutoAdv/AutoAdv">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs. Yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves an attack success rate of up to 95% on Llama-3.1-8B within six turns, a 24% improvement over single-turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests and then iteratively refines them. Extensive evaluation across commercial and open-source models (Llama-3.1-8B, GPT-4o mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoAdv：自动化对抗提示以实现大型语言模型的多轮脱笼攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）仍然容易受到脱笼攻击的影响，其中对抗性提示会引发有害输出。然而，大多数评估集中在单轮交互上，而实际攻击则通过适应性的多轮对话展开。我们提出了AutoAdv，这是一种无需训练的框架，用于自动化多轮脱笼攻击，在六轮内对Llama-3.1-8B的攻击成功率高达95%，比单轮基线提高了24%。AutoAdv独特地结合了三种适应性机制：一个模式管理器，从成功的攻击中学习以增强未来的提示；一个温度管理器，根据失败模式动态调整采样参数；以及一个两阶段重写策略，先隐藏有害请求，然后逐步优化它们。广泛的评估表明，当前的安全机制存在持续的漏洞，多轮攻击始终优于单轮方法。这些发现表明，针对单轮交互优化的对齐策略无法在长时间对话中保持鲁棒性，突显了对多轮意识防御的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AutoAdv is a training-free framework for automated multi-turn jailbreaking of large language models, achieving up to 95% attack success rate within six turns on Llama-3.1-8B, surpassing single-turn baselines by 24%. It combines a pattern manager, a temperature manager, and a two-phase rewriting strategy to enhance prompt effectiveness and adapt to failure modes. Extensive evaluations across various models show that multi-turn attacks are more effective than single-turn ones, indicating the need for multi-turn-aware defenses.</div>
<div class="mono" style="margin-top:8px">AutoAdv 是一个无需训练的框架，用于自动化多轮大语言模型的破解攻击，能够在六轮内使 Llama-3.1-8B 的攻击成功率高达 95%，比单轮基线高出 24%。该框架结合了模式管理器、温度管理器和两阶段重写策略，以动态调整提示并提高多轮交互中的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs</div>
<div class="meta-line">Authors: Dmitry Demidov, Zaigham Zaheer, Zongyan Han, Omkar Thawakar, Rao Anwer</div>
<div class="meta-line">First: 2025-12-21T22:01:29+00:00 · Latest: 2025-12-21T22:01:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18897v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.18897v1">PDF</a> · <a href="http://github.com/demidovd98/FiNDR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vocabulary-free fine-grained image recognition aims to distinguish visually similar categories within a meta-class without a fixed, human-defined label set. Existing solutions for this problem are limited by either the usage of a large and rigid list of vocabularies or by the dependency on complex pipelines with fragile heuristics where errors propagate across stages. Meanwhile, the ability of recent large multi-modal models (LMMs) equipped with explicit or implicit reasoning to comprehend visual-language data, decompose problems, retrieve latent knowledge, and self-correct suggests a more principled and effective alternative. Building on these capabilities, we propose FiNDR (Fine-grained Name Discovery via Reasoning), the first reasoning-augmented LMM-based framework for vocabulary-free fine-grained recognition. The system operates in three automated steps: (i) a reasoning-enabled LMM generates descriptive candidate labels for each image; (ii) a vision-language model filters and ranks these candidates to form a coherent class set; and (iii) the verified names instantiate a lightweight multi-modal classifier used at inference time. Extensive experiments on popular fine-grained classification benchmarks demonstrate state-of-the-art performance under the vocabulary-free setting, with a significant relative margin of up to 18.8% over previous approaches. Remarkably, the proposed method surpasses zero-shot baselines that exploit pre-defined ground-truth names, challenging the assumption that human-curated vocabularies define an upper bound. Additionally, we show that carefully curated prompts enable open-source LMMs to match proprietary counterparts. These findings establish reasoning-augmented LMMs as an effective foundation for scalable, fully automated, open-world fine-grained visual recognition. The source code is available on github.com/demidovd98/FiNDR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越标签的思考：基于推理增强LMM的无词汇细粒度识别</div>
<div class="mono" style="margin-top:8px">无词汇细粒度图像识别旨在区分元类内的视觉相似类别，而无需固定的人类定义标签集。现有解决方案要么依赖于庞大且僵化的词汇列表，要么依赖于复杂且易出错的管道，其中错误会在各个阶段传播。与此同时，近期大型多模态模型（LMMs）具备显式或隐式推理能力，能够理解视觉-语言数据、分解问题、检索潜在知识并自我纠正，这表明了一种更为原则性和有效的方法。基于这些能力，我们提出了FiNDR（基于推理的细粒度名称发现），这是首个基于推理增强LMM的无词汇细粒度识别框架。系统分为三个自动化步骤：(i) 一个推理增强的LMM为每张图像生成描述性候选标签；(ii) 视觉-语言模型筛选并排序这些候选标签，形成一个连贯的类别集；(iii) 验证后的名称实例化一个轻量级多模态分类器用于推理阶段。在流行的细粒度分类基准上的广泛实验表明，在无词汇设置下，该方法达到了最先进的性能，相对于先前方法有显著的相对优势，最高可达18.8%。此外，该方法超越了利用预定义真实名称的零样本基线，挑战了人类策划词汇定义上限的假设。另外，我们展示了精心策划的提示使开源LMM能够匹配专有版本。这些发现确立了推理增强LMM作为可扩展、全自动、开放世界细粒度视觉识别的有效基础。源代码可在github.com/demidovd98/FiNDR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve fine-grained image recognition without relying on predefined labels. It introduces FiNDR, a reasoning-augmented large multi-modal model framework that automatically generates descriptive labels, filters them, and uses them to classify images. Experiments show that FiNDR outperforms previous methods by up to 18.8% and even surpasses zero-shot baselines that use predefined names, suggesting that reasoning-augmented models can achieve better performance without human-curated vocabularies. The method also demonstrates that open-source models can match proprietary ones with carefully curated prompts.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使用推理增强的大规模多模态模型来改进无标签的细粒度图像识别。方法名为FiNDR，包括三个步骤：生成描述性标签、过滤和排序候选标签以及使用验证后的名称进行分类。实验表明，FiNDR在无标签设置下的性能比之前的方法高出高达18.8%，并且超越了零样本基线，表明推理增强的LMM可以有效地处理开放世界细粒度识别，而无需预定义的标签。这些发现表明，这样的模型可以成为可扩展的、全自动的开放世界细粒度视觉识别的有效基础。</div>
</details>
</div>
<div class="card">
<div class="title">Independent Density Estimation</div>
<div class="meta-line">Authors: Jiahao Liu, Senhao Cao</div>
<div class="meta-line">First: 2025-12-10T20:43:03+00:00 · Latest: 2025-12-21T21:19:52+00:00</div>
<div class="meta-line">Comments: 10 pages, 1 table, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10067v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10067v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>独立密度估计</div>
<div class="mono" style="margin-top:8px">大规模的视觉-语言模型在图像字幕和条件图像生成等领域取得了显著成果。然而，这些模型在实现类人的组合泛化方面仍然面临困难。在本研究中，我们提出了一种新的方法，称为独立密度估计（IDE），以应对这一挑战。IDE旨在学习句子中单个词与图像中相应特征之间的联系，从而实现组合泛化。我们基于IDE的哲学构建了两个模型。第一个模型使用完全解耦的视觉表示作为输入，第二个模型利用变分自编码器从原始图像中获得部分解耦的特征。此外，我们提出了一种基于熵的组合推理方法，用于结合句子中每个词的预测。当在各种数据集上进行评估时，我们的模型在未见过的组合泛化方面优于当前模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study proposes Independent Density Estimation (IDE) to improve compositional generalization in vision-language models. IDE learns the relationship between words and image features, and two models are built: one using fully disentangled visual representations and the other using partially disentangled features from raw images via a Variational Auto-Encoder. An entropy-based compositional inference method is also introduced. The models show better generalization to unseen compositions compared to existing models on various datasets.</div>
<div class="mono" style="margin-top:8px">研究提出了独立密度估计（IDE）方法以提高视觉语言模型的组合泛化能力。IDE学习单词与图像特征之间的关系，并开发了两种模型：一种使用完全解耦的视觉表示，另一种使用从原始图像中获得的部分解耦特征。还提出了一种基于熵的组合推理方法来结合句子中每个词的预测。这些模型在各种数据集上的表现优于现有模型，能够更好地泛化到未见过的组合中。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251223_0320.html">20251223_0320</a>
<a href="archive/20251222_0318.html">20251222_0318</a>
<a href="archive/20251221_0318.html">20251221_0318</a>
<a href="archive/20251220_0319.html">20251220_0319</a>
<a href="archive/20251219_0322.html">20251219_0322</a>
<a href="archive/20251218_0333.html">20251218_0333</a>
<a href="archive/20251217_0321.html">20251217_0321</a>
<a href="archive/20251216_0326.html">20251216_0326</a>
<a href="archive/20251215_0321.html">20251215_0321</a>
<a href="archive/20251214_0316.html">20251214_0316</a>
<a href="archive/20251213_0319.html">20251213_0319</a>
<a href="archive/20251212_0322.html">20251212_0322</a>
<a href="archive/20251211_0319.html">20251211_0319</a>
<a href="archive/20251210_0317.html">20251210_0317</a>
<a href="archive/20251209_0321.html">20251209_0321</a>
<a href="archive/20251208_0317.html">20251208_0317</a>
<a href="archive/20251207_0317.html">20251207_0317</a>
<a href="archive/20251206_0318.html">20251206_0318</a>
<a href="archive/20251205_0320.html">20251205_0320</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
