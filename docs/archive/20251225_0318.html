<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-25 03:18</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251225_0318</div>
    <div class="row"><div class="card">
<div class="title">FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</div>
<div class="meta-line">Authors: Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang</div>
<div class="meta-line">First: 2025-12-23T18:05:43+00:00 · Latest: 2025-12-23T18:05:43+00:00</div>
<div class="meta-line">Comments: Under submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20561v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20561v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlashVLM：文本引导的视觉标记选择框架用于大型多模态模型</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）通常每张图像或视频帧处理数百或数千个视觉标记，导致二次注意力成本和大量冗余。现有的标记减少方法往往忽视了文本查询或依赖于深度注意力图，这些图在激进剪枝下的不稳定性导致语义对齐下降。
我们提出了一种FlashVLM，这是一种文本引导的视觉标记选择框架，能够动态适应查询。FlashVLM 不依赖于嘈杂的注意力权重，而是计算投影图像标记与语言模型空间中归一化文本嵌入之间的显式跨模态相似性。这种外在的相关性与内在的视觉显著性通过对数域加权和温度控制锐化进行融合。此外，一种保留多样性的划分保留了少量但具有代表性的背景标记，以保持全局上下文。
在相同的标记预算和评估协议下，FlashVLM 实现了超越无损压缩的效果，即使在对LLaVA 1.5剪枝高达77.8%的情况下，仍略优于未剪枝的基线，同时保持92.8%的准确率，即使在94.4%的压缩下也是如此。在14个图像和视频基准上的广泛实验表明，FlashVLM 在保持强大鲁棒性和泛化能力的同时，提供了最先进的效率性能折衷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FlashVLM is a text-guided visual token selection framework that reduces the number of visual tokens processed by large vision-language models while maintaining performance. It computes explicit cross-modal similarity between image tokens and text embeddings, fuses this with visual saliency, and retains a diverse set of background tokens. FlashVLM achieves up to 77.8% visual token pruning while maintaining 92.8% accuracy on LLaVA 1.5, and outperforms the unpruned baseline under various compression levels. Extensive experiments on 14 benchmarks show FlashVLM&#x27;s strong efficiency and robustness across different VLMs.</div>
<div class="mono" style="margin-top:8px">FlashVLM 是一种文本引导的视觉标记选择框架，能够动态适应文本查询。它通过计算图像标记与文本嵌入的显式跨模态相似性，并结合视觉显著性，保留少量背景标记以维持上下文。FlashVLM 在 LLava 1.5 上实现了高达 77.8% 的视觉标记剪枝，同时保持 92.8% 的准确性，并在各种压缩水平下优于未剪枝基线。广泛的实验表明，FlashVLM 在 14 个图像和视频基准上提供了强大的效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</div>
<div class="meta-line">Authors: Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</div>
<div class="meta-line">First: 2025-12-23T17:56:36+00:00 · Latest: 2025-12-23T17:56:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20557v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20557v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在四维中学习推理：视觉语言模型的动态空间理解</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）在一般理解方面表现出色，但在动态空间推理（DSR），即在三维空间中随时间推移对物体几何形状和关系的推理方面仍然较弱，这主要是由于缺乏可扩展的四维感知训练资源。为了在数据集、基准和模型的各个方面弥合这一差距，我们引入了DSR套件。首先，我们提出了一种自动化管道，从野外视频中生成DSR的多项选择题-答案对。通过利用现代视觉基础模型，该管道提取了丰富的几何和运动信息，包括相机姿态、局部点云、物体掩码、方向和三维轨迹。这些几何线索使DSR-Train得以构建，进一步通过人工精炼构建DSR-Bench用于评估。与以往工作相比，我们的数据强调（i）野外视频来源，（ii）物体和场景级别的三维要求，（iii）视角变换，（iv）多物体交互，以及（v）细粒度、程序化的答案。除了数据，我们还提出了一种轻量级的几何选择模块（GSM），以无缝地将几何先验整合到VLM中，该模块浓缩了问题语义，并从预训练的四维重建先验中提取与问题相关的信息，形成一组紧凑的几何标记。这种有针对性的提取避免了向模型灌输无关知识。实验表明，将DSR-Train和GSM集成到Qwen2.5-VL-7B中显著增强了其动态空间推理能力，同时在通用视频理解基准测试中保持了准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve vision-language models&#x27; ability in dynamic spatial reasoning (DSR) by addressing the scarcity of 4D-aware training resources. It introduces DSR Suite, which includes an automated pipeline for generating DSR question-answer pairs from in-the-wild videos and a Geometry Selection Module (GSM) to integrate geometric priors into VLMs. The key findings show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability while maintaining performance on general video understanding benchmarks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决4D感知训练资源稀缺问题来提升视觉语言模型的动态空间推理能力。提出了DSR Suite，包括从野生视频自动生成DSR问题-答案对的自动化管道和轻量级的几何选择模块（GSM），以整合几何先验。DSR-Train数据集和DSR-Bench评估集强调野生来源、3D物体和场景要求、视角变换、多物体交互和细粒度答案。实验表明，将DSR-Train和GSM集成到Qwen2.5-VL-7B中可以显著增强其动态空间推理能力，同时保持一般视频理解的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</div>
<div class="meta-line">Authors: Mingwei Tang, Jiahao Nie, Guang Yang, Ziqing Cui, Jie Li</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-12-23T17:55:35+00:00 · Latest: 2025-12-23T17:55:35+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20556v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20556v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多粒度文本引导图像融合以应对多曝光和多焦距场景</div>
<div class="mono" style="margin-top:8px">图像融合旨在从在具有挑战性条件下拍摄的一对输入中合成一张高质量的图像，例如不同的曝光水平或焦距深度。核心挑战在于有效处理输入之间的动态范围和焦距深度差异。随着视觉语言模型的出现，最近的方法开始将文本描述作为辅助指导以提高融合质量。然而，简单地引入粗粒度描述会阻碍对细粒度细节的理解，并且对跨模态对齐提出了挑战。为了解决这些限制，我们提出了多粒度文本引导图像融合（MTIF），这是一种具有三个关键设计的新型融合范式。首先，它引入了多粒度的文本描述，分别捕捉细粒度细节、结构线索和语义内容，通过分层跨模态调制模块指导图像融合。其次，它在每个粒度级别引入监督信号，以促进视觉和文本特征之间的对齐并增强辅助文本的实用性。第三，它采用了一种基于显著性的增强模块，通过密集的语义内容增强训练数据，进一步加强跨模态调制和对齐。广泛的实验表明，MTIF在多曝光和多焦距图像融合任务中始终优于先前的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of image fusion under challenging conditions, such as varying exposure levels and focus depths, by proposing Multi-grained Text-guided Image Fusion (MTIF). MTIF introduces multi-grained textual descriptions to capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. The method also includes supervision signals at each granularity to enhance alignment between visual and textual features and a saliency-driven enrichment module to augment training data. Experimental results demonstrate that MTIF outperforms previous methods in both multi-exposure and multi-focus image fusion tasks.</div>
<div class="mono" style="margin-top:8px">论文提出了一种多粒度文本引导图像融合方法（MTIF），以解决在不同曝光和焦距条件下的图像融合问题。MTIF利用层次化的文本描述来引导融合过程，包括细粒度细节、结构线索和语义内容。此外，它还使用粒度特定的监督信号和显著性驱动增强模块来增强跨模态对齐。实验结果表明，MTIF在多曝光和多焦点场景中均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Video Generation Models Are Good Latent Reward Models</div>
<div class="meta-line">Authors: Xiaoyue Mi, Wenqing Yu, Jiesong Lian, Shibo Jie, Ruizhe Zhong, Zijun Liu, Guozhen Zhang, Zixiang Zhou, Zhiyong Xu, Yuan Zhou, Qinglin Lu, Fan Tang</div>
<div class="meta-line">First: 2025-11-26T16:14:18+00:00 · Latest: 2025-12-23T15:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21541v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21541v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成模型是良好的潜在空间奖励模型</div>
<div class="mono" style="margin-top:8px">奖励反馈学习（ReFL）已被证明对于使图像生成与人类偏好对齐是有效的。然而，将其扩展到视频生成面临着重大挑战。现有的视频奖励模型依赖于设计用于像素空间输入的视觉-语言模型，这将ReFL优化限制在昂贵的VAE解码之后的近完全去噪步骤中。这种像素空间的方法带来了巨大的内存开销和增加的训练时间，并且其后期优化缺乏早期监督，仅优化视觉质量而不是基本的运动动态和结构一致性。在本文中，我们展示了预训练的视频生成模型自然适合在噪声潜在空间中进行奖励建模，因为它们明确设计为可以处理任意时间步的噪声潜在表示，并通过其序列建模能力内在地保留时间信息。因此，我们提出了过程奖励反馈学习（PRFL）框架，该框架在潜在空间中完全进行偏好优化，使得可以在整个去噪链中高效地进行梯度反向传播而无需VAE解码。广泛的实验表明，PRFL显著提高了与人类偏好的对齐，同时与RGB ReFL相比实现了内存消耗和训练时间的大幅减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of applying reward feedback learning (ReFL) to video generation by proposing Process Reward Feedback Learning (PRFL). PRFL utilizes pre-trained video generation models to optimize preferences in the noisy latent space, avoiding the need for computationally expensive VAE decoding. This approach leads to better alignment with human preferences and reduces memory consumption and training time compared to traditional pixel-space ReFL methods.</div>
<div class="mono" style="margin-top:8px">本文提出了一种Process Reward Feedback Learning (PRFL)框架，利用预训练的视频生成模型在噪声的潜在空间中优化偏好，避免了昂贵的VAE解码过程。这种方法减少了内存开销和训练时间，同时提高了与人类偏好的一致性。关键发现包括显著提高偏好一致性以及内存消耗和训练时间的大幅减少，相比传统的RGB ReFL方法。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Laws for Energy Efficiency of Local LLMs</div>
<div class="meta-line">Authors: Ander Alvarez, Alessandro Genuardi, Nilotpal Sinha, Antonio Tiene, Mikail Okyay, Bakbergen Ryskulov, David Montero, Samuel Mugel, Román Orús</div>
<div class="meta-line">First: 2025-12-18T13:40:33+00:00 · Latest: 2025-12-23T15:02:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16531v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.16531v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven &quot;resolution knee&quot;, where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>局部大语言模型的能量效率扩展定律</div>
<div class="mono" style="margin-top:8px">在边缘设备上部署局部的大语言模型和视觉-语言模型需要在准确性与受限的计算和能源预算之间进行权衡。尽管图形处理器主导了现代人工智能的部署，但大多数消费级硬件（包括笔记本电脑、台式机、工业控制器和嵌入式系统）仍依赖于中央处理器。尽管如此，仅中央处理器的推理计算法则对局部语言和视觉-语言工作负载的研究仍然相对较少。我们系统地在两个广泛用于局部推理的中央处理器层级上对大型语言和视觉-语言模型进行了基准测试：一台搭载M2芯片的MacBook Pro，代表主流笔记本电脑级别的部署；以及一个Raspberry Pi 5，代表受限的、低功耗嵌入式设置。我们采用了一种基于连续采样处理器和内存使用情况并结合面积-曲线积分的统一方法，来表征计算负载随输入文本长度的变化情况（对于语言模型）和图像分辨率的变化情况（对于视觉-语言模型）。我们发现了两条经验性的扩展定律：（1）语言模型推理的计算成本大约与标记长度成线性关系；（2）视觉-语言模型表现出一种预处理驱动的“分辨率拐点”，其中计算量在内部分辨率限制以上保持恒定，在以下则急剧下降。除了这些定律之外，我们还展示了量子启发式压缩可以将处理器和内存使用量最多减少71.9%，将能耗最多减少62%，同时保持或提高语义准确性。这些结果为局部语言和视觉-语言工作负载的多模态中央处理器仅计算扩展提供了一种系统化的量化方法，并且指出了模型压缩和输入分辨率预处理作为可持续边缘推理的有效、低成本杠杆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the energy efficiency of deploying large language models and vision-language models on edge devices, focusing on central processing units. By benchmarking models on a MacBook Pro M2 and a Raspberry Pi 5, the researchers discovered two scaling laws: computational cost for language models scales linearly with token length, and vision-language models have a resolution knee where compute remains constant above a certain resolution and decreases below it. Additionally, quantum-inspired compression was found to reduce processor and memory usage by up to 71.9% and energy consumption by up to 62%, while maintaining or improving semantic accuracy.</div>
<div class="mono" style="margin-top:8px">研究探讨了在边缘设备上部署大型语言模型和视觉-语言模型时的能效问题，重点关注中央处理单元。通过在MacBook Pro M2和Raspberry Pi 5上进行基准测试，研究人员发现两个缩放定律：语言模型的计算成本随着词元长度线性增加，而视觉-语言模型在某个分辨率以上保持计算量恒定，在以下则急剧下降。此外，量子启发式压缩最多可减少71.9%的处理器和内存使用，62%的能耗，并且保持或提高了语义准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Anomaly Thoughts with Large Vision-Language Models</div>
<div class="meta-line">Authors: Pedro Domingos, João Pereira, Vasco Lopes, João Neves, David Semedo</div>
<div class="meta-line">First: 2025-12-23T15:01:05+00:00 · Latest: 2025-12-23T15:01:05+00:00</div>
<div class="meta-line">Comments: 2 pages, 3 figures, 1 table. Accepted for RECPAD 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20417v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20417v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>异常链推理与大型视觉语言模型</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型在自动视频监控中的应用受限于其对正常情况的固有偏见，往往无法检测犯罪。虽然链式推理策略在语言任务中显示出显著的潜力，但在推理过程中缺乏归纳异常偏见进一步引导模型向正常解释。为了解决这一问题，我们提出了一种异常链推理（CoAT）多智能体推理框架，通过最终的异常分类层引入归纳犯罪偏见。我们的方法显著提高了异常检测，F1分数在低分辨率视频中提高了11.8个百分点，在高分辨率视频中的异常分类提高了3.78个百分点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of large vision-language models in detecting crimes during automated video surveillance due to their bias towards normality. It proposes Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias to improve anomaly detection. The method achieves a significant improvement in F1-score by 11.8 percentage points on low-resolution footage and in Anomaly Classification by 3.78 percentage points in high-resolution videos.</div>
<div class="mono" style="margin-top:8px">论文提出了一种名为Chain-of-Anomaly-Thoughts (CoAT)的方法，通过在最终的异常分类层中引入诱导犯罪偏见来解决大型视觉-语言模型在自动视频监控中的局限性。该方法提高了异常检测和分类性能，低分辨率视频的F1分数提高了11.8个百分点，高分辨率视频提高了3.78个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer</div>
<div class="meta-line">Authors: Yuzhuo Chen, Zehua Ma, Jianhua Wang, Kai Kang, Shunyu Yao, Weiming Zhang</div>
<div class="meta-line">First: 2025-08-01T09:51:54+00:00 · Latest: 2025-12-23T14:27:42+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.00477v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.00477v2">PDF</a> · <a href="https://github.com/Suchenl/LAMIC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC&#x27;s superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC&#x27;s performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAMIC：基于布局感知的多图像合成通过多模态扩散变换器的可扩展性</div>
<div class="mono" style="margin-top:8px">在可控图像合成中，从多个参考中生成具有空间布局感知的连贯且一致的图像仍然是一个开放的挑战。我们提出了LAMIC，一种布局感知的多图像合成框架，首次以无需训练的方式将单参考扩散模型扩展到多参考场景。基于MMDiT模型，LAMIC引入了两种即插即用的注意力机制：1）组隔离注意力（GIA）以增强实体分离；2）区域调节注意力（RMA）以实现布局感知生成。为了全面评估模型能力，我们进一步引入了三个指标：1）包含比（IN-R）和填充比（FI-R）以评估布局控制；2）背景相似度（BG-S）以衡量背景一致性。大量实验表明，LAMIC在大多数主要指标上均取得了最先进的性能：在所有设置中，它在ID-S、BG-S、IN-R和AVG得分上始终优于现有的多参考基线，并在复杂合成任务中实现了最佳的DPG。这些结果表明，LAMIC在保持身份、保留背景、布局控制和遵循提示方面具有优越的能力，所有这些均无需任何训练或微调，展示了强大的零样本泛化能力。通过继承先进的单参考模型的优势并使其无缝扩展到多图像场景，LAMIC为可控多图像合成建立了一个新的无需训练的范式。随着基础模型的不断进化，LAMIC的性能预计会相应地扩展。我们的实现可在以下链接获取：https://github.com/Suchenl/LAMIC。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LAMIC is a framework that addresses the challenge of generating coherent images from multiple references with spatial layout awareness. It extends single-reference diffusion models to multi-reference scenarios without training, using two attention mechanisms: Group Isolation Attention (GIA) and Region-Modulated Attention (RMA). LAMIC outperforms existing multi-reference baselines in metrics such as Inclusion Ratio (IN-R), Fill Ratio (FI-R), and Background Similarity (BG-S), demonstrating superior identity keeping, background preservation, and layout control in complex composition tasks.</div>
<div class="mono" style="margin-top:8px">LAMIC 是一种布局感知的多图像合成框架，它将单参考扩散模型扩展到多参考场景，无需训练。它引入了两种注意力机制：Group Isolation Attention (GIA) 用于实体分离和 Region-Modulated Attention (RMA) 用于布局感知生成。LAMIC 在包括 Inclusion Ratio、Fill Ratio 和 Background Similarity 在内的多个指标上优于现有方法，展示了在复杂合成任务中出色的身份保持、背景保留和布局控制能力。</div>
</details>
</div>
<div class="card">
<div class="title">CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</div>
<div class="meta-line">Authors: V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin</div>
<div class="meta-line">First: 2025-12-23T13:44:41+00:00 · Latest: 2025-12-23T13:44:41+00:00</div>
<div class="meta-line">Comments: 37 pages, 42 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20362v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20362v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.
  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.
  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRAFT：连续推理和自主反馈调优的多模态文本到图像生成</div>
<div class="mono" style="margin-top:8px">近期研究表明，在不重新训练的情况下，推理时的连续推理和反思可以提高文本到图像生成的效果。然而，现有方法往往依赖于隐式的整体批评或不受限制的提示重写，这使得它们的行为难以解释、控制或可靠地停止。相比之下，大型语言模型得益于基于验证、目标修正和早期停止的明确结构化推理形式。
我们提出了CRAFT（连续推理和自主反馈调优），这是一种无需训练、模型无关的框架，将这种结构化推理范式引入多模态图像生成。CRAFT 将提示分解为依赖结构化的视觉问题，使用视觉语言模型验证生成的图像，并仅在约束失败时通过LLM代理应用目标修正。该过程在所有约束条件满足后使用明确的停止标准进行迭代，从而产生可解释且可控的推理时细化循环。
在多个模型家族和具有挑战性的基准测试中，CRAFT 一致地提高了组合准确性、文本呈现和基于偏好的评估，特别是在轻量级生成器方面表现尤为突出。重要的是，这些改进仅带来了微不足道的推理时开销，使得较小或更便宜的模型能够接近更昂贵系统的质量。我们的结果表明，明确结构化、基于约束的推理是提高多模态生成模型可靠性的关键成分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CRAFT is a training-free, model-agnostic framework that enhances text-to-image generation by incorporating structured reasoning and targeted prompt edits. It decomposes prompts into visual questions, verifies generated images using a vision-language model, and applies edits through an LLM agent to meet constraints. Across various benchmarks, CRAFT improves compositional accuracy, text rendering, and preference evaluations, especially for lightweight generators, with minimal inference-time overhead.</div>
<div class="mono" style="margin-top:8px">CRAFT 是一个无需训练的框架，通过引入结构化推理和目标导向的提示编辑来提升文本到图像的生成。它将提示分解为视觉问题，使用视觉语言模型验证生成的图像，并通过 LLM 代理应用编辑以满足约束。这一过程提高了组合准确性、文本渲染和偏好评估，并且具有极小的推理时间开销，使其适用于轻量级模型。</div>
</details>
</div>
<div class="card">
<div class="title">ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge</div>
<div class="meta-line">Authors: Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou</div>
<div class="meta-line">First: 2025-12-23T11:29:03+00:00 · Latest: 2025-12-23T11:29:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20276v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ActionFlow：边缘设备上视觉语言动作模型的流水线加速</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型已成为机器人感知和控制的统一范式，使长期任务执行成为可能。然而，它们在动态现实环境中的部署受到高推理延迟的严重阻碍。虽然平滑的机器人交互需要20到30赫兹的控制频率，但当前的VLA模型由于自回归解码的内存限制，通常只能在边缘设备上以每秒3到5赫兹的速度运行。现有的优化往往需要大量的重新训练或牺牲模型的准确性。为了解决这一问题，我们提出了ActionFlow，这是一种针对资源受限边缘平台的系统级推理框架。ActionFlow的核心是一种跨请求流水线策略，这是一种新颖的调度器，重新定义了VLA推理为宏流水线中的微请求。该策略在连续时间步中智能地将内存受限的解码阶段与计算受限的预填充阶段进行批处理，以最大化硬件利用率。此外，为了支持这种调度，我们提出了跨请求状态打包前向运算符和统一的KV环形缓冲区，将碎片化的内存操作融合为高效的密集计算。实验结果表明，ActionFlow在不重新训练的情况下，使OpenVLA-7B模型的每秒帧数提高了2.55倍，从而在边缘硬件上实现实时动态操作。我们的工作可在https://anonymous.4open.science/r/ActionFlow-1D47获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the high inference latency of Vision-Language-Action (VLA) models in real-world robotic applications, which limits their deployment. ActionFlow is a system-level inference framework that introduces a Cross-Request Pipelining strategy to batch memory-bound decode phases with compute-bound prefill phases, maximizing hardware utilization. This approach, combined with a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, enables a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, facilitating real-time dynamic manipulation on edge hardware.</div>
<div class="mono" style="margin-top:8px">ActionFlow 是一种系统级推理框架，旨在加速边缘设备上的 Vision-Language-Action 模型，解决高推理延迟问题。它采用了一种跨请求流水线策略，将内存受限的解码阶段与计算受限的预填充阶段进行批处理，以最大化硬件利用率。实验结果显示，ActionFlow 在 OpenVLA-7B 模型上将 FPS 提高了 2.55 倍，无需重新训练模型即可实现边缘硬件上的实时动态操作。</div>
</details>
</div>
<div class="card">
<div class="title">LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation</div>
<div class="meta-line">Authors: Daniele Cardullo, Simone Teglia, Irene Amerini</div>
<div class="meta-line">First: 2025-12-23T11:14:58+00:00 · Latest: 2025-12-23T11:14:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20257v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20257v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LADLE-MM：基于有限标注的学习集成多模态虚假信息检测器</div>
<div class="mono" style="margin-top:8px">随着生成和操控多媒体内容的工具变得易于获取，数字媒体中的现实合成篡改已成为一种普遍威胁，通常涉及多种模态的同时篡改。近年来，此类技术被越来越多地用于扭曲重要事件的叙述并在社交媒体上传播虚假信息，促使开发虚假信息检测器。在图像-文本对的虚假信息传播背景下，已经提出了几种检测方法。然而，这些方法通常依赖于计算密集型架构或需要大量标注数据。在本工作中，我们引入了LADLE-MM：基于有限标注的学习集成多模态虚假信息检测器，这是一种在有限标注设置和受限训练资源下运行的多模态虚假信息检测器。LADLE-MM 由两个单模态分支和一个增强图像和文本表示的第三个多模态分支组成，该分支使用从BLIP提取的附加多模态嵌入作为固定的参考空间。尽管LADLE-MM 的可训练参数比之前最先进的模型少60.3%，但在DGM4基准上的二分类和多标签分类任务中，LADLE-MM 达到了竞争性性能，并且在没有使用地面真实标注进行训练时，优于现有方法。此外，在VERITE数据集上评估时，LADLE-MM 超过了使用更复杂架构（涉及大型视觉-语言模型）的现有最先进的方法，展示了在开放集设置中的有效泛化能力和对单模态偏差的强大鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LADLE-MM is a multimodal misinformation detector designed for limited annotation settings. It uses two unimodal branches and a multimodal branch enhanced with multimodal embeddings from BLIP. Despite having fewer parameters, LADLE-MM performs competitively on binary and multi-label classification tasks and outperforms existing methods on the DGM4 benchmark and VERITE dataset, showing strong generalization and robustness to unimodal bias.</div>
<div class="mono" style="margin-top:8px">LADLE-MM 是一种针对有限标注场景的多模态虚假信息检测器，使用两个单模态分支和一个通过 BLIP 提取的多模态嵌入增强的多模态分支，在 DGM4 基准测试上表现出色，参数量比之前的方法少 60.3%。此外，在 VERITE 数据集上的评估中，LADLE-MM 也优于现有方法，展示了其在开放集环境下的有效泛化能力和对单模态偏差的强大鲁棒性，无需使用地面真值注释。</div>
</details>
</div>
<div class="card">
<div class="title">Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation</div>
<div class="meta-line">Authors: Teqiang Zou, Hongliang Zeng, Yuxuan Nong, Yifan Li, Kehui Liu, Haotian Yang, Xinyang Ling, Xin Li, Lianyang Ma</div>
<div class="meta-line">First: 2025-12-23T09:28:20+00:00 · Latest: 2025-12-23T09:28:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20188v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20188v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>异步快速-缓慢视觉-语言-行动策略用于全身机器人操作</div>
<div class="mono" style="margin-top:8px">大多数视觉-语言-行动（VLA）系统将视觉-语言模型（VLM）用于语义推理，并由动作专家生成连续的动作信号，但两者通常以单一的统一频率运行。因此，策略性能受限于大型VLM的低推理速度。这种强制同步执行严重限制了全身机器人操作中的控制稳定性和实时性能，因为这种操作涉及更多的关节、更大的运动空间以及动态变化的视角。我们提出了一种真正的异步快速-缓慢VLA框架（DuoCore-FS），将系统组织为一个高频动作生成的快速路径和一个丰富的VLM推理的缓慢路径。该系统具有两个关键特征。首先，潜在表示缓冲区连接了缓慢和快速系统。它存储与场景指令上下文对齐的指令语义和动作推理表示，为快速路径提供高层次的指导。其次，全身动作分词器提供了一个紧凑的、统一的全身动作表示。重要的是，VLM和动作专家仍然端到端联合训练，保持统一策略学习的同时允许异步执行。DuoCore-FS 支持一个3B参数的VLM，同时实现30 Hz的全身动作片段生成，大约比具有可比模型大小的先前VLA模型快三倍。现实世界的全身操作实验表明，与同步快速-缓慢VLA基线相比，任务成功率和响应性显著提高。DuoCore-FS 的实现，包括训练、推理和部署，由Astribot作为Astribot机器人平台的一部分提供给商业用户。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces DuoCore-FS, an asynchronous Fast-Slow Vision-Language-Action framework designed to enhance whole-body robotic manipulation. It addresses the limitations of synchronous execution by separating the system into a fast pathway for high-frequency action generation and a slow pathway for rich semantic reasoning. Key features include a latent representation buffer and a whole-body action tokenizer. The system supports a 3B-parameter VLM and generates whole-body actions at 30 Hz, approximately three times faster than previous models. Experiments show improved task success rates and enhanced responsiveness compared to synchronous baselines.</div>
<div class="mono" style="margin-top:8px">研究旨在解决Vision-Language-Action (VLA)系统在全身体型机器人操作中的同步执行限制，这些限制受到大型Vision-Language Model (VLM)慢速推理速度的制约。研究引入了DuoCore-FS，这是一种异步的Fast-Slow VLA框架，将高频率的动作生成与较慢的VLM推理解耦。关键发现包括30 Hz的全身动作片段生成速率，比之前的VLA模型快约三倍，并且在真实世界实验中表现出更高的任务成功率和响应性，优于同步基线。</div>
</details>
</div>
<div class="card">
<div class="title">Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</div>
<div class="meta-line">Authors: Fanxiao Li, Jiaying Wu, Tingchao Fu, Yunyun Dong, Bingbing Song, Wei Zhou</div>
<div class="meta-line">First: 2025-08-18T08:19:43+00:00 · Latest: 2025-12-23T09:17:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12711v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.12711v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of multimodal misinformation poses growing threats to public discourse and societal trust. While Large Vision-Language Models (LVLMs) have enabled recent progress in multimodal misinformation detection (MMD), the rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven news diversity, characterized by highly varied and complex content. We show that this diversity induces multi-level drift, comprising (1) model-level misperception drift, where stylistic variations disrupt a model&#x27;s internal reasoning, and (2) evidence-level drift, where expression diversity degrades the quality or relevance of retrieved external evidence. These drifts significantly degrade the robustness of current LVLM-based MMD systems. To systematically study this problem, we introduce DriftBench, a large-scale benchmark comprising 16,000 news instances across six categories of diversification. We design three evaluation tasks: (1) robustness of truth verification under multi-level drift; (2) susceptibility to adversarial evidence contamination generated by GenAI; and (3) analysis of reasoning consistency across diverse inputs. Experiments with six state-of-the-art LVLM-based detectors show substantial performance drops (average F1 -14.8%) and increasingly unstable reasoning traces, with even more severe failures under adversarial evidence injection. Our findings uncover fundamental vulnerabilities in existing MMD systems and suggest an urgent need for more resilient approaches in the GenAI era.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>远离真相：由GenAI驱动的新闻多样性挑战LVLM基的误信息检测</div>
<div class="mono" style="margin-top:8px">多模态误信息的泛滥对公共话语和社会信任构成了日益增长的威胁。虽然大型视觉语言模型（LVLM）在多模态误信息检测（MMD）方面取得了近期进展，但生成型AI（GenAI）工具的兴起带来了新的挑战：由高度多样化和复杂内容特征的GenAI驱动的新闻多样性。我们表明，这种多样性导致了多级漂移，包括（1）模型级感知漂移，其中风格变化干扰了模型的内部推理，以及（2）证据级漂移，其中表达多样性降低了检索外部证据的质量或相关性。这些漂移显著削弱了当前基于LVLM的MMD系统的稳健性。为了系统地研究这一问题，我们引入了DriftBench，这是一个包含16,000个新闻实例的大规模基准，涵盖了六类多样化的类别。我们设计了三个评估任务：（1）在多级漂移下的事实验证稳健性；（2）对抗由GenAI生成的虚假证据污染的易感性；以及（3）对多样输入推理一致性的分析。六种最先进的基于LVLM的检测器的实验表明，性能下降显著（平均F1 -14.8%），推理轨迹越来越不稳定，并且在对抗虚假证据注入下表现更加严重。我们的研究揭示了现有MMD系统的基本脆弱性，并建议在GenAI时代迫切需要更稳健的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of GenAI-driven news diversity in multimodal misinformation detection, which introduces multi-level drift affecting both model-level misperception and evidence-level quality. To study this, the authors developed DriftBench, a large-scale benchmark with 16,000 news instances. Experiments with six state-of-the-art LVLM-based detectors showed significant performance drops (average F1 -14.8%) and unstable reasoning traces under adversarial evidence injection, highlighting the need for more robust MMD systems in the GenAI era.</div>
<div class="mono" style="margin-top:8px">该论文探讨了由生成式AI驱动的新闻多样性对多模态 misinformation 检测系统(MMD)的挑战，这种多样性可以引起多级漂移，影响模型推理和证据质量。作者引入了DriftBench大规模基准，并评估了六种最先进的LVLM基检测器，在多样输入和对抗性证据注入下，显示出显著的性能下降和推理不稳定性。这项工作揭示了现有MMD系统的基本脆弱性，并强调了在生成式AI时代需要更加稳健的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</div>
<div class="meta-line">Authors: Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin</div>
<div class="meta-line">Venue: CVPR 2025</div>
<div class="meta-line">First: 2025-12-23T09:14:16+00:00 · Latest: 2025-12-23T09:14:16+00:00</div>
<div class="meta-line">Comments: CVPR 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20174v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20174v1">PDF</a> · <a href="http://huggingface.co/datasets/nianbing/NL-DIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自然语言的文档图像检索：新数据集与基准</div>
<div class="mono" style="margin-top:8px">文档图像检索（DIR）旨在根据给定的查询从图像库中检索文档图像。现有的DIR方法主要基于图像查询，检索同一粗略语义类别中的文档，例如报纸或收据。然而，这些方法在实际场景中难以有效检索带有细粒度语义的文本查询的文档图像。为弥合这一差距，我们引入了一个新的基于自然语言的文档图像检索（NL-DIR）基准及其相应的评估指标。在此工作中，自然语言描述作为语义丰富的查询用于DIR任务。NL-DIR数据集包含41000张真实的文档图像，每张图像配对五个高质量的细粒度语义查询，通过大型语言模型生成并结合人工验证进行评估。我们对现有的主流对比视觉-语言模型和无OCR视觉文档理解（VDU）模型进行了零样本和微调评估。进一步研究了两阶段检索方法以提高性能，同时实现时间和空间效率。我们希望提出的NL-DIR基准能带来新的机会并促进VDU社区的研究。数据集和代码将在huggingface.co/datasets/nianbing/NL-DIR公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of document image retrieval using natural language queries, which are more fine-grained and realistic than existing image-based queries. It introduces a new benchmark, NL-DIR, with 41K document images paired with five high-quality textual queries. The study evaluates existing contrastive vision-language models and OCR-free VDU models, demonstrating their performance and identifying areas for improvement. A two-stage retrieval method is proposed to enhance efficiency. The NL-DIR dataset and codes will be publicly available.</div>
<div class="mono" style="margin-top:8px">该论文提出了一个新的基于自然语言的文档图像检索（NL-DIR）基准，以解决现有依赖图像查询的文档图像检索方法的局限性。该基准使用自然语言描述作为查询，这些查询是细粒度且语义丰富的。NL-DIR数据集包含41K张文档图像，每张图像配对有五个由大型语言模型生成和验证的高质量语义查询。研究评估了现有的对比视觉-语言模型和无OCR视觉文档理解模型，并提出了一种两阶段检索方法以提高性能。该基准旨在促进视觉文档理解社区的研究。</div>
</details>
</div>
<div class="card">
<div class="title">Vision Language Models are Confused Tourists</div>
<div class="meta-line">Authors: Patrick Amadeus Irawan, Ikhlasul Akmal Hanif, Muhammad Dehan Al Kautsar, Genta Indra Winata, Fajri Koto, Alham Fikri Aji</div>
<div class="meta-line">First: 2025-11-21T07:14:46+00:00 · Latest: 2025-12-23T08:46:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.17004v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.17004v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs&#x27; stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型是困惑的游客</div>
<div class="mono" style="margin-top:8px">尽管文化维度一直是评估视觉-语言模型（VLMs）的关键方面之一，但它们在面对多样文化输入时的稳定性仍然很少被测试，尽管这对于支持多样性和多文化社会至关重要。现有的评估往往依赖于仅包含每张图像一个单一文化概念的基准测试，忽视了多种可能无关的文化线索共存的场景。为解决这一缺口，我们引入了ConfusedTourist，这是一种新的文化对抗鲁棒性套件，旨在评估VLMs在受到地理线索干扰时的稳定性。我们的实验揭示了一个关键的脆弱性，即在简单的图像堆叠干扰下准确率大幅下降，甚至在基于图像生成的变体中进一步恶化。可解释性分析进一步表明，这些失败源于系统性地将注意力转移到分散注意力的线索上，使模型偏离其预期的焦点。这些发现突显了一个关键挑战：视觉文化概念的混合可以显著削弱即使是最先进的VLMs，强调了对更具有文化鲁棒性的跨模态理解的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the lack of robustness of Vision-Language Models (VLMs) in handling diverse cultural inputs by introducing ConfusedTourist, a new suite to evaluate their stability against perturbed geographical cues. The experiments show significant drops in accuracy under simple image-stacking perturbations and even worse performance with image-generation-based variants. The findings indicate that VLMs shift their attention to irrelevant cues, leading to poor performance. This highlights the need for more culturally robust VLMs.</div>
<div class="mono" style="margin-top:8px">该研究通过引入ConfusedTourist，旨在评估Vision-Language模型在处理多种地理线索干扰下的稳定性，以弥补现有模型在面对多样文化输入时的不足。实验结果显示，在简单的图像堆叠干扰下，模型的准确性显著下降，而基于图像生成的变体则表现更差。研究发现，模型会将注意力转移到无关线索上，导致性能不佳。这表明需要开发更具文化适应性的Vision-Language模型。</div>
</details>
</div>
<div class="card">
<div class="title">Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models</div>
<div class="meta-line">Authors: Ruofan Wang, Xin Wang, Yang Yao, Xuan Tong, Xingjun Ma</div>
<div class="meta-line">First: 2025-08-03T12:51:47+00:00 · Latest: 2025-12-23T05:52:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01741v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.01741v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning open-source Vision-Language Models (VLMs) creates a critical yet underexplored attack surface: vulnerabilities in the base VLM could be retained in fine-tuned variants, rendering them susceptible to transferable jailbreak attacks. To demonstrate this risk, we introduce the Simulated Ensemble Attack (SEA), a novel grey-box jailbreak method in which the adversary has full access to the base VLM but no knowledge of the fine-tuned target&#x27;s weights or training configuration. To improve jailbreak transferability across fine-tuned VLMs, SEA combines two key techniques: Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG). FTS generates transferable adversarial images by simulating the vision encoder&#x27;s parameter shifts, while TPG is a textual strategy that steers the language decoder toward adversarially optimized outputs. Experiments on the Qwen2-VL family (2B and 7B) demonstrate that SEA achieves high transfer attack success rates exceeding 86.5% and toxicity rates near 49.5% across diverse fine-tuned variants, even those specifically fine-tuned to improve safety behaviors. Notably, while direct PGD-based image jailbreaks rarely transfer across fine-tuned VLMs, SEA reliably exploits inherited vulnerabilities from the base model, significantly enhancing transferability. These findings highlight an urgent need to safeguard fine-tuned proprietary VLMs against transferable vulnerabilities inherited from open-source foundations, motivating the development of holistic defenses across the entire model lifecycle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模拟集成攻击：跨微调视觉语言模型转移破解攻击</div>
<div class="mono" style="margin-top:8px">微调开源视觉语言模型（VLMs）创建了一个关键但尚未充分探索的攻击面：基础VLM中的漏洞可能保留在微调变体中，使其容易受到可转移的破解攻击。为了展示这种风险，我们引入了模拟集成攻击（SEA），这是一种新颖的灰盒破解方法，攻击者可以完全访问基础VLM，但不了解微调目标的权重或训练配置。为了提高跨微调VLM的破解攻击可转移性，SEA结合了两种关键技术：微调轨迹模拟（FTS）和目标提示引导（TPG）。FTS通过模拟视觉编码器参数的变化生成可转移的对抗图像，而TPG是一种文本策略，引导语言解码器生成对抗优化的输出。在Qwen2-VL家族（2B和7B）上的实验表明，SEA实现了超过86.5%的高转移攻击成功率和接近49.5%的毒性率，即使在那些特别微调以提高安全行为的多样化微调变体中也是如此。值得注意的是，虽然直接基于PGD的图像破解攻击很少在微调VLM之间转移，但SEA可靠地利用了从基础模型继承的漏洞，显著提高了可转移性。这些发现突显了迫切需要保护微调的专有VLM免受从开源基础继承的可转移漏洞的影响，激励了整个模型生命周期中全面防御的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to highlight the risk of transferable jailbreak attacks on fine-tuned Vision-Language Models (VLMs) by introducing the Simulated Ensemble Attack (SEA), a novel grey-box method. SEA combines Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG) to generate transferable adversarial images and steer language outputs, respectively. Experiments show that SEA achieves high transfer attack success rates and near 50% toxicity rates across various fine-tuned VLMs, even those fine-tuned for safety. This work underscores the need for comprehensive defenses against inherited vulnerabilities in fine-tuned models.</div>
<div class="mono" style="margin-top:8px">研究通过引入Simulated Ensemble Attack (SEA)来应对细调的Vision-Language Models (VLMs)中的可转移劫持攻击风险。SEA结合了Fine-tuning Trajectory Simulation (FTS)和Targeted Prompt Guidance (TPG)，分别生成可转移的对抗图像和引导语言输出。实验结果显示，SEA在各种细调的VLMs中实现了高转移攻击成功率和接近50%的毒性率，即使那些专门细调以提高安全行为的VLMs也不例外。这突显了需要全面防御从开源模型继承而来的漏洞的紧迫性。</div>
</details>
</div>
<div class="card">
<div class="title">CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support</div>
<div class="meta-line">Authors: Yuting Zhang, Karina V. Bunting, Asgher Champsi, Xiaoxia Wang, Wenqi Lu, Alexander Thorley, Sandeep S Hothi, Zhaowen Qiu, Baturalp Buyukates, Dipak Kotecha, Jinming Duan</div>
<div class="meta-line">First: 2025-08-18T16:17:12+00:00 · Latest: 2025-12-23T04:17:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13256v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.13256v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cardiovascular diseases (CVDs) remain the foremost cause of mortality worldwide, a burden worsened by a severe deficit of healthcare workers. Artificial intelligence (AI) agents have shown potential to alleviate this gap through automated detection and proactive screening, yet their clinical application remains limited by: 1) rigid sequential workflows, whereas clinical care often requires adaptive reasoning that select specific tests and, based on their results, guides personalised next steps; 2) reliance solely on intrinsic model capabilities to perform role assignment without domain-specific tool support; 3) general and static knowledge bases without continuous learning capability; and 4) fixed unimodal or bimodal inputs and lack of on-demand visual outputs when clinicians require visual clarification. In response, a multimodal framework, CardAIc-Agents, was proposed to augment models with external tools and adaptively support diverse cardiac tasks. First, a CardiacRAG agent generated task-aware plans from updatable cardiac knowledge, while the Chief agent integrated tools to autonomously execute these plans and deliver decisions. Second, to enable adaptive and case-specific customization, a stepwise update strategy was developed to dynamically refine plans based on preceding execution results, once the task was assessed as complex. Third, a multidisciplinary discussion team was proposed which was automatically invoked to interpret challenging cases, thereby supporting further adaptation. In addition, visual review panels were provided to assist validation when clinicians raised concerns. Experiments across three datasets showed the efficiency of CardAIc-Agents compared to mainstream Vision-Language Models (VLMs) and state-of-the-art agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CardAIc-Agents：一种具有层次适应性的多模态框架以支持心脏护理支持</div>
<div class="mono" style="margin-top:8px">心血管疾病（CVDs）仍然是全球首要的致死原因，而严重的医疗工作者短缺加剧了这一负担。人工智能（AI）代理展示了通过自动化检测和主动筛查来缓解这一差距的潜力，但其临床应用受限于：1）僵化的顺序工作流程，而临床护理往往需要适应性推理来选择特定的测试，并根据其结果指导个性化的下一步；2）仅依赖内在模型能力进行角色分配，而没有领域特定工具的支持；3）通用和静态的知识库缺乏持续学习能力；以及4）固定的一元或多元输入，缺乏当临床医生需要视觉澄清时的按需视觉输出。为此，提出了一种多模态框架CardAIc-Agents，以增强模型与外部工具的结合，并适应性地支持多种心脏任务。首先，CardiacRAG代理从可更新的心脏知识中生成任务感知计划，而Chief代理则整合工具以自主执行这些计划并提供决策。其次，为了实现适应性和案例特定的定制，开发了一种逐步更新策略，根据先前执行结果动态细化计划，一旦任务被评估为复杂。第三，提出了一支多学科讨论团队，该团队可自动调用以解释具有挑战性的案例，从而支持进一步的适应。此外，提供了视觉审查小组以协助验证，当临床医生提出疑虑时。在三个数据集上的实验表明，CardAIc-Agents相比主流的视觉-语言模型（VLMs）和最先进的代理系统更有效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CardAIc-Agents is a multimodal framework designed to support cardiac care by addressing limitations in current AI systems, such as rigid workflows and lack of adaptability. It uses a CardiacRAG agent to generate task-aware plans and a Chief agent to execute these plans autonomously. The framework also includes a stepwise update strategy for refining plans based on execution results and a multidisciplinary discussion team to interpret complex cases. Visual review panels are provided to assist in validation. Experiments demonstrated that CardAIc-Agents outperformed mainstream Vision-Language Models and state-of-the-art agentic systems in terms of efficiency across three datasets.</div>
<div class="mono" style="margin-top:8px">CardAIc-Agents 是一个多模态框架，旨在通过解决当前 AI 系统的局限性（如僵化的流程和缺乏适应性）来支持心脏护理。该框架使用 CardiacRAG 代理生成任务感知计划，并使用 Chief 代理自主执行这些计划。框架还包含一种逐步更新策略，用于根据执行结果细化计划，并提出一个多学科讨论团队来解释复杂病例。还提供了视觉审查面板以协助验证。实验表明，CardAIc-Agents 在心脏护理任务的效率和适应性方面优于主流的视觉-语言模型和最先进的代理系统。</div>
</details>
</div>
<div class="card">
<div class="title">Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</div>
<div class="meta-line">Authors: Sangoh Lee, Sangwoo Mo, Wook-Shin Han</div>
<div class="meta-line">First: 2025-12-23T03:13:39+00:00 · Latest: 2025-12-23T03:13:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20014v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as &quot;bring my cup&quot;, where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bring My Cup！使用视觉注意提示个性化视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-动作（VLA）模型在通用指令上表现出色，但在处理个性化命令如“bring my cup”时却遇到困难，其中机器人必须在视觉上相似的对象中执行特定实例的操作。我们研究了操作个人物品的场景，在这种场景中，VLA 必须使用少量参考图像来识别和控制训练期间未见过的用户特定对象。为了解决这一挑战，我们提出了视觉注意提示（VAP），这是一种简单而有效的无需训练的感知适配器，为冻结的VLA提供自上而下的选择性注意力。VAP 将参考图像视为非参数视觉记忆，通过开放式词汇检测和基于嵌入的匹配将个人对象定位在场景中，然后通过突出显示对象并重写指令将这种定位作为视觉提示注入。我们构建了两个模拟基准 Personalized-SIMPLER 和 Personalized-VLABench，以及一个真实世界的桌面基准，以评估多个机器人和任务中的个性化操作。实验表明，VAP 在成功率和正确对象操作方面始终优于通用策略和标记学习基线，有助于弥合语义理解和实例级控制之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of personalizing vision-language-action models to handle specific commands like &#x27;bring my cup&#x27;, where the robot must identify and manipulate a particular object among similar ones. The authors propose Visual Attentive Prompting (VAP), a training-free method that uses reference images to guide the model&#x27;s attention, enhancing its ability to recognize and control the correct object. Experiments demonstrate that VAP improves success rates and correct-object manipulation compared to generic policies and token-learning baselines across various tasks and robots.</div>
<div class="mono" style="margin-top:8px">本文解决了Vision-Language-Action (VLA)模型处理特定命令如&#x27;bring my cup&#x27;的个性化问题，即机器人需要在相似物体中识别并操作特定物体。作者提出了一种名为Visual Attentive Prompting (VAP)的方法，该方法利用参考图像引导VLA的注意力，增强其识别和执行个人物体的能力。实验表明，VAP在成功率和正确物体操作方面优于通用策略和基于标记的学习基线，填补了语义理解和实例级控制之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</div>
<div class="meta-line">Authors: Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, Zhaoxiang Zhang</div>
<div class="meta-line">First: 2025-12-17T17:58:35+00:00 · Latest: 2025-12-23T03:03:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15649v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15649v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model&#x27;s ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-processed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VTCBench：视觉语言模型能否通过视觉文本压缩理解长上下文？</div>
<div class="mono" style="margin-top:8px">LLM的上下文窗口扩展相关的计算和内存开销严重限制了其可扩展性。值得注意的解决方案是视觉文本压缩（VTC），如DeepSeek-OCR和Glyph等框架，将长文本转换为密集的二维视觉表示，从而实现3倍至20倍的标记压缩比。然而，这种高信息密度对视觉语言模型（VLM）的核心长上下文能力的影响仍缺乏研究。为填补这一空白，我们首次引入了VTC基准，并系统评估了VLM在三种长上下文理解设置中的性能：VTC-检索，评估模型检索和聚合信息的能力；VTC-推理，要求模型通过最小的词汇重叠来推断潜在关联以定位事实；VTC-记忆，衡量模型在长期对话记忆中进行综合问答的能力。此外，我们建立了VTCBench-Wild以模拟多种输入场景。我们在基准上全面评估了领先开源和专有模型。结果表明，尽管大多数VLM能够很好地解码文本信息（如OCR），但在处理VTC处理的信息时，它们在长上下文理解方面表现出令人惊讶的差强人意的能力，无法捕捉上下文中的长期关联或依赖关系。本研究为理解VTC提供了深入的见解，并为设计更高效和可扩展的VLM奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces VTCBench, a benchmark for evaluating vision-language models (VLMs) in understanding long context using vision-text compression (VTC). The benchmark includes three settings: VTC-Retrieval, VTC-Reasoning, and VTC-Memory, to assess VLMs&#x27; abilities to retrieve, infer, and answer questions based on VTC-processed information. The results show that while VLMs can decode textual information well, they struggle with long-context understanding when using VTC, failing to capture long-term dependencies and associations in the context. This highlights the need for improving VLMs&#x27; long-context understanding capabilities.</div>
<div class="mono" style="margin-top:8px">研究引入VTCBench来评估使用视觉文本压缩(VTC)的视觉语言模型(VLM)的长上下文理解能力。它在VTC检索、VTC推理和VTC记忆三个场景下评估VLMs，并发现尽管大多数VLMs能够很好地解码文本信息，但在处理VTC压缩信息时仍难以捕捉长关联或依赖关系。这表明需要改进VLMs的长上下文理解能力。</div>
</details>
</div>
<div class="card">
<div class="title">FiGO: Fine-Grained Object Counting without Annotations</div>
<div class="meta-line">Authors: Adriano D&#x27;Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh</div>
<div class="meta-line">First: 2025-04-16T02:05:47+00:00 · Latest: 2025-12-23T01:57:40+00:00</div>
<div class="meta-line">Comments: data - https://dalessandro.dev/datasets/lookalikes/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.11705v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.11705v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Class-agnostic counting (CAC) methods reduce annotation costs by letting users define what to count at test-time through text or visual exemplars. However, current open-vocabulary approaches work well for broad categories but fail when fine-grained category distinctions are needed, such as telling apart waterfowl species or pepper cultivars. We present FiGO, a new annotation-free method that adapts existing counting models to fine-grained categories using only the category name. Our approach uses a text-to-image diffusion model to create synthetic examples and a joint positive/hard-negative loss to learn a compact concept embedding that conditions a specialization module to convert outputs from any frozen counter into accurate, fine-grained estimates. To evaluate fine-grained counting, we introduce LOOKALIKES, a dataset of 37 subcategories across 14 parent categories with many visually similar objects per image. Our method substantially outperforms strong open-vocabulary baselines, moving counting systems from &quot;count all the peppers&quot; to &quot;count only the habaneros.&quot;</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FiGO：无需注释的细粒度对象计数</div>
<div class="mono" style="margin-top:8px">无类别计数（CAC）方法通过让用户在测试时通过文本或视觉示例定义要计数的内容来减少注释成本。然而，当前的开放式词汇方法在处理广泛的类别时效果良好，但在需要细粒度类别区分时会失效，例如区分水禽种类或辣椒品种。我们提出了FiGO，这是一种新的无需注释的方法，它使用类别名称将现有的计数模型适应到细粒度类别。我们的方法使用文本到图像的扩散模型生成合成示例，并使用联合正样本/困难负样本损失来学习一个紧凑的概念嵌入，该嵌入条件化一个专门模块，将任何冻结计数器的输出转换为准确的细粒度估计。为了评估细粒度计数，我们引入了LOOKALIKES数据集，该数据集包含14个父类别中的37个子类别，每个图像中有许多视觉上相似的对象。我们的方法显著优于强大的开放式词汇基线，使计数系统从“计数所有的辣椒”转变为“仅计数胡椒科的辣椒”。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the limitations of current class-agnostic counting methods in handling fine-grained categories. FiGO, a new annotation-free method, uses a text-to-image diffusion model to generate synthetic examples and a joint positive/hard-negative loss to learn a compact concept embedding. This embedding conditions a specialization module to convert outputs from any frozen counter into accurate, fine-grained estimates. On the LOOKALIKES dataset, FiGO significantly outperforms strong open-vocabulary baselines, demonstrating its capability to count specific subcategories like &#x27;habaneros&#x27; among similar objects.</div>
<div class="mono" style="margin-top:8px">研究旨在解决当前无类别计数方法在处理细粒度类别时的局限性。FiGO 是一种无注释方法，使用文本到图像的扩散模型生成合成示例，并使用正样本/困难负样本联合损失来学习一个紧凑的概念嵌入。该嵌入条件化一个专门模块，将任何冻结计数器的输出转换为准确的细粒度估计。在LOOKALIKES数据集上，FiGO 显著优于强大的开放词汇基线，展示了其在类似对象中计数特定子类别（如‘哈瓦纳辣椒’）的能力。</div>
</details>
</div>
<div class="card">
<div class="title">SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction</div>
<div class="meta-line">Authors: Haoyi Zhong, Fang-Lue Zhang, Andrew Chalmers, Taehyun Rhee</div>
<div class="meta-line">First: 2025-12-23T00:24:46+00:00 · Latest: 2025-12-23T00:24:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19943v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19943v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SE360: 通过层次化数据构建在360°全景图中的语义编辑</div>
<div class="mono" style="margin-top:8px">尽管基于指令的图像编辑正在兴起，将其扩展到360°全景图带来了额外的挑战。现有方法在等角投影（ERP）和透视视图中经常产生不合理的结果。为了解决这些限制，我们提出了SE360，一种用于360°全景图中多条件引导对象编辑的新框架。其核心是一个无需人工干预的自上而下的数据生成流水线。该流水线利用视觉语言模型（VLM）和自适应投影调整进行分层分析，确保对象及其物理上下文的整体分割。生成的数据对既具有语义意义又具有几何一致性，即使这些数据来自未标记的全景图。此外，我们引入了一种成本效益高的两阶段数据精炼策略，以提高数据的真实性和减轻模型过拟合以消除伪影。基于构建的数据集，我们训练了一个基于变换器的扩散模型，以允许在360°全景图中根据文本、掩码或参考图像灵活地编辑对象。我们的实验表明，与现有方法相比，我们的方法在视觉质量和语义准确性方面都表现出更优的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenges of instruction-based image editing in 360-degree panoramas by proposing SE360, a novel framework that uses a coarse-to-fine data generation pipeline with a Vision-Language Model and adaptive projection adjustment. The method generates semantically meaningful and geometrically consistent data pairs, even from unlabeled panoramas. It also introduces a two-stage data refinement strategy to enhance realism and prevent overfitting. Experiments show that SE360 outperforms existing methods in visual quality and semantic accuracy.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出SE360框架解决360度全景图指令编辑的挑战，该框架使用粗到细的数据生成管道和视觉语言模型以及自适应投影调整。该方法生成具有语义意义和几何一致性且来自未标注全景图的数据对，并引入两阶段数据精炼策略以增强现实性和减少过拟合。实验表明，SE360在视觉质量和语义准确性方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models</div>
<div class="meta-line">Authors: Seyed Mohamad Ali Tousi, Ramy Farag, John A. Lory, G. N. DeSouza</div>
<div class="meta-line">First: 2025-11-17T20:29:44+00:00 · Latest: 2025-12-22T21:37:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13891v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13891v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM&#x27;s pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用视觉语言模型的弱监督遥感图像中瞬时冲沟检测</div>
<div class="mono" style="margin-top:8px">在土壤侵蚀问题中，瞬时冲沟是农业田地中最为关注的现象之一。由于其短暂的时间周期，使用经典计算机视觉方法和遥感技术自动检测它们的难度增加。此外，由于缺乏准确标注数据以及生成准确标注数据的难度，使用机器学习自动检测瞬时冲沟受到限制，仅限于零样本方法，这些方法难以实现。为克服这些挑战，我们提出了第一个用于检测瞬时冲沟的弱监督管道。该方法依赖于遥感技术，并利用视觉语言模型（VLMs）来大幅减少手动标注的劳动密集型任务。为了实现这一点，该方法利用了：1）VLMs预训练中嵌入的知识；2）教师-学生模型，其中教师从VLMs产生的嘈杂标签中学习，学生通过弱监督使用教师生成的标签和噪声感知损失函数学习。我们还提供了首个用于半监督检测遥感图像中瞬时冲沟的数据集。该数据集由一群土壤和植物科学家标注了多个位置，并包含大量未标注的位置。数据集包含超过18,000张高分辨率遥感图像，时间跨度为13年。我们的实验结果通过显示在使用弱监督训练学生模型时，我们的方法优于VLMs和标签模型本身，证明了我们方法的有效性。该工作的代码和数据集已公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a weakly supervised pipeline for detecting ephemeral gullies in remote sensing images using Vision Language Models (VLMs). The method leverages the pretraining knowledge of VLMs and a teacher-student model to reduce the need for manual labeling. The experimental results show that the approach outperforms VLMs and the label model itself when using weak supervision to train a student model. The work also introduces a new dataset for semi-supervised detection of ephemeral gullies from remote-sensed images, consisting of over 18,000 high-resolution images labeled by experts and a large number of unlabeled images.</div>
<div class="mono" style="margin-top:8px">本文提出了一种使用Vision Language Models (VLMs)的弱监督管道来检测遥感图像中的瞬时沟壑。该方法利用VLMs的预训练知识和教师-学生模型来减少手动标注的需求。实验结果表明，该方法在使用弱监督训练学生模型时，优于VLMs和标签模型本身。此外，该工作还引入了一个新的数据集，用于半监督检测遥感图像中的瞬时沟壑，包含超过18,000张由专家标注的高分辨率图像和大量未标注图像。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</div>
<div class="meta-line">Authors: Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra</div>
<div class="meta-line">First: 2025-12-22T18:41:45+00:00 · Latest: 2025-12-22T18:41:45+00:00</div>
<div class="meta-line">Comments: 14 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19663v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP&#x27;s 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越CLIP：知识增强的多模态变换器在糖尿病视网膜病变诊断中的跨模态对齐</div>
<div class="mono" style="margin-top:8px">糖尿病视网膜病变（DR）是全球可预防失明的主要原因，需要准确的自动化诊断系统。虽然通用领域的视觉-语言模型如对比语言-图像预训练（CLIP）在自然图像任务上表现良好，但在医学领域的应用中却遇到困难，特别是在眼科图像的跨模态检索方面。我们提出了一种新颖的知识增强联合嵌入框架，通过多模态变换器架构将视网膜底片图像、临床文本和结构化患者数据结合起来，以解决医学图像-文本对齐的关键差距。我们的方法为每种模态使用单独的编码器：用于视网膜图像的视觉变换器（ViT-B/16），用于临床叙述的Bio-ClinicalBERT，以及用于结构化人口统计和临床特征的多层感知器。这些模态通过具有模态特定嵌入的联合变换器融合，使用包括模态对之间的对比损失、图像和文本的重建损失以及根据ICDR和SDRG方案的DR严重程度分类损失的多个目标进行训练。在巴西多标签眼科数据集（BRSET）上的实验结果表明，与基线模型相比有显著改进。我们的框架在文本到图像检索性能上达到99.94%的召回率@1，而微调后的CLIP仅为1.29%，同时保持SDRG的最新分类准确率为97.05%，ICDR为97.97%。此外，对未见过的DeepEyeNet数据集的零样本评估验证了其强大的泛化能力，召回率@1为93.95%，而微调后的CLIP仅为0.22%。这些结果表明，我们的多模态训练方法有效地捕捉了医学领域的跨模态关系，建立了卓越的检索能力和稳健的诊断性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve automated diagnostic systems for diabetic retinopathy (DR) by addressing the limitations of general-domain vision-language models like CLIP in medical applications. The proposed framework uses a multimodal transformer architecture with separate encoders for retinal images, clinical text, and structured patient data, and trains them with multiple objectives. Experiments on the BRSET dataset show significant improvements over baseline models, achieving 99.94% Recall@1 for text-to-image retrieval and maintaining state-of-the-art classification accuracy. Zero-shot evaluation on the DeepEyeNet dataset further validates the model&#x27;s generalizability.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决通用领域模型如CLIP在医疗应用中的局限性，提高糖尿病视网膜病变的自动化诊断系统。作者提出了一种知识增强的联合嵌入框架，使用多模态变换器架构整合视网膜图像、临床文本和结构化患者数据。该模型在文本到图像检索性能和DR严重程度分级分类准确性方面均取得了显著改进，零样本评估在未见过的数据集上进一步验证了模型的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models</div>
<div class="meta-line">Authors: Heng Zhang, Haichuan Hu, Yaomin Shen, Weihao Yu, Yilei Yuan, Haochen You, Guo Cheng, Zijian Zhang, Lubin Gan, Huihui Wei, Hao Zhang, Jin Huang</div>
<div class="meta-line">First: 2025-09-16T06:16:05+00:00 · Latest: 2025-12-22T18:22:20+00:00</div>
<div class="meta-line">Comments: This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12715v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.12715v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AsyMoE：利用模态不对称性增强大型视觉语言模型专家专业化</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）通过扩展架构和大量训练，在多模态任务上表现出色。然而，现有的混合专家（MoE）方法由于视觉和语言处理之间的不对称性而面临挑战。视觉信息是空间完整的，而语言需要保持序列上下文。因此，MoE模型难以平衡模态特定特征和跨模态交互。通过系统分析，我们观察到深层的语言专家逐渐失去上下文定位，更多依赖参数知识，而不是利用提供的视觉和语言信息。为了解决这个问题，我们提出了一种新的AsyMoE架构，该架构使用三个专门的专家组来建模这种不对称性。我们设计了跨模态专家进行模态特定处理，超曲面跨模态专家进行分层跨模态交互，并设计了证据优先语言专家以抑制参数偏差并保持上下文定位。广泛的实验表明，与vanilla MoE和模态特定MoE相比，AsyMoE分别提高了26.58%和15.45%的准确率，且参数激活量比密集模型少25.45%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of modality asymmetry in large vision-language models (LVLMs) and proposes AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. The method includes intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to maintain contextual grounding. Despite the promising results showing 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, the authors withdrew the paper due to a fundamental error in the methodology that affects the validity of the main results.</div>
<div class="mono" style="margin-top:8px">论文旨在解决现有Mixture of Experts（MoE）方法在大型视觉语言模型（LVLM）中面临的挑战，由于视觉和语言处理之间的不对称性。它提出了AsyMoE，引入了三个专门的专家组来建模这种不对称性：内模专长专家进行模态特定处理，超球体跨模专长专家进行分层跨模交互，以及证据优先语言专家以保持上下文关联性。实验表明，AsyMoE在准确率上分别比vanilla MoE和模态特定MoE高出26.58%和15.45%，并且激活的参数比密集模型少25.45%。然而，作者因方法论中的根本错误撤回了提交。</div>
</details>
</div>
<div class="card">
<div class="title">GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</div>
<div class="meta-line">Authors: Heng Zheng, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Hao Zhang, Wenjun Huang, Jin Huang</div>
<div class="meta-line">First: 2025-11-02T11:58:55+00:00 · Latest: 2025-12-22T18:21:18+00:00</div>
<div class="meta-line">Comments: This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00908v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00908v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphGeo：基于异构图神经网络的多智能体辩论框架用于视觉地理定位</div>
<div class="mono" style="margin-top:8px">视觉地理定位需要广泛的空间知识和复杂的推理来确定图像位置，而不依赖GPS元数据。传统检索方法受限于数据库的覆盖范围和质量。最近的大规模视觉-语言模型（LVLMs）能够直接从图像内容进行位置推理，但单个模型在处理多样化的地理区域和复杂的场景时存在困难。现有的多智能体系统通过模型协作来提高性能，但对所有智能体间的交互处理方式相同。它们缺乏有效处理相互矛盾预测的机制。我们提出 **GraphGeo**，一种使用异构图神经网络的多智能体辩论框架，用于视觉地理定位。我们的方法通过类型化的边来建模多样的辩论关系，区分支持性的合作、竞争性的论辩以及知识转移。我们引入了一种结合节点级细化和边级论辩建模的双重级辩论机制。跨级别的拓扑结构细化策略使图结构和智能体表示能够共同进化。在多个基准上的实验表明，GraphGeo 显著优于现有最佳方法。我们的框架通过结构化的辩论将智能体之间的认知冲突转化为增强的地理定位准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GraphGeo is a multi-agent debate framework for visual geo-localization using heterogeneous graph neural networks. It models diverse debate relationships and introduces a dual-level debate mechanism for node-level refinement and edge-level argumentation. Experiments show that GraphGeo significantly outperforms existing methods, but the submission was withdrawn due to a fundamental error in the methodology that affects the validity of the results.</div>
<div class="mono" style="margin-top:8px">GraphGeo 是一种使用异构图神经网络的多代理辩论框架，用于视觉地理定位。它建模了多样化的辩论关系，并引入了双层辩论机制。实验表明，GraphGeo 显著优于现有最佳方法。但由于方法中的根本错误影响了主要结果的有效性，该提交已被作者撤回。</div>
</details>
</div>
<div class="card">
<div class="title">CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</div>
<div class="meta-line">Authors: Moritz Böhle, Amélie Royer, Juliette Marrie, Edouard Grave, Patrick Pérez</div>
<div class="meta-line">First: 2025-12-22T16:21:39+00:00 · Latest: 2025-12-22T16:21:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19535v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CASA：通过自注意力实现的跨注意力高效视觉-语言融合</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通常通过将预训练视觉编码器中的图像标记插入语言模型的文本流中来进行训练。这使得文本和图像信息能够在模型内部完全相互注意，但对高分辨率图像、长对话或流式视频来说，无论是内存还是计算成本都非常高昂。利用跨注意力的VLMs是标记插入的高效替代方案，但在涉及精细视觉细节的任务上表现出明显的性能差距。我们发现，提高此类模型的关键在于在专门的跨注意力层中也启用局部文本到文本的交互。基于此，我们提出了CASA（Cross-Attention via Self-Attention），一种简单且高效的范式，该范式在常见的图像理解基准测试上显著减少了与完整标记插入之间的差距，同时在处理如流式视频字幕等长上下文多模态任务时保持与跨注意力模型相同的可扩展性。如需查看示例和代码，请访问我们的项目页面 https://kyutai.org/casa 。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the efficiency of vision-language models (VLMs) by addressing the computational and memory costs associated with token insertion for high-resolution images and long conversations. The proposed method, CASA (Cross-Attention via Self-Attention), introduces local text-to-text interaction within cross-attention layers to bridge the performance gap with full token insertion. Experimental results show that CASA significantly reduces the gap on common image understanding benchmarks while maintaining scalability for long-context tasks like streaming video captioning.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决高分辨率图像和长对话中由于token插入带来的计算和内存成本问题，提高视觉语言模型（VLMs）的效率。提出的CASA（Cross-Attention via Self-Attention）方法在交叉注意力层中引入局部文本到文本的交互，以缩小与全token插入方法的性能差距。实验结果显示，CASA在常见的图像理解基准测试中显著减少了差距，同时保持了对长上下文任务如流式视频字幕生成的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</div>
<div class="meta-line">Authors: Li Puyin, Tiange Xiang, Ella Mao, Shirley Wei, Xinye Chen, Adnan Masood, Li Fei-fei, Ehsan Adeli</div>
<div class="meta-line">First: 2025-12-22T16:18:00+00:00 · Latest: 2025-12-22T16:18:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19526v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM&#x27;s physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM&#x27;s performance on estimating an object&#x27;s size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QuantiPhy：评估视觉语言模型物理推理能力的定量基准</div>
<div class="mono" style="margin-top:8px">理解物理世界对于通用人工智能代理至关重要。然而，尚不清楚最先进的视觉感知模型（例如大型VLM）是否能够进行定量的物理属性推理。现有的评估主要基于VQA且为定性的，提供的关于这些模型能否从视频观察中推断出移动物体的动力学量的见解有限。为解决这一问题，我们提出了QuantiPhy，这是第一个旨在定量测量VLM物理推理能力的基准。QuantiPhy包含超过3300个视频-文本实例，具有数值真实值，评估VLM在给定时间戳估计物体大小、速度和加速度的表现，其中一个属性作为输入先验。基准标准化了提示和评分，以评估数值准确性，从而实现模型之间的公平比较。我们在最先进的VLM上的实验揭示了它们的定性合理性与实际数值正确性之间的一致差距。我们进一步深入分析了关键因素，如背景噪声、反事实先验和策略性提示，发现最先进的VLM在进行定量动力学属性推理时，严重依赖预训练的世界知识，而不是忠实使用提供的视觉和文本输入作为参考。QuantiPhy提供了第一个严格的、可扩展的测试平台，使VLM超越单纯的口头合理性，迈向基于数字的物理理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">QuantiPhy is a benchmark designed to evaluate the quantitative physical reasoning abilities of vision-language models. It consists of over 3,300 video-text instances with numerical ground truth, testing models&#x27; ability to estimate an object&#x27;s size, velocity, and acceleration. Experiments show a gap between models&#x27; qualitative plausibility and numerical accuracy, indicating reliance on pre-trained knowledge rather than visual and textual inputs for quantitative reasoning.</div>
<div class="mono" style="margin-top:8px">QuantiPhy 是一个基准，用于评估视觉语言模型的定量物理推理能力。它包含超过 3,300 个带有数值 ground truth 的视频-文本实例，评估模型估计物体大小、速度和加速度的能力。实验显示，模型的定性合理性与数值准确性之间存在差距，表明它们在定量推理物理属性时更多依赖预训练知识而非视觉和文本输入。该基准提供了一种标准化的方法来比较模型，并推动它们向基于数值的物理理解迈进。</div>
</details>
</div>
<div class="card">
<div class="title">VERDI: VLM-Embedded Reasoning for Autonomous Driving</div>
<div class="meta-line">Authors: Bowen Feng, Zhiting Mei, Baiang Li, Julian Ost, Filippo Ghilotti, Roger Girgis, Anirudha Majumdar, Felix Heide</div>
<div class="meta-line">First: 2025-05-21T18:24:36+00:00 · Latest: 2025-12-22T15:37:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.15925v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.15925v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VERDI: VLM嵌入式自主驾驶推理</div>
<div class="mono" style="margin-top:8px">尽管自主驾驶（AD）堆栈在部分可观测性和现实世界复杂性下难以做出决策，但人类驾驶员能够利用常识推理在信息有限的情况下做出近乎最优的决策。最近的工作尝试利用微调的视觉-语言模型（VLMs）在推理时进行轨迹规划，以模拟人类行为。尽管这些方法在基准测试中取得了成功，但它们往往难以部署（一个700亿参数的VLM推理需要每秒8个词，内存超过160G），并且其单一网络结构禁止安全性分解。为了解决这一问题，我们提出了自主驾驶嵌入式推理（VERDI），这是一种训练时框架，将VLMs的推理过程和常识知识提炼到AD堆栈中。VERDI通过将模块化可微端到端（e2e）AD模型的中间模块输出与VLMs生成的解释驾驶推理过程的文本特征对齐，在感知、预测和规划阶段进行对齐。通过在潜在空间中促进对齐，VERDI使模块化AD堆栈能够内化结构化推理，而不必承担大型VLMs的推理时间成本。我们在开环（NuScenes和Bench2Drive基准）和闭环（HugSim模拟器）环境中验证了VERDI。我们发现，与不嵌入推理的现有端到端方法相比，VERDI在$\ell_{2}$距离上提高了11%，在驾驶性能上提高了11%，同时保持了实时推理速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VERDI is a training-time framework that integrates Vision-Language Models (VLMs) into autonomous driving systems to enhance decision-making under limited information. By aligning intermediate outputs with text explanations from VLMs, VERDI enables modular AD models to internalize structured reasoning without the high inference costs of large VLMs. Experimental results show that VERDI outperforms existing end-to-end methods in both open-loop and closed-loop settings, achieving up to 11% improvement in $l_2$ distance and driving performance while maintaining real-time inference speed.</div>
<div class="mono" style="margin-top:8px">VERDI 是一个训练时框架，将视觉-语言模型（VLM）嵌入到自动驾驶系统中，以增强在有限信息下的决策能力。通过将中间输出与 VLM 生成的文本解释对齐，VERDI 使模块化的 AD 模型能够内化结构化的推理，而不增加大型 VLM 的推理成本。实验结果表明，VERDI 在开环和闭环设置中均优于现有端到端方法，分别在 $l_2$ 距离和驾驶性能上提高了 11%，同时保持实时推理速度。</div>
</details>
</div>
<div class="card">
<div class="title">SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning</div>
<div class="meta-line">Authors: Xiaojun Guo, Runyu Zhou, Yifei Wang, Qi Zhang, Chenheng Zhang, Stefanie Jegelka, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin, Yisen Wang</div>
<div class="meta-line">First: 2025-10-18T09:22:40+00:00 · Latest: 2025-12-22T15:14:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16416v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.16416v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework&#x27;s generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSL4RL：重新审视自我监督学习作为视觉-语言推理内在奖励的方法</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通过结合大型语言模型和视觉输入展示了显著的能力。然而，它们往往未能充分利用视觉证据，要么依赖于视觉中心任务中的语言先验，要么在推理过程中求助于文本捷径。尽管强化学习（RL）可以将模型与期望的行为对齐，但将其应用于VLMs受到了缺乏可扩展和可靠的奖励机制的阻碍。为克服这一挑战，我们提出了一种名为SSL4RL的新框架，该框架利用自我监督学习（SSL）任务作为基于RL的微调的验证性奖励来源。我们的方法将SSL目标，如预测图像旋转或重建遮罩的片段，重新表述为密集的自动奖励信号，从而消除了对人类偏好数据或不可靠的人工智能评估者的需要。实验表明，SSL4RL在视觉中心和视觉-语言推理基准测试中显著提高了性能。此外，通过系统性的消融实验，我们确定了影响SSL4RL任务有效性的关键因素，如任务难度、模型规模和与目标领域的语义对齐，为未来的工作提供了新的设计原则。我们还通过将其应用于图学习，展示了该框架的通用性，其中它带来了显著的收益。SSL4RL建立了一种灵活且有效的范式，用于使用可验证的自我监督目标对多模态模型进行对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the performance of vision-language models by integrating self-supervised learning (SSL) as intrinsic rewards in reinforcement learning (RL). The method involves converting SSL tasks like image rotation prediction and masked patch reconstruction into dense, automatic reward signals. Experiments demonstrate that this approach significantly improves performance on vision-centric and vision-language reasoning benchmarks, and it also shows effectiveness in graph learning tasks. Key factors influencing the effectiveness include task difficulty, model scale, and semantic alignment with the target domain.</div>
<div class="mono" style="margin-top:8px">该研究提出了SSL4RL框架，利用自我监督学习（SSL）任务作为强化学习（RL）微调视觉语言模型（VLM）的内在奖励。这种方法通过提供密集的自动奖励信号，而无需人类偏好或不可靠的评估者，提高了视觉中心和视觉语言推理基准上的性能。影响SSL4RL效果的关键因素包括任务难度、模型规模和与目标领域的语义对齐。该框架还被证明在图学习任务中有效，展示了其普适性。</div>
</details>
</div>
<div class="card">
<div class="title">EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration</div>
<div class="meta-line">Authors: Runze Li, Yuwen Zhai, Bo Xu, LiWu Xu, Nian Shi, Wei Zhang, Ran Lin, Liang Wang</div>
<div class="meta-line">First: 2025-12-22T13:42:18+00:00 · Latest: 2025-12-22T13:42:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19396v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19396v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital &#x27;&#x27;amnesia&#x27;&#x27; results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable &#x27;&#x27;memories&#x27;&#x27;. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent&#x27;s reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EchoTrail-GUI：通过评论引导自我探索构建可操作的记忆以构建GUI代理</div>
<div class="mono" style="margin-top:8px">当代GUI代理由于大型视觉-语言模型（VLMs）的进步而变得越来越强大，但它们通常以一个关键限制为代价：它们将每个任务视为独立的，缺乏系统地从过去成功中学习的机制。这种数字“健忘症”导致性能不佳、重复错误和对新挑战的糟糕泛化。为了弥合这一差距，我们提出了EchoTrail-GUI，这是一种新型框架，旨在通过为代理提供动态且易于访问的记忆来模拟人类经验学习。我们的框架分为三个阶段。首先，在经验探索阶段，代理自主与GUI环境交互，构建由奖励模型验证的成功任务轨迹数据库。关键的是，整个知识库构建过程完全自动化，无需人类监督。其次，在记忆注入阶段，当收到新任务时，我们的系统高效地检索最相关的过去轨迹，作为可操作的“记忆”。最后，在GUI任务推理阶段，这些记忆作为上下文指导注入，以指导代理的推理和决策过程。我们在包括Android World和AndroidLab在内的基准测试上展示了我们方法的有效性。结果显示，EchoTrail-GUI 显著提高了基线代理的任务成功率和操作效率，验证了结构化记忆在创建更强大和智能的GUI自动化方面的力量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EchoTrail-GUI addresses the issue of digital amnesia in contemporary GUI agents by introducing a framework that allows agents to learn from past experiences. The framework consists of three stages: Experience Exploration, where agents autonomously build a database of successful task trajectories; Memory Injection, where relevant past trajectories are retrieved for new tasks; and GUI Task Inference, where these memories guide the agent&#x27;s decision-making. Experiments on Android World and AndroidLab show that EchoTrail-GUI enhances task success rates and operational efficiency compared to baseline agents.</div>
<div class="mono" style="margin-top:8px">EchoTrail-GUI通过引入一个框架来解决GUI代理在处理任务时缺乏记忆的问题，该框架使代理能够从过去的经验中学习。该框架包括三个阶段：经验探索，代理自主构建成功的任务轨迹数据库；记忆注入，为新任务检索相关的历史轨迹；以及GUI任务推理，这些记忆作为上下文指导来引导代理的决策。实验结果表明，EchoTrail-GUI在Android World和AndroidLab上的任务成功率和操作效率都优于基线代理。</div>
</details>
</div>
<div class="card">
<div class="title">Xiaomi MiMo-VL-Miloco Technical Report</div>
<div class="meta-line">Authors: Jiaze Li, Jingyang Chen, Yuxun Qu, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, Wenhui Tan, Pei Fu, Jianzhong Ju, Zhenbo Luo, Jian Luan</div>
<div class="meta-line">First: 2025-12-19T10:43:37+00:00 · Latest: 2025-12-22T13:27:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17436v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17436v2">PDF</a> · <a href="https://github.com/XiaoMi/xiaomi-mimo-vl-miloco">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We open-source MiMo-VL-Miloco-7B and its quantized variant MiMo-VL-Miloco-7B-GGUF, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at https://github.com/XiaoMi/xiaomi-mimo-vl-miloco to support research and deployment in real-world smart-home applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小米MiMo-VL-Miloco技术报告</div>
<div class="mono" style="margin-top:8px">我们开源了MiMo-VL-Miloco-7B及其量化变体MiMo-VL-Miloco-7B-GGUF，这是一个面向家庭的视觉-语言模型对，能够在家庭场景理解和通用多模态推理方面取得优异表现。基于MiMo-VL-7B骨干网络，MiMo-VL-Miloco-7B专门针对智能家居环境，实现了手势识别和常见家庭场景理解的领先F1分数，并在视频基准测试（如Video-MME、Video-MMMU和Charades-STA）以及语言理解基准测试（如MMMU-Pro和MMLU-Pro）中也取得了持续的改进。在我们的实验中，MiMo-VL-Miloco-7B在家庭场景理解和多个多模态推理基准测试中均优于强大的闭源和开源基线。为了平衡专业化和通用性，我们设计了一种两阶段训练管道，结合了监督微调和基于组相对策略优化的强化学习，利用高效多域数据。我们进一步引入了思维链监督和基于token预算的推理，使模型能够在数据高效学习的同时，也能高效推理。我们的分析表明，针对家庭场景的训练不仅增强了活动和手势理解，还仅以适度的文档中心任务权衡提高了文本推理能力。模型检查点、量化GGUF权重以及我们的家庭场景评估工具包可在https://github.com/XiaoMi/xiaomi-mimo-vl-miloco 公开获取，以支持在实际智能家居应用中的研究和部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop specialized vision-language models for smart-home environments, focusing on gesture recognition and home-scenario understanding. The method involves a two-stage training pipeline combining supervised fine-tuning and reinforcement learning, along with chain-of-thought supervision and token-budget-aware reasoning. Key findings show that MiMo-VL-Miloco-7B outperforms strong baselines on home-scenario understanding and various multimodal reasoning benchmarks, while maintaining efficiency and data efficiency.</div>
<div class="mono" style="margin-top:8px">研究旨在开发专门针对智能家居环境的视觉-语言模型，专注于手势识别和家居场景理解。方法包括结合监督微调和基于组相对策略优化的强化学习的两阶段训练管道，以及链式思考监督和令牌预算感知推理。关键发现表明，MiMo-VL-Miloco-7B在家居场景理解和多种多模态推理基准测试中优于强基线，同时保持高效性和数据效率。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251224_0320.html">20251224_0320</a>
<a href="archive/20251223_0320.html">20251223_0320</a>
<a href="archive/20251222_0318.html">20251222_0318</a>
<a href="archive/20251221_0318.html">20251221_0318</a>
<a href="archive/20251220_0319.html">20251220_0319</a>
<a href="archive/20251219_0322.html">20251219_0322</a>
<a href="archive/20251218_0333.html">20251218_0333</a>
<a href="archive/20251217_0321.html">20251217_0321</a>
<a href="archive/20251216_0326.html">20251216_0326</a>
<a href="archive/20251215_0321.html">20251215_0321</a>
<a href="archive/20251214_0316.html">20251214_0316</a>
<a href="archive/20251213_0319.html">20251213_0319</a>
<a href="archive/20251212_0322.html">20251212_0322</a>
<a href="archive/20251211_0319.html">20251211_0319</a>
<a href="archive/20251210_0317.html">20251210_0317</a>
<a href="archive/20251209_0321.html">20251209_0321</a>
<a href="archive/20251208_0317.html">20251208_0317</a>
<a href="archive/20251207_0317.html">20251207_0317</a>
<a href="archive/20251206_0318.html">20251206_0318</a>
<a href="archive/20251205_0320.html">20251205_0320</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
